ID,Proposal title,Proposal state,Pending proposal state,Session type,Track,created,Tags,Abstract,Description,Notes,Internal notes,Duration,Slot Count,Language,Show this session in public list of featured sessions.,Don't record this session.,Session image,Speaker IDs,Speaker names,Room,Start,End,Median score,Average (mean) score,Resources,Abstract as a tweet,Category [High Performance Computing],"Category [Community, Education, and Outreach]",Category [Machine and Deep Learning],Category [Scientific Applications],Category [Data Science and Visualization],Expected audience expertise: Domain,Expected audience expertise: Python,Talk submissions only : Would you be prepared to do a poster as fallback,Public link to supporting material,Project Homepage / Git
3D7SAQ,Model Documentation: The Keystone towards Inclusivity and Accessibility,confirmed,,Talk (15 mins + Q&A),"Community, Education, and Outreach",2023-04-30T21:43:22.635463+00:00,request as poster,"The use of AI documentation such as repository cards (model and dataset cards), as a means of transparently discussing ethical and inclusive problems that could be found within the outputs and/or during the creation of AI artefacts, with the aim of inclusivity, fairness and accountability, has increasingly become part of the ML discourse. As limitations and risks centred documentation approaches have become more standard and anticipated with launches of new development e.g Chatgpt/GPT-4 system card and other LLM model cards. 

This talk highlights the inclusive approaches that the broader open source community could explore when thinking about their aims when creating documentation.","In this talk we will first cover some of the current literature and standard approaches of documentation found within the open source community and within the ethical/ AI space. Building on this overview, we will then detail how to build on the strengths of the open source  community and its ability to bridge the gap between academia and research. From which we will map to more inclusive and ethics focused components found in AI model documentation practises, and how they could be incorporated within open source documentation methods. 

By the end of this talk we would have not only identified the limitations within current open source documentation practises, we will also explore how centering fairness, ethics and inclusivity can create richer documentation that is more wholly inclusive of the open source community it represents.",,,20,1,en,False,False,,AFB7PG,Ezi Ozoani,HS 120,2023-08-17T09:20:00+00:00,2023-08-17T11:40:00+02:00,2.00,1.8,,"Rethinking open source documentation: bridging academia & industry, while creating richer documentation that is wholly inclusive of the open source community.",,Learning and Teaching Scientific Python,,,,none,some,yes,,
3ENHXR,Exploring Geospatial data for Machine Learning using Google Earth Engine: An introduction,confirmed,,Talk (15 mins + Q&A),Scientific Applications,2023-06-02T06:33:24.473182+00:00,,"Have you ever wondered what type of data you can get about a certain location on the globe? What if I told you that you can access an enormous amount of information while sitting right there at your laptop? In this talk, I'll show you how to use Google Earth Engine to enrich your dataset. Either your exploring, or planning your next ML project, Geospatial data can provide you with a lot of information you did not know you had access to. Let me show you how!","This talk is aimed at Machine Learning Engineers, Data Scientists, and researchers that have good experience with Python. The goal is to teach these professsionals how they can leverage Google Earth Engine (GEE) to enrich and explore their datasets. By the end of this talk, I expect that the audience has a good grasp of what GEE is, and how they can use it in their next project!

We'll go through a small introduction of Geospatial data, and the different types of providers out there. I'll introduce Google Earth Engine and the Python API that has been recently in the works. Finally, I'll go over some use cases that users might explore, as well as some examples from the trenches. 

Here's the outline: 

- Geospatial data: What is it? 
- A miriad of providers: From LandSat, to Modius, to Sentinel 
- Bringing it all together: Google Earth Engine
- An intro to Google Earth Engine
- The Python API, how to use it? 
- Example: Enriching a dataset for carbon stock measurements in Farms
- Tips and tricks
- Where to go from here",,,20,1,en,True,False,,KZDHYW,Duarte Carmo,Aula,2023-08-17T09:20:00+00:00,2023-08-17T11:40:00+02:00,2.00,1.7,,Want to learn of to extract data from Satelites to enrich your dataset? Learn how to use Google Earth Engine with Python!,,,,Geo Science,,some,some,no,,
73E7GM,"The Graphic Server Protocol, a joint effort to facilitate the interoperability of Python scientific visualization libraries",confirmed,,Maintainer track,Data Science and Visualisation,2023-04-14T14:13:04.471197+00:00,,"The graphic server protocol is a proposal to mutualize efforts across scientific visualization libraries, languages and platforms such as to provide a unified intermediate-level protocol to render graphical primitives independently of the specifics of the high-level visualization interfaces.","Scientific figures, as complex as they might be, always end up being made of a (large) set of elementary graphical components, namely points (or markers), lines, glyphs (text), polygons, and volumes. Glyphs, markers and polygons can be further decomposed into triangles such that, apart from the specific case of 3D volume rendering, the elementary components of most scientific figures are essentially points, lines and triangles.

When designing a scientific figure, one rarely manipulates these components explicitly. Instead, one uses high-level plotting functions that ultimately produce these elementary components. What defines a scientific visualization library is the choice and the definition of these high-level functions. The library may provide high-level functions that allow them to quickly design a figure (e.g. [ggplot], [vega], [seaborn]), or lower-level functions that allows them to further tune the figure (e.g. [matplotlib], [vispy]). In the most extreme case, the user may even be provided a total freedom at the price of complexity ([TikZ]). There is no definitive one-size-fits-all API because some users prefer the simplicity of a high-level interface that makes most decisions transparently, while others prefer to have a total control of all aspects of the figure.

In all cases, however, the rendering task is the same: drawing points, lines, markers, glyphs, polygons, meshes and volumes as efficiently as possible, possibly taking advantage of low-level libraries and hardware. Ultimately, rendering represents significantly redundant efforts across visualization libraries, and it becomes increasingly complex when considering low-level hardware-accelerated graphics interfaces such as OpenGL, Metal, DirectX, or Vulkan. Our experience with the development of hardware-accelerated scientific visualization libraries such as [glumpy], galry, and [vispy] has shown that this complexity is a significant obstacle to the development of an efficient and scalable scientific visualization library in Python.

The graphic server protocol is a proposal to mutualize efforts across scientific visualization libraries, languages and platforms such as to provide a unified intermediate-level protocol to render graphical primitives independently of the specifics of the high-level visualization interfaces. The goal is not to provide yet another graphical library, but a foundational protocol-based architecture that any scientific visualization library can reuse.

During this talk, we will introduce the graphical server protocol we have designed and show a couple of early examples using two different rendering backends: matplotlib and [Datoviz] (which is based on Vulkan, a low-level interface for high-performance rendering and computing using GPUs). These two backends will render the same figure with the same visual quality using the same Python script, even though the Vulkan backend is expected to be significantly faster than the matplotlib one. Currently, the matplotlib instance can produce 3D graphics (meshes, surfaces and volumes) even though it is limited by the absence of a depth buffer that prevents proper sorting. The roadmap for the protocol is still largely open and we would like to gather feedback from developers of other scientific visualization libraries to potentially create an international steering group that could help define the roadmap.

[ggplot]: https://ggplot2.tidyverse.org/reference/ggplot.html
[vega]: https://vega.github.io/vega/
[seaborn]: https://seaborn.pydata.org/
[matplotlib]: https://matplotlib.org/
[vispy]: https://vispy.org/
[tikz]: https://en.wikibooks.org/wiki/LaTeX/PGF/TikZ
[Datoviz]: https://datoviz.org/
[glumpy]: http://glumpy.github.io/",,,45,1,en,False,False,,EGADXL,Nicolas Rougier,HS 119 - Maintainer track,2023-08-17T13:30:00+00:00,2023-08-17T16:15:00+02:00,1.50,1.2,,"The Graphic Server Protocol, a joint effort to facilitate the interoperability of Python scientific visualization libraries",,,,,Data Visualization,some,some,no,,
7P3AYM,PPML: Machine Learning on data you cannot see,confirmed,,Tutorial,Machine and Deep Learning,2023-06-20T13:05:58.908213+00:00,,"Privacy guarantee is **the** most crucial requirement when it comes to analyse sensitive data. However, data anonymisation techniques alone do not always provide complete privacy protection; moreover Machine Learning models could also be exploited to _leak_ sensitive data when _attacked_, and no counter-measure is applied. *Privacy-preserving machine learning* (PPML) methods hold the promise to overcome all these issues, allowing to train machine learning models with full privacy guarantees. In this tutorial we will explore several methods for privacy-preserving data analysis, and how these techniques can be used to safely train ML models _without_ actually seeing the data.","Privacy guarantees are **the** most crucial requirement when it comes to analyse sensitive data. These requirements could be sometimes very stringent, so that it becomes a real barrier for the entire pipeline. Reasons for this are manifold, and involve the fact that data could not be _shared_ nor moved from their silos of resident, let alone analysed in their _raw_ form. As a result, _data anonymisation techniques_ are sometimes used to generate a sanitised version of the original data. However, these techniques alone are not enough to guarantee that privacy will be completely preserved. Moreover, the _memoisation_ effect of Deep learning  models could be maliciously exploited to _attack_ the models, and _reconstruct_  sensitive information about samples used in training, even if these information were not originally provided. 

*Privacy-preserving machine learning* (PPML) methods hold the promise to overcome all those issues, allowing to train machine learning models with full privacy guarantees.

This workshop will be mainly organised in **two** main parts. In the first part,  we will focus on Machine learning demonstrating how DL models could be exploited (i.e. _inference attack_ ) to reconstruct original data solely analysing models predictions; and then we will explore how **differential privacy** can help us protecting the privacy of our model, with _minimum disruption_ to the original pipeline. 

In the second part we will considering more complex ML scenarios to train Deep learning networks on encrypted data, with specialised **distributed federated learning** strategies.",,,90,1,en,False,False,,7FNYLG,Valerio Maggio,Aula,2023-08-15T08:30:00+00:00,2023-08-15T12:00:00+02:00,,,,Privacy Preserving Machine Learning: ML on Data you cannot see,Parallel Computing,Learning and Teaching Scientific Python,Algorithmic bias and Trustworthy AI,Astronomy,Data Analysis and Data Engineering,some,some,,,
8NHQGJ,Predictive survival analysis and competing risk modeling with Gradient Boosted CIF,confirmed,,Poster,Machine and Deep Learning,2023-05-25T22:08:42.611428+00:00,,This tutorial is an introduction to our new survival analysis estimator called Gradient Boosted CIF. This model predicts cumulative incidence functions (CIF) in competing risk settings by leveraging scikit-learn's HistGradientBoostingClassifier.,"You probably already have encountered survival analysis problems in your data science problems. From predictive maintenance and biostatistics to marketing studies on churn, lifetime values, and upselling, there is a blooming field of application for time-to-event regression within the Python community.

What is the average time for a mechanical part to fail? What quantile of your marketing cohort will churn within the next six months? What are the patient's survival probability function curves through a clinical study? Conversely, what is the probability of incidence of each cause of death for different comorbidities?

To answer all these questions, we introduce the Gradient Boosted CIF, our new survival estimator named after its ability to predict cumulative incidence functions (CIF) in competing risk settings —in addition to regular survival probabilities functions. Under the hood, this estimator leverages the HistGradientBoostingClassifier of scikit-learn and is therefore adapted to scikit-learn pipelines.

We will illustrate how to use Gradient Boosted CIF from the hazardous library and compare its performances to alternatives like Random Survival Forest, XGBoost-based XGBSE models, and deep learning models like DeepHit. We will also detail its design and how to inspect it.",,,90,1,en,True,False,,QCVBZD,Vincent Maladiere,,,,2.00,1.8,,"Introducing Gradient Boosted CIF, survival analysis in competing risk settings with scikit-learn!",,,Supervised Learning,,,some,some,,https://github.com/soda-inria/survival-analysis-benchmark,https://github.com/soda-inria/hazardous
8ZANVV,"Sparse Data in the Scientific Python Ecosystem: Current Needs, Recent Work, and Future Improvements",confirmed,,Maintainer track,High Performance Computing,2023-05-14T20:26:00.747396+00:00,,"This maintainer track aims to lead discussions about the current needs for sparse data in the scientific python Ecosystem. It will present achievements and pursuit of the work initiated in the first Scientific Python Developer Summit, which took from 22nd May to 28th May 2023.","Sparse data refers to datasets where a high percentage of the values are zero or empty. Sparse arrays are one possible data structure for efficiently handling such datasets. Sparse Matrices from SciPy have been existing and have been used extensively within the scientific python ecosystem since its beginning.

While those foundational representations are still relevant for most use cases, edge cases and recent downstream libraries' needs remain to be considered. Moreover, the generalization of sparse matrices to sparse arrays comes with an important refactoring whose changes impact existing workflow and historical decisions and implementations.
 
This maintainer track aims to lead discussions about the current needs for sparse data in the scientific python Ecosystem. It will present achievements and pursuit of the work initiated in the first Scientific Python Developer Summit, which took from 22nd May to 28th May 2023.","For some information, the reader can refer to the following material:

 - https://scientific-python.org/summits/sparse/
 - https://github.com/scientific-python/summit-2023/issues/2

More information will be available after the end of the first Scientific Python Developer Summit (i.e. 28th May 2023).",,45,1,en,False,False,,PHNY9R,Julien Jerphanion,HS 119 - Maintainer track,2023-08-16T11:30:00+00:00,2023-08-16T14:15:00+02:00,2.00,2.0,,"Sparse Data in the Scientific Python Ecosystem: Current Needs, Recent Work, and Future Improvements",Vector and Array Manipulation,,,,,some,some,no,https://scientific-python.org/summits/sparse/,
98ZVJH,"Build Drug Discovery web applications with PyScript, Ketcher and rdkit",confirmed,,Talk (15 mins + Q&A),Scientific Applications,2023-05-09T08:58:44.256606+00:00,,"So you don't know JavaScript but know how to use python? Do you want to build an app where you can draw molecules for some application like properties prediction? Then come to this talk where I'll show you how to use Ketcher, EPAM tool for small molecule drawing, PyScirpt and rdkit for your next drug discovery app.","In this talk, the speaker will demonstrate how to build an app using Ketcher, a tool for small molecule drawing, PyScript, and RDKit for drug discovery applications. Even if you don't know JavaScript, you can still learn how to create an app for predicting molecular properties by following this tutorial. With Ketcher's intuitive interface, you can draw small molecules easily, while PyScript provides the backend programming capabilities. RDKit is a powerful toolkit for working with chemical structures and can be used to predict molecular properties. By the end of this talk, you will have the skills necessary to develop a drug discovery app that could be used to predicts the properties of molecules. This talk is ideal for those interested in drug discovery, molecular modeling, and computational chemistry, as well as those who are comfortable with Python programming language.","My talk will be based on this blog post that I made half a year ago after PyCon sweden. https://churnikov.github.io/blog/pyscript-demo-with-molecules/

In addition to what is already present I want to add a demonstration on how to work with some backend service for molecule properties prediction or something else. The point is to show that it's possible to make a proper drug discovery app with just python.",,20,1,en,True,False,,LXQDTL,Nikita Churikov,HS 120,2023-08-17T08:30:00+00:00,2023-08-17T10:50:00+02:00,2.00,2.0,,"Want to build an app to predict molecule properties but don't know JavaScript? Learn how to with Ketcher, PyScript, and RDKit at my talk! #DrugDiscovery #MolecularModeling #PythonProgramming",,,,Drug Discovery,,some,some,yes,https://churnikov.github.io/blog/pyscript-demo-with-molecules/,https://churnikov.github.io/demo/pyketch/index.html
9VVRFQ,Developing pandas extensions in Rust,confirmed,,Tutorial,High Performance Computing,2023-07-26T05:31:35.791599+00:00,,"pandas is a batteries included dataframe library, implementing hundreds of generic operations for tabular data, such as math or string operations, aggregations and window functions... In some case, domain specific code may benefit from user defined functions (UDFs) that implement some particular logic. These functions can sometimes be implemented using more basic pandas vectorized operations, and they will be reasonably fast, but in some others a Python function working with the individual values needs to be implemented, and those will execute orders of magnitude slower than their equivalent vectorized  versions. In this tutorial we will see how to implement functions in Rust that can be used with dataframe values at the individual level, but run at the speed of vectorized code, and in some cases faster.","While this tutorial will cover complex topics of low level programming languages like Rust, it'll be presented for a beginner audience. No previous knowledge about Rust is required, or any other knowledge other than basic pandas understanding is needed to follow the tutorial.

The tutorial will cover how libraries developed in a low level programming language like Rust can be called from Python, the basics of the internal representation of pandas dataframes, the Apache Arrow C data interface, and how to write a simple function in Rust.

To be able to follow the hands on part of this tutorial, participants should bring their laptops and have a working Python with a recent version od pandas and PyArrow, and have a Rust compiler.",,,90,1,en,False,False,,SXWKLX,Marc Garcia,Aula,2023-08-14T11:30:00+00:00,2023-08-14T15:00:00+02:00,,,,Learn how to develop a high performant pandas extension in Rust from pandas core developer @datapythonista.,Vector and Array Manipulation,Learning and Teaching Scientific Python,Supervised Learning,Other,Data Analysis and Data Engineering,none,some,,,
A3EMMC,Python versioning in a changing world,confirmed,,Talk (15 mins + Q&A),Scientific Applications,2023-04-30T20:09:39.532269+00:00,,"Python versioning is a critical aspect of maintaining a consistent ecosystem of packages, yet it can be challenging to get right. In this talk, we will explore the difficulties of Python versioning, including the need for upper bounds, and discuss mitigation strategies such as lockfiles in the Python packaging ecosystem (pip, poetry, and conda / mamba). We will also highlight a new community effort to analyze Python libraries dynamically and statically to detect the symbols (or libraries) they are using. By analyzing symbol usage, we can predict when package combinations will start breaking with each other, achieving a high rate of correct predictions. Our goal is to gather more community inputs to create a robust compatibility matrix. Additionally, we are doing similar work in C/C++ using libabigail to address ABI problems.","Python versioning is crucial for ensuring compatibility between different packages, but it can be challenging to get right. In this talk, we will discuss the challenges of Python versioning and present mitigation strategies in different Python packaging systems (lockfiles in poetry and conda-lock, repodata patching in the conda-forge ecosystem). We then introduce a new community effort to analyze Python libraries dynamically and statically to detect the symbols (or libraries) they are using. 
By analyzing symbol usage, we can predict when package combinations will start breaking with each other, achieving a high rate of correct predictions. For this we are using the `caliper` tooling.

Our approach relies on building a compatibility matrix that takes into account the usage of symbols in Python libraries. We will discuss how this matrix can be used to help developers make informed decisions about which package combinations to use to avoid compatibility issues. We will also present some preliminary results showing the effectiveness of our approach.

In addition to Python, we are also working on similar efforts for C/C++ using libabigail to address ABI problems. We believe that this approach will be useful for developers across a wide range of industries and projects, and we are eager to gather more community inputs to further improve the accuracy of our compatibility matrix.

Overall, this talk will be of interest to anyone working with Python or C/C++ who wants to ensure compatibility between different packages and maintain a consistent ecosystem of tools and libraries. We look forward to sharing our work with the SciPy community and hearing your feedback and suggestions.",,,20,1,en,False,False,,M7CWJZ,Wolf Vollprecht,HS 120,2023-08-17T12:40:00+00:00,2023-08-17T15:00:00+02:00,2.00,1.6,,Predicting Python package compatibility with static analysis - first results!,,,,Other,,some,some,no,,https://github.com/vsoch/caliper
A8BJLV,"PyBAMM, an open-source Python package for battery simulation, contributes to a solid understanding of lithium-ion batteries degradation",confirmed,,Poster,Scientific Applications,2023-04-26T08:19:42.023875+00:00,,"Being able to predict the degradation (capacity and power fade) of lithium-ion batteries is critical for the procurement and asset management of batteries. Often, this exercise is undertaken using empirical or semi-empirical models, which blend out the degradation mechanisms in favour or computational speed and simplicity.

By using PyBaMM (an open source package for battery simulation), we build bottom-up understanding of the discrete ageing mechanisms that impact battery State-of-Health indicators. In addition, PyBaMM comes complete with validated sets of parameters for different chemistries, which accelerates the deployment of the model.","Battery energy storage systems play an important role in reducing carbon emissions by electrification. Accordingly, being able to perform simulations of battery systems (deployed in stationary or mobility applications) is fundamental. Understanding the electrochemical principles of batteries (including phenomena leading to degradation or ageing) is critical for optimizing the selection and procurement of these battery systems as well as for optimizing the operation of battery fleets at different time scales. 

PyBamm is an open-source Python-based battery simulation model (with a focus on lithium-ion chemistries) which implements various electrochemistry-based battery models such as Doyle-Fuller-Newman and covers degradation mechanisms.  The task of calibrating the input parameters required for an electrochemical model ist complicated beyond the scope of the current work. In this aspect, PyBaMM comes complete with validated parameter sets from literature.

In this presentation, we focus on comparing and integrating the results from a semi-empirical battery simulation model to the results obtained from PyBaMM. We argue in favour of cross-validating results: Bottom-up understanding of the phenomena simulated in a detailed PyBaMM model can reinforce the certainty -or lack thereof- from empirical models often used by engineers and consultants in large battery projects.",,,90,1,en,False,False,,QFBU3Z,Maria Kaninia,,,,1.50,1.5,,Python-PyBaMM used for simulating lithium-ion batteries with a concrete view of understanding degradation phenomena,,,,"Simulations (e.g., Physics, CFD, ESMs)",,none,some,yes,,
AHXAXM,Introduction to matplotlib for visualization in Python,confirmed,,Tutorial,Data Science and Visualisation,2023-06-27T18:52:49.488801+00:00,,This tutorial explains the fundamental ideas and concepts of matplotlib. It's suited for complete beginners to get started as well as existing users who want to improve their plotting abilities and learn about best practices.,"Matplotlib is one of the most-used and powerful visualization libraries for python. Nevertheless, there has been and still is some confusion on how use it properly. This has a number of reasons ranging from an evolution of the API and lack of good documentation to the complexity that comes with the large feature set and flexibility. But these issues can be overcome.

This tutorial will explain the main concepts and intended usage patterns of matplotlib. Knowing these, lets you effectively use high-level functions for most of the cases. But you will be able to go into the details if you need to fine-tune certain aspects of the plot. We'll also touch some nowadays discouraged ways of working from the past (you should know what not to do - even though that's still found in lots of examples on the web) and we may get a glimpse into the future.",,,90,1,en,False,False,,X8YQJR,Tim Hoffmann,HS 120,2023-08-15T06:30:00+00:00,2023-08-15T10:00:00+02:00,,,,Introduction to matplotlib for visualization in Python,Parallel Computing,Learning and Teaching Scientific Python,Supervised Learning,Astronomy,Data Analysis and Data Engineering,none,some,,,
ALH9X8,scicode-widgets: A toolkit to bring computational thinking into the classroom,confirmed,,Poster,"Community, Education, and Outreach",2023-04-30T14:50:57.412602+00:00,,"We introduce scicode-widgets, a Python package designed for educational purposes that facilitates the creation of interactive exercises for students in interdisciplinary fields of computational sciences. The implementation of computational experiments often demands extensive coding, hindering students' ability to effectively learn the interwork of coding experiments and analyzing their results. To reduce this workload for students, instructors can already provide a codebase and demand educational contributions from students. These contributions have to be embedded into a general workflow that involves coding experiments and analyzing their results. For that task scicode-widget provides the tools to connect custom pre- and postprocessing of students’ code, with the ability to verify the solution and to pass it to interactive visualizations driven by jupyter widgets. In the talk we demonstrate an example from a materials science class for teaching atomistic modeling and discuss a potential direction to integrate with nbgrader.","We introduce scicode-widgets, a Python package designed for educational purposes that facilitates the creation of interactive exercises for students in interdisciplinary fields of computational sciences. The implementation of computational experiments often demands extensive coding, hindering students' ability to effectively learn the interwork of coding experiments and analyzing their results. To reduce this workload for students, instructors can already provide a codebase and demand educational contributions from students. These contributions have to be embedded into a general workflow that involves coding experiments and analyzing their results. For that task scicode-widget provides the tools to connect custom pre- and postprocessing of students’ code, with the ability to verify the solution and to pass it to interactive visualizations driven by jupyter widgets. In the talk we demonstrate an example from a materials science class for teaching atomistic modeling and discuss a potential direction to integrate with nbgrader.",,,90,1,en,False,False,,YHHDDK,Alexander Goscinski,,,,1.00,0.8,,scicode-widgets: a Python package designed to facilitate the creation of interactive exercises for students in interdisciplinary fields of computational sciences #computationalthinking,,Learning and Teaching Scientific Python,,,,none,some,yes,https://github.com/ceriottm/iam-notebooks,https://github.com/osscar-org/scicode-widgets
AQUCLV,Introduction to scikit-learn,confirmed,,Tutorial,Machine and Deep Learning,2023-06-27T19:03:01.341862+00:00,,This tutorial will provide a beginner introduction to scikit-learn. Scikit-learn is a Python package for machine learning. We will talk about what Machine Learning is and how scikit-learn can implement it. In the practical part we will learn how to create a predictive modelling pipeline and how to fine tune its hyperparameters to improve the model's score.,"### Workshop Outline

<ul>
  <li>Machine Learning 101 (10 min.)</li>
  <li>What is scikit-learn? (5 min.)</li>
  <li>Practical Part (+60 min.)
    <ul>
      <li>Predictive modeling pipeline</li>
      <li>Evaluation of models</li>
      <li>Hyperparameters tuning</li>
    </ul>
  </li>
</ul> 

### Description
We will start with covering the main ideas behind Machine Learning and we introduce scikit-learn as a machine learning library.  There will be plenty of room to ask questions.

The practical part of his tutorial will be subdivided into three parts. First, we will present how to design a predictive modeling pipeline that deals with heterogeneous types of data. Then, we will go more into detail in the evaluation of models and the type of trade-offs to consider. Finally, we will show how to tune the hyperparameters of the pipeline. 

You are encouraged to code along with me.

### Prerequisites
This workshop will serve you best when you have some basic knowledge of Python and know how to use a Jupyter Notebook. We will start from an empty notebook and add cells at every step.

Bring your laptop.

Have a virtual environment with numpy, pandas and scikit-learn installed.",,,90,1,en,False,True,,G3NKSV,Stefanie Sabine Senger,HS 120,2023-08-15T08:30:00+00:00,2023-08-15T12:00:00+02:00,,,,Introduction to machine learning using scikit-learn,Scalability,Learning and Teaching Scientific Python,Supervised Learning,Other,Data Analysis and Data Engineering,none,some,,https://inria.github.io/scikit-learn-mooc/,
ARBBQF,"Let’s exploit pickle, and `skops` to the rescue!",confirmed,,Talk (25 mins + Q&A),Machine and Deep Learning,2023-05-16T09:35:08.901578+00:00,,"Pickle files can be evil and simply loading them can run arbitrary code on your system. This talk presents why that is, how it can be exploited, and how `skops` is tackling the issue for scikit-learn/statistical ML models. We go through some lower level pickle related machinery, and go in detail how the new format works.","The pickle format has many vulnerabilities and loading them alone can run arbitrary code on the user’s system [1]. In this session we go through the process used by the pickle module to persist python objects, while demonstrating how they can be exploited. We go through how `__getstate__` and `__setstate__` are used, and how the output of a `__reduce__` method is used to reconstruct an object, and how one can have a malicious implementation of these methods to create a malicious pickle file without knowing how to manually create a pickle file by manipulating a file on a lower level. We also briefly touch on other known exploits and issues related to the format [2]. 

We also show how one can look inside a pickle file and the operations run by it while loading it, and how one could get an equivalent python script which would result in the output of the pickle file [3]
Then I present an alternative format from the `skops` library [4] which can be used to store scikit-learn based models. We talk about what the format is, and how persistence and loading is done, and what we do to prevent loading malicious objects or to avoid running arbitrary code. This format can be used to store almost any scikit-learn estimator, as well as xgboost, lightgbm, and catboost models.

- [1] https://peps.python.org/pep-0307/#security-issues
- [2] https://github.com/moreati/pickle-fuzz
- [3] https://github.com/trailofbits/fickling
- [4] https://skops.readthedocs.io/en/stable/persistence.html",,,30,1,en,True,False,,HGSWKF,Adrin Jalali,Aula,2023-08-17T12:05:00+00:00,2023-08-17T14:35:00+02:00,2.00,1.8,,"Let’s exploit pickle, and `skops` to the rescue! Why pickle is dangerous an how to mitigate some of the issues.",,,Other,,,some,some,no,,https://skops.readthedocs.io/en/stable/
BYFB3Y,Ibis: Because SQL is everywhere but you don't want to use it,confirmed,,Talk (25 mins + Q&A),Data Science and Visualisation,2023-04-30T15:01:19.480589+00:00,,"We love to use Python in our day jobs, but that enterprise database you run your ETL job against may have other ideas. It probably speaks SQL, because SQL is ubiquitous, it’s been around for a while, it’s standardized, and it’s concise.
But is it really standardized? And is it always concise? No!

Do we still need to use it? Probably!

What’s a data-person to do? String-templated SQL?
print(f”That way lies {{ m̴͕̰̻̏́ͅa̸̟̜͉͑d̵̨̫̑n̵̖̲̒͑̾e̸̘̼̭͌s̵͇̖̜̽s̸̢̲̖͗͌̏̊͜ }}”.)

Instead, come and learn about Ibis! It offers a dataframe-like interface to construct concise and composable queries and then executes them against a wide variety of backends (Postgres, DuckDB, Spark, Snowflake, BigQuery, you name it.).","Ibis is a pure Python library that lets you write Python to build up expressions that can be executed on a wide array of backends (SQLite, DuckDB, Postgres, Spark, Clickhouse, Snowflake, BigQuery, and more!). It offers a dataframe-like interface and helps you to write concise and composable interactive analytics code.

Have you ever had to translate a proof-of-concept from Pandas to PySpark to run on the “real data”?

Or download a huge parquet file because the upstream data is the result of 500 lines of dense SQL and you’re afraid to mess with it?

Have a love/hate relationship with SQL because it lets you get your job done, but think, there must be a better way?

Well, if you’re a data-engineer, data-scientist, data-hobbyist, or data-anything, come and join us for a tour of what Ibis can do for you!",,,30,1,en,False,False,,"WUEKGF, YE9YBK","Gil Forsyth, Phillip Cloud",Aula,2023-08-16T08:30:00+00:00,2023-08-16T11:00:00+02:00,2.00,2.0,,Ibis: Because SQL is everywhere but you don't want to use it,,,,,Data Analysis and Data Engineering,some,some,no,https://ibis-project.org,https://github.com/ibis-project/ibis
CB9WMH,Interoperability in the Scientific Python Ecosystem,confirmed,,Maintainer track long,Scientific Applications,2023-07-08T14:17:12.853193+00:00,,"This slot will cover the effort regarding interoperability in the scientific Python ecosystem. Three topics will be covered during this track

- Using the Array API for array-producing and array-consuming libraries
- Apache Arrow: connecting and accelerating dataframe libraries across the PyData ecosystem
- DataFrame interchange and namespace APIs
- Entry Points: Enabling backends and plugins for your libraries

### Using the Array API for array-producing and array-consuming libraries

Already using the Array API or wondering if you should in a project you maintain? Join this maintainer track session to share your experience and exchange knowledge and tips around building array libraries that implement the standard or libraries that consume arrays.

### Apache Arrow: connecting and accelerating dataframe libraries across the PyData ecosystem

Apache Arrow is a multi-language toolbox for accelerated data interchange and in-memory processing, and is becoming the de facto standard for tabular data. This talk will give an overview of the recent developments both in Apache Arrow itself as how it is being adopted in the PyData ecosystem (and beyond) and can improve your day-to-day data analytics workflows.

### Entry Points: Enabling backends and plugins for your libraries

In this talk, we will discuss how NetworkX (a python library for graph theory) used entry points to enable more efficient computation backends like GraphBLAS to plug into NetworkX","### Using the Array API for array-producing and array-consuming libraries

This session is for maintainers of projects that either implement the Array API (Numpy, cupy, pytorch, etc) or projects that use Array API inputs (scikit-learn, scipy, etc). Or maybe you are wondering if you should start investing in Array API for your project.

The Array API standard aims to specify a common API for multidimensional arrays. Solving the problem of subtle API differences between the many array libraries that exist. This means it provides a minimum set of functions and behaviours for array libraries to implement. As a result array consuming libraries do not need code to handle the slight differences, they can rely on these functions and behaviours to exist and be standard compliant in all array libraries.

The Array API standard is not yet in widespread use. Adoption across the ecosystem has only just started.

This session is a place to discuss and share your experiences in using the Array API standard for your array library or your library that consumes arrays.

### Apache Arrow: connecting and accelerating dataframe libraries across the PyData ecosystem

The Apache Arrow (https://arrow.apache.org/) project specifies a standardized language-independent columnar memory format for tabular data. It enables shared computational libraries, zero-copy shared memory, efficient (inter-process) communication without serialization overhead, etc. Nowadays, Apache Arrow is supported by many programming languages and projects, and is becoming the de facto standard for tabular data.

But what does that mean in practice? There is a growing set of tools in the Python bindings, PyArrow, and a growing number of projects that use (Py)Arrow to accelerate data interchange and actual data processing. This talk will give an overview of the recent developments both in Apache Arrow itself as how it is being adopted in the PyData ecosystem (and beyond) and can improve your day-to-day data analytics workflows.

### Entry Points: Enabling backends and plugins for your libraries

As a maintainer of an open source library, you always need to wrestle with the fact that there are new experimental things which could really help your users, but it may just be too experimental for now (especially for old projects).

There are always questions like:

-   If we add this new requirement, we can use the new X feature, but now we depend on that library.
-   A new change will help a lot of users, but this will utterly destroy all the code written using the library in the last 20 years.
-    A new fork comes out, splits the community.
-   You don’t want to write and maintain C/Rust/FORTRAN and want to ship that bit to other packages.

and many more!

With this talk, we would explore an option of providing your user community a workflow to develop plugins directly for your package.

We will look at an example case study from NetworkX, specifically using the entry points mechanism for plugin discovery. In NetworkX these plugins are currently used to swap in the computation bits.

-    (2 minutes) Quick introduction about NetworkX and GraphBLAS
-    (3 minutes) Quick Introduction about entry_points
-    (5 minutes) Use NetworkX API - but get the speed of GraphBLAS
-    (5 minutes) Demo-ing the plugin mechanism and implementation details",,,90,1,en,False,False,,"7VUXWM, G9FDBT, NEUMLP, LPYNCD, UAM73R, QKVYNA","Joris Van den Bossche, Tim Head, Olivier Grisel, Franck Charras, Mridul Seth, Sebastian Berg",HS 119 - Maintainer track,2023-08-17T08:30:00+00:00,2023-08-17T12:00:00+02:00,,,,Interoperability in the Scientific Python Ecosystem,Parallel Computing,Learning and Teaching Scientific Python,Supervised Learning,Astronomy,Data Analysis and Data Engineering,none,some,,,
CJCV9W,"Solara: A Pure Python, React-style Framework for Scaling Your Data Apps",confirmed,,Talk (25 mins + Q&A),Data Science and Visualisation,2023-05-14T20:57:42.808350+00:00,,"Solara is a pure Python web framework designed to scale complex applications. Leveraging a React-like API, Solara offers the scalability, component-based coding, and simple state management that have made React a standard for large web applications. Solara uses a pure Python implementation of React, Reacton, to create ipywidgets-based applications that work both in the Jupyter Notebook environment and as standalone web apps with frameworks like FastAPI. This talk will explore the design principles of Solara, illustrate its potential with case studies and live examples, and provide resources for attendees to incorporate Solara into their own projects. Whether you're a researcher developing interactive visualizations or a data scientist building complex web applications, Solara provides a Python-centric solution for scaling your projects effectively.","Python's growing prominence in the scientific community has led to a demand for more sophisticated and scalable web applications. While several Python web frameworks exist, most are designed for smaller data applications or employ paradigms that haven't proven their scalability for larger projects.

In this talk, we introduce Solara, a pure Python web framework developed to address this gap. Solara employs a React-like API, leveraging the scalability, component-based code, and simple state management principles that have made React a go-to for large-scale web applications.

Our framework uses a pure Python implementation of React, named Reacton, to create applications based on ipywidgets. These applications function both within the Jupyter Notebook environment and as standalone web apps in conjunction with frameworks like FastAPI. By building atop ipywidgets, we harness an existing ecosystem of widgets and support a range of platforms, including JupyterLab, Jupyter Notebook, Voila, Google Colab, DataBricks, and JetBrains Datalore.

We'll explore the design principles of Solara, the benefits of a React-like approach in Python, and demonstrate its potential through case studies and live examples. Our goal is to showcase how Solara can help the Python scientific community develop more robust and scalable web applications, thereby expanding the reach and potential of their work.

Attendees will leave with an understanding of Solara's capabilities and potential applications, as well as resources to begin exploring its utility in their own projects. Whether you're a researcher seeking to develop interactive visualizations or a data scientist looking to build complex web applications, Solara offers a Python-centric solution to scale your projects effectively.

 * https://github.com/widgetti/solara/
 * https://solara.dev",,,30,1,en,True,False,,38EYUA,Maarten Breddels,HS 120,2023-08-16T12:05:00+00:00,2023-08-16T14:35:00+02:00,2.00,2.0,,"Solara: A Pure Python, React-style Frame work for Scaling Your Web Apps. Designed for complex apps, Solara offers simple state management & component based UIs",,,,,Data Analysis and Data Engineering,none,some,no,https://solara.dev,https://github.com/widgetti/solara/
EX3C7K,Introduction to NumPy,confirmed,,Tutorial,Scientific Applications,2023-06-27T17:43:29.846466+00:00,,"NumPy is one of the foundational packages for doing data science with Python. It enables numerical computing by providing powerful N-dimensional arrays and a suite of numerical computing tools. In this tutorial, you'll be introduced to NumPy arrays and learn how to create and manipulate them. Then, you'll see some of the tools that NumPy provides, including random number generators and linear algebra routines.","This tutorial will introduce numerical computing with Python and the NumPy library. It's intended for people new to NumPy and Python's scientific stack or those needing a refresher. 

In this tutorial, you'll learn about different ways to create NumPy arrays. You'll see how to pull out individual elements or sub-arrays from them. Even more importantly, you'll learn about broadcasting and vectorization. These are NumPy's superpowers that help you write readable and fast code.

NumPy's arrays are the foundational building blocks. However, you'll also dip your toes into some of the numerical computing tools that are part of the library. In particular, you'll learn the best practices for working with random numbers in NumPy. Furthermore, you'll see how to perform linear algebra operations like matrix addition, multiplication, and inversion.

The workshop consists of 90 minutes of live code demonstrations and hands-on exercises.",,,90,1,en,False,False,,CSTAZX,Geir Arne Hjelle,HS 120,2023-08-14T11:30:00+00:00,2023-08-14T15:00:00+02:00,,,,"Learn the basics of NumPy, Python's foundational library for numerical computing",Parallel Computing,Learning and Teaching Scientific Python,Supervised Learning,Astronomy,Data Analysis and Data Engineering,none,some,,,
EXVUBJ,MyST & Thebe: Community-driven tools for awesome open science communication with Jupyter[lite] backed computation,confirmed,,Talk (25 mins + Q&A),"Community, Education, and Outreach",2023-05-14T13:24:27.791558+00:00,,"Imagine a world where there are tools allowing any researcher to easily produce high quality scientific websites. Where it's trivial to include rich interactive figures that connect to Jupyter servers or run in-browser with `WASM` & `pyodide`, all from a local folder of markdown files and Jupyter notebooks.

We introduce MyST Markdown (https://mystmd.org/), a set of open-source, community-driven tools designed for open scientific communication. 

It's a powerful authoring framework that supports blogs, online books, scientific papers, preprints, reports and journals articles. It includes `thebe` a minimal connector library for Jupyter, and `thebe-lite` that bundles a JupyterLite server with `pyodide` into any web page for in-browser `python`. It also provides publication-ready tex and pdf generation from the same content base, minimising the rework of publishing to the web and traditional services.","The MyST (Markedly Structured Text) project grew out of the ExecutableBooks team, developers of MyST Markdown and JupyterBook. Originally based on Sphinx and RST, over the past year the ExecutableBooks team has been working on a MyST Specification to coordinate development of the markup language and extensions across multiple languages & parsers (e.g. implementations in Python & Javascript). 

The new javascript based MyST tools run directly in the browser, opening up new workflows for components to be used in web-based editors, directly in Jupyter and in JupyterLite. The libraries work with current MyST Markdown documents/projects and can export to LaTeX/PDF, Microsoft Word and JATS as well as multiple website templates using a modern React-based renderer. There are currently over 400 scientific journals that are supported through templates, with new LaTeX templates that can be added easily (using Jinja-based templating) and contributed via an open community repository (https://github.com/myst-templates).

Thebe lets you easily add interactive visualisations, reproducible figures and interactive code editors to any to static HTML web page -- backed by a kernel from a Jupyter server or an in-browser WASM kernel with thebe-lite.

It’s a compact and versatile library that makes it easy to add Jupyter based interactivity, by default code blocks are turned into editors and made ready for execution, just by adding a couple of additional script tags. `thebe` enables the flexible interleaving of Jupyter cells and output areas with other content (e.g. from myst markdown files), maintaining the ability to run the underlying notebooks or individual code cells. 

Over the last year, `thebe` has seen major upgrades allowing it to be integrated into modern web frameworks and it have been integrated tightly with MyST's web themes, making it easy to add computation into any MyST based online article or publication. Now creating and deploying an interactive `ipywidgets` based figure as part of an online paper can be achieved in minutes.

And while we've seen interactive figures in web based scientific papers before, the MyST toolchain democratizes their creation and makes it easy for any researcher to publish publication-quality web-based articles  from their desktop (or from CI) that the large online journals would need a post-production  web development teams to help deliver. MyST's architecture also allows for multiple researchers to contribute to lab websites and computational journals.

In our presentation we will give an overview of the MyST ecosystem, how to use MyST tools in conjunction with existing Jupyter Notebooks, markdown documents, and JupyterBooks to create interactive websites, books, blogs and scientific articles as well as professional PDFs. We give special attention to the additions around structured data, standards in publishing (e.g. efforts in representing Notebooks as JATS XML), rich frontmatter, bringing cross-references and persistent IDs to life with interactive hover-tooltips and making connections to Jupyter based and in-browser `python` kernels to run interactive figures through the additional of a few simple configuration options. We'll share some compelling examples of online papers and journals published with MyST.

Our presentation is aimed at attendees who are looking to incorporate Jupyter Notebooks with other materials in new and novel ways - to create compelling scientific communication materials whether in the form of books, blogs or articles, for education or research. The talk will cover how and where `thebe` can be applied effectively, as well as going into some different configurations. Some knowledge of basic web development with HTML, will be beneficial for walk-through element of the talk as we'll cover some code and configuration, but we aim for the talk to be accessible by anyone interested in putting interactive scientific communication on the web, whether they develop themselves or not.","We expect the talk **to be of significant interest to researchers, scientists and RSE's** with interest in progressing their work in **open science and the proliferation of open access science communication**, the open source tool chains we'll cover are a *huge enabler* for researchers in this regard.

While we've all seen interactive figures in web based scientific papers for some time, **the MyST toolchain democratizes this** and makes it easy for any researcher, RSE or scientist working in python & jupyter to publish from their desktop a **publication quality** web based article, that a journal would need a post production team and web developers to deliver and deploy.

Although this talk may be quite different from others that you are considering but we think **it is important, timely and very relevant for the scientific python community** -- tools for publishing and communicating are essential under today's open science agenda, so we felt that it fits quite naturally into this track.

We've asked for a 30 min talk but are able to accept and compress to a 15 minute talk if needs be, although this will be a bit limiting, we can still deliver a valuable talk in that time and provide attendees with some entry points as well as demonstrations.

We very much appreciate the chance to share this work.",,30,1,en,True,False,,"EHAJXJ, 9ENEUE","Steve Purves, Rowan Cockett",HS 120,2023-08-17T12:05:00+00:00,2023-08-17T14:35:00+02:00,2.00,1.8,https://pretalx.com/media/euroscipy-2023/submissions/EXVUBJ/resources/myst-jupytercon_Uv8rIpu_gQDKbDD.png,"Transform your science communication with MyST! Create interactive computational scientific websites, blogs, preprints & articles with an open-source, community-driven toolset. Dive in: https://mystmd.org/",,Other,,,,some,some,no,https://myst-tools.org,https://github.com/executablebooks
FQSXE7,Molgri: grids for molecular dynamics,confirmed,,Poster,Scientific Applications,2023-04-05T10:26:13.198088+00:00,,"Computational chemistry studies how molecules approach and interact at atomic precision and femtosecond timescales, a level of detail that is difficult to achieve in experiments. This enables us to understand important processes like the binding of drugs to receptors. 

However, especially the first phase of binding – the approach and guidance towards a binding site - is difficult to study with the usual molecular dynamics approach. In the python package molgri, we instead create two sets of grids to study binding pathways. A grid of approach directions is generated based on a subdivision of an icosahedron; a grid of approach orientations based on a division of a half-hypercube. 

After the user provides two molecular structures and the desired level of discretization, the molgri package generates a sequence of structures (a pseudo-trajectory) featuring all combinations of approach directions, distances and orientations. The output is immediately compatible with chemical software like GROMACS and openMM, so that this grid-based approach can be used as a starting point for simulations or used on its own to study long-range interactions guiding ligands to binding sites and to extract simulation-free models of kinetics (rate matrix).

The ease of use is taken into account during development; the generation of pseudo-trajectories can be easily done as a single command-line instruction, with examples provided in documentation.","Computational chemistry studies the mechanisms of molecular interactions at the level of atomic precision that is hardly ever achievable in experiments. However, especially for large biomolecules with complex structures, it is difficult to predict how they will interact with each other and with smaller particles like drug molecules – especially in the long-range part of the process, when the particles are only beginning to approach each other. It is often assumed that there are guiding interactions orienting one molecule correctly towards the other, but the process is difficult to study with typical molecular dynamics methods since the approach pathways occur in a vast, highly-dimensional space with only a few narrow energetic minima, making the computational time needed to come across all of them prohibitively long.

To close this gap in computational chemistry research, we developed the python package molgri that quickly and easily provides the user with a uniform selection of approach vectors and orientations of a pair of molecules. After comparing several generation algorithms, we focused on the two with the most desirable properties, generating approach vectors with an incremental subdivision of a projected icosahedron and approach orientations (in quaternion framework) with an incremental subdivision of a projected half-hypercube.

This product of approach vectors and orientations at any level of coarseness is then automatically applied to any two molecular structures, generating a sequence of combination structures (pseudo-trajectories) in a format immediately compatible with programs like GROMACS or openMM. This means that the proposed structures can be immediately evaluated within a force field or optimized further with the software that is already widely used in the field. The resulting pattern of energetically favorable and unfavorable areas in space enables the user to visualize how the binding process may have proceeded and what type of interaction played the role in the process.

The package molgri is under active development as an open-source project on GitHub, with the last stable version always released as a python package. We focus on a user-friendly interface, providing several scripts within the installation that enable end users the generation of grids, pseudo-trajectories and visualizations with single-line commands; for advanced users, well-maintained documentation is published online via ReadTheDocs. Currently, we are working to extend the functionality of the module to simulation-free models of molecular kinetics (square-root approximation, rate matrix) and the use of maze algorithms like Dijkstra’s algorithm to identify distinct approach pathways.",,,90,1,en,False,False,,MM88QK,Hana Zupan,,,,1.00,1.2,,"Interested in studying optimal ways in which (bio)molecules approach and bind? Easily generate possible binding orientations and approach pathways with python package molgri, the long-range complement to molecular dynamics simulations.",,,,Drug Discovery,,some,none,yes,https://molgri.readthedocs.io/en/latest/,https://github.com/bkellerlab/molecularRotationalGrids
FRHXYW,(in)Complete introduction to AI Safety,confirmed,,Talk (25 mins + Q&A),Machine and Deep Learning,2023-04-29T23:51:24.657207+00:00,,"AI is poised to be ""Our final invention,"" either the key to a never-ending utopia or a direct road to dystopia (or apocalypse). Even without the eschatological framing, it's still a revolutionary technology increasingly embedded in every aspect of our life, from smartphones to smart cities, from autonomous agents to autonomous weapons. In the face of acceleration, there can be no delay: if we want AI to shape a better tomorrow, we must discuss safety today.","AI is our generation's most important technological breakthrough; beyond all the discussion about hype and Doom lie serious safety, technical and ethical considerations. In the face of accelerating AI capabilities, if we want to create a better world, we must also accelerate our safety efforts, exploring ethics and biases, deep learning failures and their alignment implications, or AI and computing policies.

This talk aims at providing a brief introduction and overview of the principal axes of AI safety: Ethics, Alignment, and Policies). Hopefully, it will work as a gateway for further forays into these crucial and intertwined areas; references and conceptual maps will accompany the talk's slides.

The talk will be roughly structured like this (sections and subsections may vary, but a higher focus will be put on Alignment):

- Risks: From misuse to Doom
- Alignment:
  - Black Boxes: Interpretability or the lack thereof
  - Mesa-Optimizer and Reward Hacking
  - Of RLHF and Waluigis: on the brittleness of LLMs
- Policies: Can we Regulate A(G)I?
- Ethics: breaking the vicious cycle of unfair models, datasets, and societies.","The final structure of the talk might vary as the field is in a state of accelerating development; the focus will most likely be on Alignment and Risks, while Ethics and Policies section will probably be kept at 5m each, mainly because Alignment is currently the hottest and more technically relevant topic amongst those three.",,30,1,en,False,False,,GAWBSA,"Michele ""Ubik"" De Simoni",Aula,2023-08-17T11:30:00+00:00,2023-08-17T14:00:00+02:00,1.00,1.2,,"Making safe, ethical, aligned AIs is the only way to avoid dystopia (or Doom) and build (hopefully) everlasting utopia. In the face of accelerating capabilities, we need to discuss safety now.",,,Algorithmic bias and Trustworthy AI,,,none,none,yes,,
FU9SYJ,Accelerating your Python code - a systematic overview,confirmed,,Talk (25 mins + Q&A),High Performance Computing,2023-04-30T20:31:52.295146+00:00,,"Python is slow. We feel the performance limitations when doing computationally intensive work. There are many libraries and methods to accelerate your computations, but which way to go? This talk serves as a navigation guide through the world of speeding up Python. At the end, you should have a high-level understanding of performance aspects and know which way to go when you want to speed up your code next time.","We start with the fundamental reasons why Python is slow by design - and why it's nevertheless often a good language choice. From there we'll cover basic Python programming paradigms, standard data libraries (NumPy, pandas), Just-in-time compilation (PyPy, numba), GPU-Acceleration, Multithreading, Multiprocessing, calling other languages (C/C++, Julia, Rust) as well as distributed computing. We'll discuss the benefits and costs of all these technologies, so that you know which way to go in different usage scenarios.",,,30,1,en,False,False,,X8YQJR,Tim Hoffmann,Aula,2023-08-16T12:05:00+00:00,2023-08-16T14:35:00+02:00,2.00,1.8,,Learn which ways to go in the jungle of libraries and technologies that can speed up your python code.,Other,,,,,none,some,no,,
GRHSGN,Streamlining Geospatial Machine Learning with SRAI,confirmed,,Poster,Scientific Applications,2023-05-14T21:11:59.510305+00:00,request as poster,"This talk will introduce the audience to the Spatial Representations for Artificial Intelligence (srai), a Python library that provides an efficient, unified, and easy-to-use approach to geospatial problems. Attendees will get insights into the key functionalities and use cases of srai with a focus on how it enhances the Python scientific stack and encourages the application of Python in scientific and industrial geospatial research.","In the GeoAI domain, tooling is scarce, datasets and models are often private or unavailable, and there is a lack of unified, user-friendly libraries for geospatial problem-solving. However, the srai library aims to address these challenges by providing comprehensive tools, algorithms, and seamless integration with existing frameworks. It enables researchers, developers, and data scientists to overcome the limitations of scarce tooling and private datasets, fostering collaboration and simplifying the implementation of geospatial solutions.

Outline:

1. Introduction - Relevance and challenges of working with GeoAI. Introduction of the srai library and its objectives.
2. Key Features of SRAI - Core functionalities of srai, including OSM data downloading and processing, GTFS processing, regionization, embedding, and visualization tools.
3. Example applications - Example applications of srai in real-world scenarios, such as predicting Bike Sharing System (BSS) stations' locations. How srai streamlines these use cases.
4. Future Developments and Conclusion - Plans for future library's functionalities, the development of geospatial AI tooling as a whole.",,,90,1,en,False,False,,"NEMRFE, DLQCAA, CEEVGF, AEKH3Q, PHTF7Q","Szymon Woźniak, Piotr Szymański, Piotr Gramacki, Kamil Raczycki, Kacper Leśniara",,,,2.00,2.0,,"Discover srai, the Python library redefining geospatial problem-solving! Dive into its key features, use cases and its game-changing role in scientific and industrial research. #Python #GeospatialAI #srai",,,"ML Applications (e.g. NLP, CV)",Geo Science,,some,some,yes,https://srai-lab.github.io/srai,https://github.com/srai-lab/srai
GSKSQJ,SciPy❤Java with GraalPy,confirmed,,Poster,Data Science and Visualisation,2023-04-06T09:05:46.664959+00:00,,"GraalPy brings up to date data science packages from Python to Java. Whether
analyzing data from Java enterprise applications or embedding Python
visualizations in Java's cross-platform UIs, GraalPy makes it easier than ever.
In this talk we will show you how to get GraalPy from Conda or Pyenv, how it
performs on scientific Python workloads, and how to easily set up a
multi-language application with Java, Pygal, and SciPy.","GraalPy, the fast, Java-based implementation of Python, is bridging modern
Python and the Java ecosystem. Whether you need to use NumPy's data
processing with data from Java, want to use Java's cross-platform UI libraries
to visualize your data, or Java's JDBC drivers to pull data into your Python
code. GraalPy has got you covered.

After 5 years of development, we are maturing and now testing hundreds of the
top Python packages every day to improve compatibility with the latest and
greatest of the Python ecosystem. GraalPy is available via standalone
installers, RPM packages, from Pyenv or Conda, as part of GraalVM
distributions.

In this talk we want to give you an overview of the state of GraalPy, show our
performance and compatibility data, and what can be expected to work. We will
also demonstrate how a GraalPy environment can be easily set up with Conda to
develop a multi-language application with Java, Pygal, and Scipy in PyCharm.",,,90,1,en,False,False,,"RB9MRK, VBDD98","Tim Felgentreff, Cosmin Basca",,,,1.00,1.0,,"Join us, as we demonstrate how setting up a GraalPy project to bring Python data science libraries and Java applications together is easier than ever!",,,,,Data Visualization,some,some,yes,,https://www.graalvm.org/python/
GYYTCH,Get the best from your scikit-learn classifier: trusted probabilties and optimal binary decision,confirmed,,Talk (25 mins + Q&A),Machine and Deep Learning,2023-05-14T10:55:27.481416+00:00,,"When operating a classifier in a production setting (i.e. predictive phase), practitioners are interested in potentially two different outputs: a ""hard"" decision used to leverage a business decision or/and a ""soft"" decision to get a confidence score linked to each potential decision (e.g. usually related to class probabilities).

Scikit-learn does not provide any flexibility to go from ""soft"" to ""hard"" predictions: it uses a cut-off point at a confidence score of 0.5 (or 0 when using `decision_function`) to get class labels. However, optimizing a classifier to get a confidence score close to the true probabilities (i.e. a calibrated classifier) does not guarantee to obtain accurate ""hard"" predictions using this heuristic. Reversely, training a classifier for an optimum ""hard"" prediction accuracy (with the cut-off constraint at 0.5) does not guarantee obtaining a calibrated classifier.

In this talk, we will present a new scikit-learn meta-estimator allowing us to get the best of the two worlds: a calibrated classifier providing optimum ""hard"" predictions. This meta-estimator will land in a future version of scikit-learn: https://github.com/scikit-learn/scikit-learn/pull/26120.

We will provide some insights regarding the way to obtain accurate probabilities and predictions and also illustrate how to use in practice this model on different use cases: cost-sensitive problems and imbalanced classification problems.","When operating a classifier in a production setting (i.e. predictive phase), practitioners are interested in potentially two different outputs: a ""hard"" decision used to leverage a business decision or/and a ""soft"" decision to get a confidence score linked to each potential decision (e.g. usually related to class probabilities).

Scikit-learn does not provide any flexibility to go from ""soft"" to ""hard"" predictions: it uses a cut-off point at a confidence score of 0.5 (or 0 when using `decision_function`) to get class labels. However, optimizing a classifier to get a confidence score close to the true probabilities (i.e. a calibrated classifier) does not guarantee to obtain accurate ""hard"" predictions using this heuristic. Reversely, training a classifier for an optimum ""hard"" prediction accuracy (with the cut-off constraint at 0.5) does not guarantee obtaining a calibrated classifier.

In this talk, we will present a new scikit-learn meta-estimator allowing us to get the best of the two worlds: a calibrated classifier providing optimum ""hard"" predictions. This meta-estimator will land in a future version of scikit-learn: https://github.com/scikit-learn/scikit-learn/pull/26120.

We will provide some insights regarding the way to obtain accurate probabilities and predictions and also illustrate how to use in practice this model on different use cases: cost-sensitive problems and imbalanced classification problems.",,,30,1,en,True,False,,KMDJAL,Guillaume Lemaitre,HS 120,2023-08-16T09:05:00+00:00,2023-08-16T11:35:00+02:00,2.00,2.0,,Get the best from your scikit-learn classifier: trusted probabilties and optimal binary decision,,,Supervised Learning,,,some,some,no,https://github.com/scikit-learn/scikit-learn/pull/26120,https://github.com/scikit-learn/scikit-learn/pull/26120
HCATHS,"Ibis: A fast, flexible, and portable tool for data analytics.",confirmed,,Tutorial,Data Science and Visualisation,2023-04-30T19:55:28.466219+00:00,,"Ibis provides a common dataframe-like interface to many popular databases and analytics tools  (BigQuery, Snowflake, Spark, DuckDB, …). This lets users analyze data using the same consistent API, regardless of which backend they’re using, and without ever having to learn SQL. No more pains rewriting pandas code to something else when you run into performance issues; write your code once using Ibis and run it on any supported backend. In this tutorial users will get experience writing queries using Ibis on a number of local and remote database engines.","Tabular data is ubiquitous, and Pandas has been the de facto tool in Python for analyzing it. However, as data size scales, analysis using Pandas may become untenable. Luckily, modern analytical databases (like DuckDB) are able to analyze this same tabular data, but perform orders-of-magnitude faster than Pandas, all while using less memory. Many of these systems only provide a SQL interface though; something far different from Pandas’ dataframe interface, requiring a rewrite of your analysis code.

This is where Ibis comes in. Ibis provides a common dataframe-like interface to many popular databases and analytics tools (BigQuery, Snowflake, Spark, DuckDB, …). This lets users analyze data using the same consistent API, regardless of which backend they’re using, and without ever having to learn SQL. No more pains rewriting pandas code to something else when you run into performance issues; write your code once using Ibis and run it on any supported backend.

In this tutorial we’ll cover:

- The basic operations of Ibis (select, filter, group_by, join, and aggregate), and how these operations may be composed to form more complicated queries.
- How Ibis may be used on a number of different local and remote backend engines to execute the same queries on different systems.
- The tradeoffs of different database engines, and recommendations for how to choose the best tool for the job.
- How Ibis integrates into the larger Python data ecosystem, including tools like Scikit-Learn or Matplotlib

This is a hands-on tutorial, with numerous examples to get your hands dirty. Participants should ideally have some experience using Python and Pandas, but no SQL experience is necessary.",,,90,1,en,False,False,,"WUEKGF, YE9YBK","Gil Forsyth, Phillip Cloud",Aula,2023-08-15T06:30:00+00:00,2023-08-15T10:00:00+02:00,2.00,2.0,,"Ibis: A fast, flexible, and portable tool for data analytics.",,,,,Data Analysis and Data Engineering,none,some,,,https://ibis-project.org
HJLJCQ,Getting started with JupyterLab,confirmed,,Tutorial,Scientific Applications,2023-05-14T15:05:22.739525+00:00,,"JupyterLab is very widely used in the Python scientific community. Most, if not all, of the other tutorials will use Jupyter as a tool. Therefore, a solid understanding of the basics is very helpful for the rest of the conference as well as for your later daily work.
This tutorial provides an overview of  important basic Jupyter features.","# Outline

## Introduction

* Terminology: JupyterLab, Notebook, IPython (10 min)
* Notebook approach - cells, code, markdown and more (15 min)

## Tools

* Help system and history (10 min)
* Magic functions basics (15 min)

## Development

* Runtime measurements and profiling (20 min)
* Exceptions and debugging (20 min)

The tutorial will be hands on.
While the students will receive a comprehensive PDF with all course content,
I will not distribute pre-filled Notebooks.
Instead, I will start with a blank Notebook for each topic and develop the
content step-by-step.
The participants are encouraged to type along.
My typing speed is usually appropriate and allows participants to follow.
In addition, the supplied PDF contains all needed code and commands to get back
on track, if I should be too fast.
I also explicitly ask for feedback if I am too fast or things are unclear.
I encourage questions at any time.
In fact, questions and my answers are often an important part of my teaching,
making the learning experience much more lively and typically more useful.

### Software Requirements

You need to have Python and JupyterLab installed. I will use Python 3.11. Older versions such as 3.8., 3.9 or 3.10 should work too. If you use Anaconda, you should be all set. Otherwise, if you use conda install with `conda install -c conda-forge jupyterlab` (or use `mamba` instead of `conda`); if you use `pip` install with `pip install jupyterlab`.",,,90,1,en,True,False,,9KSJ3K,Mike Müller,HS 120,2023-08-14T06:30:00+00:00,2023-08-14T10:00:00+02:00,2.00,2.0,,"Affordable travel to Jupyter, no rocket (science) required",,,,Other,,none,some,,,
HWKYXA,Incidents prediction with Hawkes model and other Tech AIOps projects in ING,confirmed,,Talk (25 mins + Q&A),Data Science and Visualisation,2023-04-26T19:07:18.510972+00:00,,"In this talk, we will discuss incident prediction with a Hawkes process within the IT infrastructure. We show how model previously applied for earthquake predictions can help answer the question ‘what caused what’ in a major European bank.","ING as European leading bank is continuously keeping track of its digital assets such as servers, network devices and software programs. Asset management is a challenging task because of the complex dependencies and hierarchical nature. Despite these difficulties, many monitoring tools were successfully implemented in ING, including metric anomaly detection, resource capacity forecasting, IT structure optimization and incident prediction with natural language processing of logs and events.

In this talk, we will discuss incident prediction with a Hawkes process. This model was previously successfully applied for earthquake predictions based on aftershocks and capturing the dynamics of full order books in finance. The Hawkes process model is well defined mathematically and can process a large volume of data to uncover Granger causal structures in data if implemented appropriately. We show how Hawkes processes help answer the question ‘what caused what’ within the IT infrastructure of a major European bank.",,,30,1,en,False,False,,"FQUEVZ, 7MFHEK","Arkadiusz Trawiński, PhD, Joost Göbbels",HS 120,2023-08-17T13:30:00+00:00,2023-08-17T16:00:00+02:00,2.00,2.0,,We will discuss incident prediction with a Hawkes process within the IT infrastructure of ING.,,,,,Model Deployment,none,none,,,
HXD9EH,Deploying multi-GPU workloads on Kubernetes in Python,confirmed,,Talk (15 mins + Q&A),High Performance Computing,2023-05-02T15:51:32.971050+00:00,,"By using Dask to scale out RAPIDS workloads on Kubernetes you can accelerate your workloads across many GPUs on many machines. In this talk, we will discuss how to install and configure Dask on your Kubernetes cluster and use it to run accelerated GPU workloads on your cluster.","The RAPIDS suite of open-source software libraries gives you the freedom to execute end-to-end data science and analytics pipelines entirely on GPUs with minimal code changes and no new tools to learn.

Dask is an open-source library which provides advanced parallelism for Python by breaking functions into a task graph that can be evaluated by a task scheduler that has many workers.

By using Dask to scale out RAPIDS workloads on Kubernetes you can accelerate your workloads across many GPUs on many machines. In this talk, we will discuss how to install and configure Dask on your Kubernetes cluster and use it to run accelerated GPU workloads on your cluster.",,,20,1,en,False,False,,EE7H7J,Jacob Tomlinson,Aula,2023-08-17T12:40:00+00:00,2023-08-17T15:00:00+02:00,2.00,1.6,,Use Dask on Kubernetes to run accelerated GPU workloads on your cluster,Parallel Computing,,,,,some,expert,yes,,
JACZHL,Estimagic: A library that enables scientists and engineers to solve challenging numerical optimization problems,confirmed,,Talk (15 mins + Q&A),"Community, Education, and Outreach",2023-04-29T14:11:05.609568+00:00,,"estimagic is a Python package for nonlinear optimization with or without constraints. It is particularly suited to solving difficult nonlinear estimation problems. On top, it provides functionality to perform statistical inference on estimated parameters.

In this presentation, we give a tour through estimagic's most notable features and explain its position in the ecosystem of Python libraries for numerical optimization.","Challenging numerical optimization problems arise in many places in science and industry, for example, in the calibration of scientific models, engineering, and statistics. Solving them requires high-quality optimizers and diagnostic tools that help select a suitable algorithm and monitor the optimization's progress. 

Estimagic provides a unified interface to optimization algorithms from scipy, nlopt, pygmo, and many other libraries. The minimize function feels familiar to users of scipy.optimize who are looking for a more extensive set of supported optimizers. Advanced users can use optional arguments to configure every aspect of the optimization, create a persistent log file, turn local optimizers global with a multistart framework, and more. Estimagic can calculate numerical derivatives in parallel, and many optimizers can leverage parallel hardware without requiring changes to the user's criterion function. 

In this presentation, we give a tour through estimagic's most notable features and explain its position in the ecosystem of Python libraries for numerical optimization.",,,20,1,en,False,False,,DMPC8P,Janos Gabler,Aula,2023-08-16T12:40:00+00:00,2023-08-16T15:00:00+02:00,2.00,2.0,,estimagic is a Python package for nonlinear optimization with or without constraints. Come to Euroscipy2023 to learn to see it in action and learn more about its powerful features!,,Learning and Teaching Scientific Python,,,,some,some,yes,https://estimagic.readthedocs.io/en/stable/,https://github.com/OpenSourceEconomics/estimagic
JBUYJL,RNA-seq data normalization in Python,confirmed,,Poster,Scientific Applications,2023-04-25T08:00:50.078183+00:00,,"Gene expression is commonly measured using RNA sequencing. In a nutshell, bioinformatic analysis includes aligning sequencing reads to a reference genome and counting expression as the number of reads per gene or transcript. Normalization of gene counts is important for accurate interpretation and downstream analysis, and the widely used methods like TPM and RPKM make us believe that RNA-seq normalization is a solved problem. This poster will present some alternatives and discuss why there may be more suitable ways of transforming gene expression counts for machine learning applications.

We developed [RNAnorm](https://github.com/genialis/rnanorm), an RNA-seq data normalization toolkit in Python. This open-source package (re-)implements a range of normalization methods including CPM, FPKM, TPM, TMM, UQ, CUF, and CTF. The code architecture is compatible with scikit-learn’s fit and transform interface, allowing users to rapidly prototype normalization strategies and assess their impact on model performance. 

The poster will present the theoretical background of the methods from the perspective of the bias they address (i.e. library size, gene length, RNA composition) and their respective strengths and weaknesses. It will also showcase the usage od RNAnorm package to perform various normalization tasks: from simple one-line normalization to a more complex grid search to find the best combination of normalizations for a given machine learning problem.","RNA sequencing (RNA-seq) has [revolutionized the field of transcriptomics](https://www.nature.com/articles/nrg2484) and has become a widely used method for analyzing gene expression. The process to obtain ML-ready data involves a series of steps, such as RNA extraction, PCR amplification, sequencing, and [bioinformatics](https://www.nature.com/articles/s41598-020-76881-x). Normalization is an essential step in RNA-seq data analysis, as it removes biases arising due to varying library size between the samples, different gene length within each sample, and the varying proportions of different RNA molecules within and between samples (RNA composition). Biologists, bioinformaticians, and data scientists working with RNA sequencing data must have a good understanding of the normalization methods to accurately analyze and interpret these data.

The poster will first provide a brief refresher on the basic steps of RNA-Seq data analysis to set the scene for the three main sources of systematic bias — library size, gene length, and RNA composition. Than, each method will be presented from the perspective of these biases. This will be done in an algorithmic fashion as well as using easy to follow calculus and pseudo code to really drive home the concepts. 
Assumptions, advantages, and shortcomings of each method will be investigated, using real-world examples. Special care will be taken to evaluate method utility for differential expression, co-expression networks, and machine learning. 

The poster will then present a Python implementation of these methods. We have developed an open-source RNA-seq normalization toolkit in Python called [RNAnorm](https://github.com/genialis/RNAnorm). The package implements the most popular methods, such as [CPM](https://academic.oup.com/bioinformatics/article/26/1/139/182458), [FPKM](https://www.nature.com/articles/nmeth.1226), [TPM](https://academic.oup.com/bioinformatics/article/26/4/493/243395), [UQ](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-94), [TMM](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2010-11-3-r25), [CUF and CTF](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02568-9), follows Python coding best practices, is well documented, has exhaustive tests, and is freely available on Github under a permissive license. The code is compatible with [scikit-learn’s](https://scikit-learn.org/stable/index.html) fit and transform interface, which makes it convenient for evaluating various combinations of normalization methods in machine learning applications. 

Code examples will be used to present the unified interface of RNAnorm. The examples will progress from simple one-line normalization commands to a more complex grid search to find the best combination of normalizations for a given machine learning problem.

Finally, poster will present lessons learned from years of experience on the topic and recommend best practices for gene expression data normalization for the development of predictive biomarkers.","We have developed a Python library for normalizing RNAseq data. We truly believe it is great resource for the Python data science community and would like to show-case it at EuroScipy. The methods implemented in this package were previously scattered across various Python packages or only available in R programming language. However, nowadays Python is becoming the default choice for data science and the need for such package is only increasing.

We believe that the 90 minutes tutorial format would be the best option for this. However, we realize there might be applications for even more important or appealing tutorials. So, as a fallback we also applied a poster with the same title. Please chose only one of the two during the application review process.",,90,1,en,False,False,,H88LLA,Jure Zmrzlikar,,,,2.00,2.0,,Wish to do RNAseq normalization in Python? All the common methods in one place? Scikit-learn style API? Use RNAnorm Python package!,,,,Healthcare and Biomedicine,,some,some,,,https://github.com/genialis/RNAnorm
K9MHRH,Content-based recommendation-system for the examples in sphinx-gallery,confirmed,,Talk (15 mins + Q&A),"Community, Education, and Outreach",2023-04-26T09:53:39.793471+00:00,request as poster,"The gallery of your project might group the examples by module, by use case, or some other logic. But as examples grow in complexity, they may be relevant for several groups. In this talk we discuss some possible solutions and their drawbacks to motivate the introduction of a new feature to sphinx-gallery: a content-based recommendation system.","Imagine a scikit-learn example on text clustering using silhouette scores, as a maintainer, would you assign it to the `sklearn.cluster`, the `sklearn.feature_extraction.text` or the `sklearn.metrics` group of examples? As a user where would you look for it?

Some solutions such as adding human implemented tags have been proposed to cross-link examples that can be grouped by different logics, with the disadvantage of requiring maintenance and consensus. Instead we could have a recommender system based on similarity (nearest neighbors tf-idf model) to automatically link to the most relevant related content . This could be introduced at the end of each example.

Libraries with several examples such as scikit-learn and matplotlib may benefit from this new feature.

For more information visit https://github.com/sphinx-gallery/sphinx-gallery/pull/1125",,,20,1,en,False,False,,HRFVLY,Arturo Amor,HS 120,2023-08-17T09:45:00+00:00,2023-08-17T12:05:00+02:00,2.00,2.0,,Keep the example gallery of a project easy to navigate with the help of an example-recommender system.,,Other,,,,none,none,no,,https://sphinx-gallery.github.io/stable/index.html
LGUKNE,From Implementation to Ecosystem: The Journey of Zarr,confirmed,,Talk (15 mins + Q&A),"Community, Education, and Outreach",2023-05-14T15:38:51.565556+00:00,,"Zarr is an API and cloud-optimized data storage format for large, N-dimensional, typed arrays, based on an open-source technical specification. In the last 4 years it grew from a Python implementation to a large ecosystem. In this talk, we want to share how this transformation happened and our lessons learned from this journey. Today, Zarr is driven by an active community, defined by an extensible specification, has implementations in C++, C, Java, Javascript, Julia, and Python, and is used across domains such as Geospatial, Bio-imaging, Genomics and other Data Science domains.","This talk covers the following points:

* What is Zarr & how does it work?
    * Illustrated Mechanisms of Zarr & Examples
    * When and Why should you use Zarr?
    * Cloud-optimized file/object-storage systems
* Early Development of Zarr and Adaption Across Implementations & Domains
  * Implementations in C++, C, Java, Javascript, Julia, and Python
  * Usage across Geospatial, Bio-imaging, Genomics and other Data Science domains
* The Zarr Enhancement Proposal ([ZEP](https://zarr.dev/zeps/)) process
* [Zarr v3](https://zarr-specs.readthedocs.io/en/latest/v3/core/v3.0.html) & [ZEP0001](https://zarr.dev/zeps/draft/ZEP0001.html): From Implementation-driven Development to Spec first
* Lessons learned while developing Zarr v3

In this talk you will

* understand the basics of Zarr and its specification,
* find inspiration for processes and tools in growing projects and ecosystems, and
* get essential takeaways regarding OSS project transitions from a young to a mature stage.","The track of this talk is arguably ambiguous. It's placed now in Community, Education, and Outreach, since the focus is on the ecosystem and community. But it could also be one of the following if the organizers see better fit there:
* Data Science and Visualization
* High Performance Computing
* Scientific Applications

Also, the talk could easily be extended to 30 mins if preferred.",,20,1,en,True,False,,YWKXWU,Jonathan Striebel,Aula,2023-08-17T08:30:00+00:00,2023-08-17T10:50:00+02:00,2.00,1.6,,Zarr is an API and data storage format for n-dimensional arrays. In the last years it grew from a Python implementation to a large ecosystem. In this talk we share how this transformation happened and our lessons learned from this journey.,,Other,,,,some,none,yes,,https://zarr.dev
LYNMEB,From Complex Scientific Notebook to User-Friendly Web Application,confirmed,,Tutorial,Data Science and Visualisation,2023-05-05T10:07:01.524498+00:00,,"Learn how to show your work with the MERCURY framework. This open-source tool perfectly matches your computed notebook (e.g., written in Jupyter Notebook). Without knowledge of frontend technologies, you can present your results as a web app (with interactive widgets), report, dashboard, or report. Learn how to improve your notebook and make your work understandable for non-technical mates. Python only!","Mercury is a tool that lets you add interactive widgets to your Jupyter Notebook. With these widgets, you can easily turn your notebook into a web application for creating dashboards and presentations. You can even schedule automatic updates. Mercury also provides a way to control who can access your notebooks with a built-in authentication module. Best of all, it's free and open-source.

The tutorial will include the following:

1. Start with Jupyter Notebook.
2. How to start with MERCURY (installing and setting up the needed environment).
3. Overview of the features as downloading results as PDF, restricting authentication, showing/hiding code. 
3. Add widgets to your notebook. Select the right widgets.
4. Set up a web app with MERCURY.
5. Deploy and share your web with others.",,,90,1,en,True,False,,"JWK7RV, B9JJ3N","Piotr Płoński, Aleksandra Płońska",Aula,2023-08-15T13:30:00+00:00,2023-08-15T17:00:00+02:00,1.00,1.0,,From Complex Scientific Notebook to User-Friendly Web Application #datascience #python #webapp,,,,,Data Analysis and Data Engineering,none,some,,https://runmercury.com/,https://github.com/mljar/mercury
N8YGEW,DataFrame-agnostic code: are we there yet?,confirmed,,Talk (15 mins + Q&A),Data Science and Visualisation,2023-05-14T17:31:30.697484+00:00,,"Have you ever wanted to write a DataFrame-agnostic function, which should perform the same operation regardless of whether the input is pandas / polars / something else? Did you get stuck with special-casing to handle all the different APIs? All is good, the DataFrame Standard is here to help!","If you want to write a DataFrame-agnostic function, you currently have three choices:
- convert the input DataFrame to pandas (say), perform operations, then convert
  back to the original DataFrame library;
- write the same code multiple times, with if-then statements to deal with the differences between APIs;
- give up, and only support a single DataFrame (usually pandas).

However, there's a new solution in town: use the DataFrame Standard. The DataFrame Standard provides you with a minimal, strict, and predictable API. It allows you to develop with confidence, knowing that your code will work regardless of whether the caller uses pandas, polars, or some other DataFrame library.

Talk outline will be (roughly):
5 mins: motivation - why do we even need this?
5 mins: demo - let's write a DataFrame-agnostic function!
5 mins: stability, usability, future plans",,,20,1,en,True,False,,KEUJ9U,Marco Gorelli,Aula,2023-08-16T09:40:00+00:00,2023-08-16T12:00:00+02:00,2.00,2.0,,"Learn how to write your code in such a way that it will support pandas, polars, and more - all without special-casing or data conversions!",,,,,Data Analysis and Data Engineering,expert,expert,no,https://data-apis.org/dataframe-api/draft/index.html,https://github.com/data-apis/dataframe-api
NPNXNA,scverse: Foundational tools for single-cell omics analysis,confirmed,,Poster,Scientific Applications,2023-04-30T15:39:48.946495+00:00,request as poster,"scverse (scverse.org) is a consortium of core Python tools for storage and analysis of high-dimensional single-cell data. These tools provide scalable infrastructure for hundreds of dependent packages and have been used to study the human immune system, aging, and COVID-19. The consortium provides a continued maintenance plan, community platforms, and organization for the hubs of our research software ecosystem. We will introduce scverse and present ongoing work addressing scalability, spatial omics, and community building.","scverse (https://scverse.org) is a consortium of core Python tools for storage and analysis of high-dimensional single-cell data. These tools provide scalable infrastructure for hundreds of dependent packages and have been used to study the human immune system, aging, and COVID-19. The consortium provides a continued maintenance plan, community platforms, and organization for the hubs of our research software ecosystem. We will introduce scverse and present ongoing work addressing scalability, spatial omics, and community building.

**Background**

In the past decade, new technologies to measure bio-molecules, in particular mRNA, at single cell resolution have emerged. The data generated by such technologies are high dimensional and are represented as very large matrices with thousands of genes (variables) and hundreds of thousands to millions of cells (observations). The volume and complexity of this data required the development of new tools for data handling and analysis.

Usage of Python and the PyData ecosystem for developing single-cell omics data analysis has been catalyzed by Scanpy (single-cell analysis in Python) and AnnData, enabling an ecosystem of packages around this infrastructure. The single cell analysis ecosystem in Python has outgrown a single package or modality and now routinely includes tools at the frontier of single-cell research and development (https://scverse.org/packages). To handle the growing needs of this ecosystem, we've formed scverse.

**Scalability**

As in many fields, the scale of data is rapidly increasing. We will show how we’re integrating with zarr and Dask for handling large amounts of data, accelerators for computing on that data, and benchmarking with asv to make sure our tools stay fast.

**Spatial omics**

Emerging technologies that combine barcoding strategies with microscopy allow us to measure spatially-resolved high-dimensional omics data. We will show how scverse is tackling the challenges and opportunities of this new data by combining our existing ecosystem with the excellent spatial analysis libraries in the scientific Python ecosystem.

**Community**

Since founding scverse last year, we’ve been working on building our community. We’ll present our approach so far, and the community platforms created including: hackathons, a registry for community packages, and regular community calls.",I'm flexible on format. This could be expanded to a longer talk or put into a poster.,,90,1,en,False,False,,EDWUND,Isaac Virshup,,,,2.00,2.0,,scverse: an ecosystem for single-cell omics analysis,,,,Healthcare and Biomedicine,,some,some,yes,,https://scverse.org
P38Y7L,Where is the flock? The use of graph neural networks for bird identification with meteorological radar.,confirmed,,Talk (15 mins + Q&A),Machine and Deep Learning,2023-05-15T10:10:41.671511+00:00,,"In this project we generate tools to identify birds within the spatial extent of a meteorological radar.  Using the opportunities created by modern dual-polarization radars we build graph neural networks to identify bird flocks. For this, the original point cloud data is converted to multiple undirected graphs following a set of predefined rules, which are then used as an input in graph convolutional neural network (Kipf and Welling, 2017, https://doi.org/10.48550/arXiv.1609.02907). Each node has a set of features such as range, x, y, z coordinates and several radar specific parameters e.g. differential reflectivity and phase shift which are used to build model and conduct graph-level classification. This tool will alleviate problem of manual identification and labelling which is tedious and time intensive. Going forward we also focus on using the temporal information in the radar data. Repeated radar measurements enable us to track these movements across space and time. This makes it possible for regional movement studies to bridge the methodological gap between fine-scale, individual-based tracking studies and continental-scale monitoring of bird migration. In particular, it enables novel studies of the roles of habitat, topography and environmental stressors on movements that are not feasible with current methodology. Ultimately, we want to apply the methodology to data from continental radar networks to study movement across scales.","In this project we generate tools and a python package to identify birds within the spatial extent of a meteorological radar.  Using the opportunities created by modern dual-polarization radars we develop graph neural networks (GNN) to identify bird flocks. For this, the original point cloud data is converted to multiple undirected graphs following a set of predefined rules. For example, each point of interest needs to have a label and a minimum number of neighbours within a specified range. Graphs are then used as an input in graph convolutional neural network (Kipf and Welling, 2017, https://doi.org/10.48550/arXiv.1609.02907). Each node has a set of features such as range, x, y, z coordinates and several radar specific parameters e.g. differential reflectivity and phase shift which are used to build model and conduct graph-level classification.  Model learns hidden layer representations that encode both local graph structure and features of nodes and is based on an efficient variant of convolutional neural network. 

This tool will alleviate problem of manual identification and labelling which is tedious and time intensive. Going forward we also focus on using the temporal information in the radar data. Repeated radar measurements enable us to track these movements across space and time. This makes it possible for regional movement studies to bridge the methodological gap between fine-scale, individual-based tracking studies and continental-scale monitoring of bird migration. In particular, it enables novel studies of the roles of habitat, topography and environmental stressors on movements that are not feasible with current methodology. Ultimately, we want to apply the methodology to data from continental radar networks to study movement across scales. This project is a collaboration between the Netherlands eScience Center and the University of Amsterdam. This is an ongoing project and feedback provided by the community is highly valued. 

Git Repo of the package: https://github.com/point-cloud-radar/bird-cloud-gnn",,,20,1,en,True,False,,"QVP8EU, 3RVYDP","Olga Lyashevska, Abel S. Siqueira",HS 120,2023-08-17T08:55:00+00:00,2023-08-17T11:15:00+02:00,2.00,2.0,,Where is the flock? Or how graph convolutional neural networks help to identify birds from metheorological radar,,,"ML Applications (e.g. NLP, CV)",,,some,some,yes,,https://github.com/point-cloud-radar/bird-cloud-gnn
P39VK3,Runtime-Control of Numerical Groundwater Models,confirmed,,Poster,Scientific Applications,2023-05-14T14:50:04.046692+00:00,,"The widely-used finite volume groundwater flow and transport model MODLFOW 6 is implemented in Fortran. The typical work flow consists of the steps (1) pre-processing on input data, (2) running the model, and (3) post-processing of model results. For a variety of problems this workflow has to be applied repeatedly, modifying values of input data based on modeling results  The here presented tool `pymf6` enables the model user to access intermediate model results at runtime and modifying model variables with Python. This allows to greatly reduce the number of model runs. Often, one model run can replace many manual iterations, leading to a much faster workflow. Furthermore, complex tasks that would not be feasibly with the manual workflow can be solved. `pymf6` can also be used to couple MODLFOW 6 with other numerical or analytical models.","The widely-used finite volume groundwater flow and transport model MODLFOW 6 is implemented in Fortran. The typical work flow consists of the steps (1) pre-processing on input data, (2) running the model, and (3) post-processing of model results. For a variety of problems this workflow has to be applied repeatedly, modifying values of input data based on modeling results  The here presented tool `pymf6` enables the model user to access intermediate model results at runtime and modifying model variables with Python. This allows to greatly reduce the number of model runs. Often, one model run can replace many manual iterations, leading to a much faster workflow. Furthermore, complex tasks that would not be feasibly with the manual workflow can be solved. `pymf6` can also be used to couple MODLFOW 6 with other numerical or analytical models.",,,90,1,en,True,False,,"QWPUQW, WVJ8YD","Mike Müller, Lúcia Pedrosa",,,,2.00,2.0,,Making numerical modeling program interactive - Speed up groundwater modeling tasks,,,,Earth and Ocean Sciences,,some,some,,https://pymf6.readthedocs.io/en/latest/index.html,https://github.com/hydrocomputing/pymf6
P9E7HX,Automating data collection and analysis in citizen science projects,confirmed,,Poster,Scientific Applications,2023-05-14T11:48:11.450033+00:00,,"Citizen science is a powerful tool used by scientists in a wide range of fields to aid the utilisation of scientific data, that would otherwise take a very long time to process and analyse, while being an important way to engage with the public. RFI has a number of citizen science projects, under the name ‘Science Scribbler’, for collecting annotation data from biological 3D imaging techniques. Annotated data can be used to guide segmentation from images in order to understand morphological features at the cellular level in health and disease. To support current citizen science projects, and to facilitate planning of future ones, automating the way images are generated and annotation data is processed is of major importance. A web tool for on-the-fly image generation from 3D volumes of data is being created to provide an easy way to generate images for annotation by volunteers via the Zooniverse citizen science platform, but also to allow minimisation of storage space requirements. Furthermore a library for automating data processing and analysis is in construction that can be used as a pipeline to extract, pool, visualise, filter and prepare annotation data for segmentation and other types of analysis.","The primary goal of RFI's Zooniverse projects is to understand differences in cell morphology in health and disease and to facilitate diagnosis from imaging data. It can do so by leveraging the advantages of citizen science to produce more representative, less biassed training datasets while reducing the amount of time needed by the expert to generate training data for segmentation. Better training data lead to better machine learning algorithms that are more efficient at discriminating between different cellular morphologies. There are two aspects of this research that would benefit from automation. First it is the design of the experiment which involves generating large amounts of images that can be used for data collection. An easy to use web tool for on-the-fly generation of images allows quick sampling and image generation from the 3D volume space and therefore helps the researcher to quickly survey images and/or generate subsets of images for use with the annotation platform. A rich API allows manipulation of the 3D volume, generation of single images and sets of images, application of colormaps / filters and specification of image format. The API endpoints can be accessed directly from the Zooniverse server for fetching an image directly or can be used by researchers to pre-generate images and upload to Zooniverse. The second aspect of the research that is being automated is processing of annotation data. The goal of data processing is to produce a number of different training datasets that can be assessed on how they improve the segmentation task. For this purpose the ‘Zookeeper’ library which is currently being developed will be used as a pipeline for processing annotation data. The library will provide a set of tools for extracting data from databases, will allow pooling data from different datasets and filtering using different metrics utilising annotation metadata. Finally it will provide the tools to optimise data clustering. The library can be imported in project specific scripts such as Science Scribbler: Huntington project to automate the preparation of data for segmentation.",,,90,1,en,True,False,,KGPTSN,Dimitrios Ladakis,,,,2.00,2.0,,Using citizen science to understand cell morphology in health and disease. The combination of citizen science and automation of data processing can lead to better data for image analysis.,,,,Healthcare and Biomedicine,,some,some,,,
PQRRLN,Introduction to Python for scientific programming,confirmed,,Tutorial,Scientific Applications,2023-06-27T18:49:54.596813+00:00,,"This tutorial will provide an introduction to Python intended for beginners.

It will notably introduce the following aspects:

- built-in types
- controls flow (i.e. conditions, loops, etc.)
- built-in functions
- basic Python class","This tutorial will provide an introduction to Python intended for beginners.

It will notably introduce the following aspects:

- built-in types
- controls flow (i.e. conditions, loops, etc.)
- built-in functions
- basic Python class

We introduce here the Python language. Only the bare minimum necessary for getting started with Numpy and Scipy is addressed here. To learn more about the language, consider going through the excellent tutorial https://docs.python.org/tutorial.",,,90,1,en,False,False,,JZ3LXH,Milton Gomez,HS 120,2023-08-14T08:30:00+00:00,2023-08-14T12:00:00+02:00,,,,Introduction to the Python language for scientific programming,Parallel Computing,Learning and Teaching Scientific Python,Supervised Learning,Astronomy,Data Analysis and Data Engineering,none,some,,,
PWER3Z,Introduction to Data Analysis Using Pandas,confirmed,,Tutorial,Data Science and Visualisation,2023-04-29T20:14:58.655151+00:00,,"Working with data can be challenging: it often doesn’t come in the best format for analysis, and understanding it well enough to extract insights requires both time and the skills to filter, aggregate, reshape, and visualize it. This session will equip you with the knowledge you need to effectively use pandas – a powerful library for data analysis in Python – to make this process easier.","#### Section 1: Getting Started With Pandas
We will begin by introducing the Series, DataFrame, and Index classes, which are the basic building blocks of the pandas library, and showing how to work with them. By the end of this section, you will be able to create DataFrames and perform operations on them to inspect and filter data.

#### Section 2: Data Wrangling
To prepare our data for analysis, we need to perform data wrangling. We will learn how to clean and reformat data (e.g. renaming columns, fixing data type mismatches), restructure/reshape it, and enrich it (e.g. discretizing columns, calculating aggregations, combining data sources).

#### Target Audience
This tutorial is for anyone with basic knowledge of Python and an interest in learning how to analyze data in Python. We will be working with Jupyter Notebooks, so attendees should familiarize themselves with the interface (i.e., know how to run/edit a cell) beforehand.

#### Prerequisites
Bring a laptop (preferably your personal one) with the virtual environment configured as indicated [here](https://github.com/stefmolin/pandas-workshop#setup-instructions). Come to the session with your environment set up so we can dive right into the material.","This is a hands-on tutorial broken down into 2 sections, which each consist of lectures using Jupyter Notebooks for following along with the code and a set of exercises dispersed throughout to test each attendee’s understanding. There are more questions provided in each section than the attendees will have time to answer so that those at all levels of prior experience have a challenge. After some time, I will go over the solutions to those exercises before resuming the lecture. There is also a 3rd section on data visualization in pandas, which can be included if the session is extended.",,90,1,en,False,True,,9WJJPL,Stefanie Molin,HS 120,2023-08-14T13:30:00+00:00,2023-08-14T17:00:00+02:00,2.00,2.0,,"""Introduction to Data Analysis Using Pandas"" will equip you with the knowledge you need to effectively use pandas – a powerful library for data analysis in Python.",,,,,Data Analysis and Data Engineering,none,some,,https://stefmolin.github.io/pandas-workshop/slides/html/workshop.slides.html#/,https://github.com/stefmolin/pandas-workshop
Q9F8GA,My foray from Scientific Python into the Pyodide / WebAssembly universe,confirmed,,Talk (25 mins + Q&A),Data Science and Visualisation,2023-04-14T09:41:11.233053+00:00,,"Pyodide is a Python distribution for the browser and Node.js based on WebAssembly / Emscripten.
Pyodide supports most commonly used scientific Python packages, like numpy, scipy, scikit-learn, matplotlib and there is growing interest to use it for improving package documentation through interactivity.

In this talk we will describe the work we have done in the past nine months to improve the state of Pyodide in a scientific Python context, namely:
- running the scikit-learn and scipy test suites with Node.js to get a view of what currently works, what does not, and what can be hopefully be fixed one day
- packaging OpenBLAS in Pyodide and use it for Pyodide scipy package to improve its stability, maintainability and performance
- adding JupyterLite functionality to sphinx-gallery, which is used for example galleries of popular scientific Python package like scikit-learn, matplotlib, scikit-image, etc ...
- adding the sphinx-gallery Jupyterlite functionality for scikit-learn example gallery

We will also mention some of the Pyodide sharp bits and conclude with some of the ideas we have to use it even more widely.","Pyodide is a Python distribution for the browser and Node.js based on WebAssembly. Pyodide supports most commonly used scientific Python packages, like numpy, scipy, scikit-learn, matplotlib and there is growing interest to use it for improving documentation through interactivity.

In this talk we will describe the work we have done in the past six months, including:
- regularly running the scikit-learn and scipy test suites with Node.js to get  a view of what currently works, what does not, and what can be  hopefully be fixed one day
- packaging OpenBLAS in Pyodide and use it for Pyodide scipy package to improve  its stability and maintainability
- adding JupyterLite functionality to sphinx-gallery, which is used for example  galleries of popular scientific Python package like scikit-learn, matplotlib,  scikit-image, etc ...
- adding the sphinx-gallery Jupyterlite functionality for scikit-learn example  gallery

We will also mention some of the Pyodide sharp bits and conclude with some of the ideas we have to use it even more widely.

Here are the references for the work mentioned above:
- running scipy and scikit-learn test suite inside Pyodide: https://github.com/lesteve/scipy-tests-pyodide and https://github.com/lesteve/scikit-learn-tests-pyodide
- OpenBLAS Pyodide PR: https://github.com/pyodide/pyodide/pull/3331
- sphinx-gallery JupyterLite-related PRs: https://github.com/pyodide/pyodide/pulls?q=is%3Apr+sort%3Aupdated-desc+author%3Alesteve
- scikit-learn JupyterLite examples PR: https://github.com/scikit-learn/scikit-learn/pull/25887",,,30,1,en,False,False,,N7TBEV,Loïc Estève,HS 120,2023-08-17T11:30:00+00:00,2023-08-17T14:00:00+02:00,2.00,1.8,,My foray from Scientific Python into the Pyodide / WebAssembly universe,,,,,Other,none,some,no,,
RE98N3,Sprint EuroSciPy 2023 - Morning Session,confirmed,,Sprint,,2023-08-08T18:47:22.798039+00:00,,"A one-day sprint is organized on August 18. Open-source sprints are sessions where maintainers and contributors (including first-time contributors) meet and work together in improving the open-source projects. This can include bug fixes, new features, improvements to the documentation and others.","# Sprint

### Project sprint

A one-day sprint is organized on August 18. Open-source sprints are sessions where maintainers and contributors (including first-time contributors) meet and work together in improving the open-source projects. This can include bug fixes, new features, improvements to the documentation and others.

Sprints are a great way to get started in open-source development, learn, and meet like-minded people. All EuroSciPy attendees are welcome and encouraged to join the sprints.

The definitive schedule is not yet known and more sprints will be added when they are proposed, and some will be improvised in the same day. If you want to propose a sprint, please send us a message to maintainers@euroscipy.org .

### List of confirmed sprints:

- scikit-learn
     - GPU programming for scikit-learn with sklearn-numba-dpex and Array API.
     - Addressing issues from the GitHub issue tracker.
- pandas
- VizPy
- braindecode
     - Restructuring the documentation to make it more accessible for new users.
     - Re-implement and optimization the DataLoaders.
     - Re-implement the cropped training.


### Sprints Location

Address: 
Roshoffgasse 2,
CH-4051 Basel,
Switzerland",,,180,1,en,False,False,,,,,,,,,,Coding Sprint at EuroSciPy 2023 for Morning Session,Parallel Computing,Learning and Teaching Scientific Python,Supervised Learning,Astronomy,Data Analysis and Data Engineering,none,some,,,
RLJETB,Writing tests for your containers with Container Canary,confirmed,,Poster,"Community, Education, and Outreach",2023-05-02T15:53:37.716263+00:00,"New track - MLOps, Deployment","When we write code we also write tests to ensure that our software functions as expected. But with containers, it is far more common to document assumptions and leave it at that.

Containers are the building blocks of modern compute platforms and they have interfaces like any other software. The software they contain, commands they run and ports they listen on all make up that interface and when composed into larger systems many assumptions are made about how containers will behave. So we should test that!

Container Canary is a black-box testing tool for container images and in this talk, we will talk about why you should test your containers, how to write canary test manifests and how to use the canary CLI tool to run them.","Many modern compute platforms support bring-your-own-container models where the user can provide container images with their custom software environment. However, platforms commonly have a set of requirements that the container must conform to, such as using a non-root user, having the home directory in a specific location, having certain packages installed or running web applications on specific ports.
Container Canary is a tool for recording those requirements as a manifest that can be versioned and then validating containers against that manifest. This is particularly useful in CI environments to avoid regressions in containers.",,,90,1,en,False,False,,EE7H7J,Jacob Tomlinson,,,,2.00,1.5,,Containers are the building blocks of modern compute platforms but we don't test them well enough,,Other,,,,some,expert,yes,https://github.com/NVIDIA/container-canary,
SCLBPU,skrub: preparing tables for machine learning,confirmed,,Poster,Data Science and Visualisation,2023-05-02T14:51:22.387838+00:00,,"Are you wondering what to do with all of the tables you have?
And how to make sense of all of the dirty data?

Data science is increasingly faced with multiple-sourced, non-standardized dirty tables.

We have what you need: skrub is a Python package for preparing your dirty tables for machine learning!

dirty-cat evolves into skrub and gives you the right tools to clean, transform and join your tables for machine learning.","skrub - formerly dirty_cat - is a Python package that aims to facilitate machine learning on dirty data. 
The focus is on common, real applications, of working with non-standardized tables from various sources.

Main features include:
- encoding dirty data. Our encoders - the SimilarityEncoder, GapEncoder and MinHashEncoder - are designed for handling dirty data! 
- use the TableVectorizer to automatically turn your pandas dataframe into a numpy array suitable for machine learning.
- fuzzy joining tables. This includes joining on string, numerical, date and mixed type columns.
- cleaning your data. Using methods such as deduplication, you can easily merge categories with similar morphology.",,,90,1,en,False,False,,A7PTVY,Jovan Stojanovic,,,,2.00,2.0,,Are you wondering what to do with all of the tables you have ? How to make sense of all of the dirty data ? We have the answer: skrub is a Python package that aims to facilitate machine learning on dirty data.,,,,,Data Analysis and Data Engineering,none,some,,https://github.com/orgs/skrub-data/repositories,https://dirty-cat.github.io/stable/
SE9WTD,Introduction to numerical optimization,confirmed,,Tutorial,High Performance Computing,2023-04-28T12:43:39.185373+00:00,,"In this hands-on tutorial, participants will delve into numerical optimization fundamentals and engage with the optimization libraries scipy.optimize and estimagic. estimagic provides a unified interface to many popular libraries such as nlopt or pygmo and provides additional diagnostic tools and convenience features. Throughout the tutorial, participants will get the opportunity to solve problems, enabling the immediate application of acquired knowledge. Topics covered include core optimization concepts, running an optimization with scipy.optimize and estimagic, diagnostic tools, algorithm selection, and advanced features of estimagic, such as bounds, constraints, and global optimization.","In this focused tutorial, participants will be introduced to the fundamentals of numerical optimization and various optimization libraries, including scipy.optimize and estimagic. The session is divided into three blocks, with each focusing on a specific aspect of optimization:

Introduction to numerical optimization and scipy.optimize
Introduction to estimagic and how to pick optimizers
Strategies and tools for advanced optimization

The tutorial is designed to be hands-on, dedicating ample time to practice sessions. Including numerous smaller practice sessions allows participants to apply their knowledge of each topic immediately.

During the first block, participants will learn the basics of numerical optimization, code up their first optimization problem and solve it with scipy.optimize.

In the second block, we introduce estimagic, as a unified interface to algorithms from scipy, nlopt, pygmo, and others. Participants will try different optimizers and learn how estimagic's diagnostic tools can be used to select the suitable algorithm for a given problem.

The third block will focus on practical strategies for advanced optimization problems. Participants will learn about logging and restarting optimizations, global optimization, bounds, and constraints, as well as derivative-free and noisy optimization problems.

The session will conclude with a summary of the main learnings, ensuring that participants have gained a solid understanding of numerical optimization and the various optimization libraries covered.","We selected the track 'High Performance Computing' because we thought of it as most suitable; however, we view our tutorial more as a general session for scientific computing across many disciplines.

We presented a tutorial on numerical optimization last year at the Scipy 2022 in Austin: https://www.youtube.com/watch?v=ftlw0rARrtI. Since then, we've updated its content and reduced its length, but the interactive style will be similar to last year.",,90,1,en,False,False,,"STKQES, DMPC8P, EF8NZ3","Tim Mensinger, Janos Gabler, Tobias Raabe",HS 120,2023-08-15T11:30:00+00:00,2023-08-15T15:00:00+02:00,2.00,2.0,,"Attention Scientific Python Developers: Join our tutorial on numerical optimization, exploring fundamentals with #scipy.optimize and estimagic. Learn about core concepts, diagnostic tools, and algorithm selection.",Other,,,,,some,some,,,
STXCKT,The Helmholtz Analytics Toolkit (Heat) and its role in the landscape of massively-parallel scientific Python,confirmed,,Talk (25 mins + Q&A),High Performance Computing,2023-04-24T10:40:44.247629+00:00,request as poster,"Handling and analyzing massive data sets is highly important for the vast majority of research communities, but it is also challenging, especially for those communities without a background in high-performance computing (HPC). The Helmholtz Analytics Toolkit (Heat) library offers a solution to this problem by providing memory-distributed and hardware-accelerated array manipulation, data analytics, and machine learning algorithms in Python, targeting the usage by non-experts in HPC.

In this presentation, we will provide an overview of Heat's current features and capabilities and discuss its role in the ecosystem of distributed array computing and machine learning in Python.","**co-authors:**  *C. Comito (FZJ), M. Götz (KIT),  J. P. Gutiérrez Hermosillo Muriedas (KIT), B. Hagemeier (FZJ), P. Knechtges (DLR), K. Krajsek (FZJ), A. Rüttgers (DLR), A. Streit (KIT), M. Tarnawa (FZJ)*

When it comes to enhancing exploitation of massive data, machine learning methods are at the forefront of researchers’ awareness. Much less so is the need for, and the complexity of, applying these techniques efficiently across large-scale, memory-distributed data volumes. In fact, these aspects typical for the handling of massive data sets pose major challenges to the vast majority of research communities, in particular to those without a background in high-performance computing. Often, the standard approach involves breaking up and analyzing data in smaller chunks; this can be inefficient and prone to errors, and sometimes it might be inappropriate at all because the context of the overall data set can get lost. 

The Helmholtz Analytics Toolkit (Heat) library offers a solution to this problem by providing memory-distributed and hardware-accelerated array manipulation, data analytics, and machine learning algorithms in Python. The main objective is to make memory-intensive data analysis possible across various fields of research ---in particular for domain scientists being non-experts in traditional high-performance computing who nevertheless need to tackle data analytics problems going beyond the capabilities of a single workstation. The development of this interdisciplinary, general-purpose, and open-source scientific Python library started in 2018 and is based on collaboration of three institutions (German Aerospace Center DLR, Forschungszentrum Jülich FZJ, Karlsruhe Institute of Technology KIT) of the Helmholtz Association. The pillars of its development are...

* ...to enable memory distribution of n-dimensional arrays, 
* to adopt PyTorch as process-local compute engine (hence supporting GPU-acceleration),
* to provide memory-distributed (i.e., multi-node, multi-GPU) array operations and algorithms, optimizing asynchronous MPI-communication (based on mpi4py) under the hood, and 
* to wrap functionalities in NumPy- or scikit-learn-like API to achieve porting of existing applications with minimal changes and to enable the usage by non-experts in HPC. 

In this talk we will give an illustrative overview on the current features and capabilities of our library. Moreover, we will discuss its role in the existing ecosystem of distributed computing in Python, and we will address technical and operational challenges in further development.","If the topic is not considered apprpriate for a 30min talk, we would be fine with a 15min time slot as well. Also, if --depending on the other submissions-- another track would better fit the topic, we would be open to be moved.",,30,1,en,False,False,,FU9BXF,Fabian Hoppe,Aula,2023-08-17T14:05:00+00:00,2023-08-17T16:35:00+02:00,2.00,1.3,,"You need to handle and analyze data that go beyond the capabilities of your workstation, but you are not an expert in HPC? - Come to our talk and learn about Heat..",Parallel Computing,,,,,none,some,yes,,https://github.com/helmholtz-analytics/heat
SYEFDW,"Anomaly Detection in Time Series: Techniques, Tools and Tricks",confirmed,,Talk (25 mins + Q&A),Data Science and Visualisation,2023-05-15T14:14:30.471968+00:00,,"From sensor data to epidemic outbreaks, particle dynamics to environmental monitoring, much of crucial real world data has temporal nature. Fundamental challenges facing data specialist dealing with time series include not only predicting the future values, but also determining when these values are alarming. Standard anomaly detection algorithms and common rule-based heuristics often fall short in addressing this problem effectively. In this talk, we will closely examine this domain, exploring its unique characteristics and challenges. You will learn to apply some of the most promising techniques for detecting time series anomalies as well as relevant scientific Python tools that can help you with it.","This talk will walk you through several most common and effective approaches for tackling anomaly detection in time series, while explaining why traditional anomaly detection techniques might not be very applicable here. Among these approaches we will discuss rule-based anomaly detection, Error-Trend-Seasonality decomposition, structural modelling approach, and short-term forecasting model solutions. Each time we will differentiate between different types of temporal anomalies and why each method may or may not be suited for them. Further, for each approach we will consider several open-source scientific Python tools such as scipy, statsmodels, Prophet, tensorflow / keras and more. At the center of our conversation will be a real-world dataset from the field of environmental monitoring, which can also be easily translated into other fields.","Note for organisers: if necessary this talk can be shortened into a 15 minute one if 30-minute format would not be possible.

Additional note: I am applying after a helpful suggestion to participate from one of the EuroSciPy organisers/volunteers whom I met after my (different) talk at PyData Berlin this year",,30,1,en,True,False,,JF8QLJ,Vadim Nelidov,HS 120,2023-08-16T08:30:00+00:00,2023-08-16T11:00:00+02:00,2.00,1.8,,"This talk will introduce you to Anomaly Detection in Time Series, from why this field is unique to how to applying some of the most promising detection techniques and which scientific Python tools can help you with that",,,,,Data Analysis and Data Engineering,some,some,no,,
SZZ8Z7,Scaling pandas to any size with PySpark,confirmed,,Talk (25 mins + Q&A),High Performance Computing,2023-07-15T08:38:08.722117+00:00,,"This talk discusses using the pandas API on Apache Spark to handle big data, and the introduction of Pandas Function APIs. Presented by an Apache Spark committer and a product manager, it offers technical and managerial insights.","Undoubtedly, pandas plays a crucial role in data wrangling and analysis tasks. However, its limitation lies in handling big data processing. This creates a dilemma for data practitioners: should they sacrifice information by downsampling the data, or should they explore distributed processing frameworks to handle larger workloads? One popular option is Apache Spark, a mainstream distributed processing tool. Yet, using Spark means learning a new language, PySpark, which can be a challenge.

Thankfully, there is a silver lining. The pandas API on Spark offers equivalent functionalities to pandas in PySpark. This allows pandas users to seamlessly transition from single-node to distributed environments by merely replacing the pandas package with pyspark.pandas.

Conversely, existing PySpark users may need to create custom user-defined functions (UDFs) that are not available in the PySpark API. With the introduction of Pandas Function APIs in Spark 3.0+, users can now apply arbitrary Python native functions with type hints, using pandas instances as input and output, on a PySpark dataframe. This empowers data scientists to train ML models based on each data group with just a single line of code.

And, you don't even need to write PySpark code now! English is the new programming language and we will introduce the English SDK for PySpark. The English SDK understands Spark tables and DataFrames, handles the complexity for you behind the scenes, and returns a DataFrame directly based on your English questions and directions.

In a joint presentation by a top open-source Apache Spark committer and a product manager, this talk has both the software engineer and product manager perspectives. Prior working knowledge of pandas, basic Spark, and machine learning will be helpful for the audience.",,,30,1,en,False,False,,"SDEGBS, ERMJYC","Hyukjin Kwon, Allan Folting",HS 120,2023-08-17T14:05:00+00:00,2023-08-17T16:35:00+02:00,,,,Scaling data workloads using the best of both worlds: pandas and Spark,Parallel Computing,Science and Engineering Portals,Other,Other,Data Analysis and Data Engineering,some,some,,,
TVJEAD,Transformations in Three Dimensions,confirmed,,Talk (15 mins + Q&A),Scientific Applications,2023-04-04T13:32:47.526904+00:00,,"Rigid transformation in 3D are complicated due to the multitude of different conventions and because they often form complex graphs that are difficult to manage. In this talk I will give a brief introduction to the topic and present the library pytransform3d as a set of tools that can help you to tame the complexity. Throughout the talk I will use examples from robotics (imitation learning, collision detection, state estimation, kinematics) to motivate the discussed features, even though presented solutions are useful beyond robotics.","This talk focuses on rotation and translation, that is, rigid transformations, in three  dimensions. There are various representations of these. We often combine several software components with different conventions. Furthermore, we usually combine multiple transformations that form complex graphs of transformations, and we are often interested in transformations that are not directly available, but can be computed from a combination of multiple transformations. Both problems can be handled with pytransform3d, a Python library for transformations in three dimensions.

pytransform3d offers...

* operations for most common representations of rotation / orientation and
  translation / position
* conversions between those representations
* clear documentation of conventions
* tight coupling with matplotlib to quickly visualize (or animate)
  transformations
* the TransformManager which organizes complex chains of transformations
* the UrdfTransformManager which is able to load transformations from URDF
  files
* a matplotlib-like interface to Open3D’s visualizer to display geometries and
  transformations

I will present several features of the library in this talk and I will use examples from robotics for illustration, for example,

* imitation learning - learning robotic motion from human demonstration
* kinematics - translation of a human hand motion to a robotic hand
* collision detection - between a robot arm and it's environment
* state estimation - estimation of a robot's location and its uncertainty

There are several pitfalls that we will discuss as well.",,,20,1,en,False,False,,33UUJG,Alexander Fabisch,Aula,2023-08-17T09:45:00+00:00,2023-08-17T12:05:00+02:00,2.00,2.0,,Introduction to 3D transformations in Python illustrated with problems in robotics,,,,Robotics & IoT,,none,some,yes,,https://github.com/dfki-ric/pytransform3d
UBT8PH,Why I Follow CI/CD Principles When Writing Code: Building Robust and Reproducible Applications,confirmed,,Talk (25 mins + Q&A),Scientific Applications,2023-04-28T19:54:26.230060+00:00,"New track - MLOps, Deployment","This talk will discuss the importance of Continuous Integration and Continuous Delivery (CI/CD) principles in the development of scientific applications, with a focus on creating robust and reproducible code that can withstand rigorous testing and scrutiny. The presentation will cover best practices for project structure and code organization, as well as strategies for ensuring reproducibility, collaboration, and managing dependencies. By implementing CI/CD principles in scientific application development processes, researchers can improve efficiency, reliability, and maintainability, ultimately accelerating research.","Scientists often face the challenge of creating robust and reproducible code that can withstand rigorous testing and scrutiny, which is essential to ensure the validity and reliability of research findings. This talk will explore how Continuous Integration and Continuous Delivery (CI/CD) principles can be applied in the context of scientific application development to help address these challenges.

The presentation will begin with the code organization for maintainability and testability. Reproducibility is a critical aspect of scientific research, and the talk will explore how version control and automated testing can help ensure that code is reproducible and that results can be validated by other researchers. Collaboration is also essential, and strategies for setting up a repository and introducing automated testing to ensure the codebase is consistent and minimise errors will be discussed.

The talk will also show the benefits of running automated tests at every stage of the development process, which can catch errors and defects early on, leading to higher quality and reduced risk of errors and inconsistencies. 

Finally, during the presentation I will explore effective strategies for managing dependencies using tools like Renovate, which can automate dependency updates. By implementing CI/CD principles in scientific application development processes, scientists can improve efficiency, reliability, and maintainability, ultimately accelerating research.

In this talk, I will cover:
* The basic principles of CI/CD and how they apply to scientific application development
* Best practices for project structure and code organization to enhance maintainability and testability
* The benefits of running automated tests and ensuring reproducibility in scientific research
* How CI/CD facilitates collaboration and enables robust, error-free applications
* Effective strategies for managing dependencies with tools like Renovate",,,30,1,en,False,False,,3W79QG,Artem Kislovskiy,HS 120,2023-08-16T11:30:00+00:00,2023-08-16T14:00:00+02:00,1.50,1.5,,"Learn how CI/CD principles can help scientists develop robust and reproducible code for their applications. Join me as I share best practices for project structure, automated testing, and managing dependencies to accelerate research.",,,,Other,,none,some,no,,
UCAYVT,Chalk’it: an open-source framework for rapid web applications,confirmed,,Talk (15 mins + Q&A),Data Science and Visualisation,2023-04-29T22:36:53.210826+00:00,request as poster,"Chalk'it is an open-source framework that transforms Python scripts into distributable web app dashboards. It utilizes drag-and-drop widgets to establish an interface linked to a dataflow connecting Python code and various data sources. Chalk'it supports multiple Python graphics libraries, including Plotly, Matplotlib and Folium for interactive mapping and visualization. The framework operates entirely in web browsers using Pyodide. In our presentation, we will showcase Chalk'it, emphasizing its primary features, software architecture, and key applications, with a special focus on geospatial data visualization.","Chalk'it is a new open-source framework designed for effortlessly converting Python scripts into shareable standalone web application dashboards. Powered by Pyodide, Chalk'it allows for installing and running Python packages directly in the browser. It supports various graphical libraries like Matplotlib, Plotly, and JavaScript libraries such as ECharts, Vega, Plotly.js, and Leaflet. Dashboard design is streamlined through intuitive widget drag-and-drop functionality, while data and widget interactions are established via bidirectional bindings called connections. Scripts execution and orchestration are organized as a directed acyclic graph, and besides Python, Chalk'it can also use JavaScript libraries.

Chalk'it's web-application development revolves around the following concepts:

* Workspace: Comparable to Matlab or IPython workspaces, it contains JSON or Python objects with specific states, accessible to other tool entities.
* DataNode: Similar to a Jupyter Notebook cell, it functions as a JSON result producer or a Python object output. DataNodes are typically evaluated based on user interactions or at predetermined intervals.
* “DataNodes” keyword: Facilitates dataflow creation between dataNodes (nodes) and accesses data from other nodes, establishing execution dependencies on preceding nodes.
* Execution graph: Represents the sum of dependency relationships implicitly defined by using the “dataNodes” keyword, organizing application logic. User actions on widgets trigger the relevant dataflow execution.
* Widget: A graphic object defined by an HTML page layout rectangle, interacting with the workspace through connection points called actuators. Actions include clicking buttons, entering text/numeric values, scrolling cursors, and selecting items from drop-down lists.
* Connection: Defines the link between a widget (via its actuator) and a dataNode.
* Document: A JSON document that describes the entire working model, including the Chalk'it dashboard. The document has an “xprjson” extension.
* Page: The result of the xprjson document, an HTML page containing the dashboard, acting as the web application. It can be hosted on a static page server and shared via its URL.

While popular Python dashboard-building packages like Streamlit and Plotly Dash exist, Chalk'it differentiates itself with four main features: drag-and-drop dashboard editing, document-based dashboard serialization, dataflow application description, and full in-browser execution. 

Chalk'it originates from an IFPEN tool called xDash, utilized internally for creating hundreds dashboards for various topics, such as CO2 reduction at city level, real-driving emissions characterization, image-based rock geological feature prediction, or catalyst performance estimation.

During our presentation, we will introduce the Chalk'it tool and demonstrate its primary features. We will then explore its software architecture and present two application examples: geospatial data analysis and car total cost of ownership estimation. We will conclude with a discussion of the roadmap.

Please visit our Github repository: https://github.com/ifpen/chalk-it

Please also visit the templates galleries, that can be explored online using the hosted version of Chalk’it : https://ifpen.github.io/chalk-it/",,,20,1,en,False,False,,JJ8PPC,Mongi BEN GAID,HS 120,2023-08-16T12:40:00+00:00,2023-08-16T15:00:00+02:00,2.00,2.0,"https://pretalx.com/media/euroscipy-2023/submissions/UCAYVT/resources/chalk-it_zFLeEsF.png, https://pretalx.com/media/euroscipy-2023/submissions/UCAYVT/resources/live-demo-py_tIx3BLL.png","Introducing Chalk'it: an open-source framework turning Python scripts into web app dashboards with drag-and-drop widgets. Enjoy multiple graphics libraries & interactive mapping, all in-browser via Pyodide! Stay tuned for our demos",,,,,Data Analysis and Data Engineering,some,some,yes,,https://github.com/ifpen/chalk-it
URZEJE,Image processing with scikit-image,confirmed,,Tutorial,Scientific Applications,2023-07-14T17:57:11.660971+00:00,,"Image data are used in many scientific fields such as astronomy, life sciences or material sciences. This tutorial will walk you through image processing with the scikit-image library, which is the numpy-native image processing library of the scientific python ecosystem.

The first hour of the tutorial will be accessible to beginners in image processing (some experience with numpy array is a pre-requisite), and will focus on some basic concepts of digital image manipulation and processing (filters, segmentation, measures). In the last half hour, we will focus on more advanced aspects and in particular Emma will speak about performance and acceleration of image processing.","Image data are used in many scientific fields such as astronomy, life sciences or material sciences. This tutorial will walk you through image processing with the scikit-image library, which is the numpy-native image processing library of the scientific python ecosystem.

The goal of the tutorial is to give confidence to beginners in image processing to get started processing their images with scikit-image, to understand some basic concepts of image processing and to understand how to find help and documentation to go further after the tutorial. For more advanced users, a last part will focus on performance aspects.

The first hour of the tutorial will be accessible to beginners in image processing. Some experience with manipulating numpy arrays is a pre-requisite. We will first explain how to manipulate image pixels as elements of numpy arrays, and how to make basic transformation of images through numpy arrays. Then we will move to some basic concepts of digital image processing:
- image histogram and contrast
- image filtering: transformations of an image resulting in a new image of similar size (for example, thresholding, edge enhancement, etc.)
- image segmentation: partitioning an image into several regions (objects)
- measures on binary images
This part will be hands-on with several exercises, and we will show how to use the scikit-image documentation to find relevant information.

In the last half hour, Emma will focus on more advanced aspects and in particular will speak about performance and acceleration of image processing.",,,90,1,en,False,False,,"KMDJAL, DG7LSG","Guillaume Lemaitre, Joan Massich",HS 120,2023-08-15T13:30:00+00:00,2023-08-15T17:00:00+02:00,,,,Getting started with image processing using scikit-image: make the most out of your image data,Parallel Computing,Learning and Teaching Scientific Python,Supervised Learning,Astronomy,Data Analysis and Data Engineering,none,some,,,
UVBBQZ,Exploring GPU-powered backends for scikit-learn,confirmed,,Talk (25 mins + Q&A),High Performance Computing,2023-04-29T21:47:15.337993+00:00,,"Could scikit-learn future be GPU-powered ? This talk will discuss the performance improvements that GPU computing could bring to existing scikit-learn algorithms, and will describe a plugin-based design that is being foresighted to open-up scikit-learn compatibility to faster compute backends, with special concern for user-friendliness, ease of installation, and interoperability.","GPUs are known to be the preferred hardware for deep-learning based applications, but their use for a wide range of other algorithms has also been proved to be relevant: k-means, random forests, nearest neighbors search,... CPU-based implementations can be outshined and more particularly so where the data is plentiful to the point where the duration for training an estimator becomes a bottleneck. But at what point does it really start to matter, and can it really be a concern for scikit-learn users ? we explore a few usecases to try to highlight what is at stake.

But bringing more options for accelerated computing backends could challenge the principles of ease-of-use, ease of installation, and user friendliness, that are at the core of scikit-learn design. As for today, GPU computing software doesn't benefit from seemlessly cross-vendor portability features as much as CPU software does, as a result end-users risk confusion with choosing carefully the hardware and compatible libraries, some of which could be proprietary, and could face high interoperability cost if changing the hardware requires changing the software stack. The talk introduces the open-source SYCL-based software toolchain that aims at unlocking interperobility accross all hardware accelerators and all manufacturers.

The scikit-learn library furthermore envisions a plugin-based system that enable external projects to provide alternative compute backends to existing estimators. The plugin-based system eases the development and distributions of backends that could be maintained under the umbrella of the scikit-learn project, while also opening up to third-party providers. Plugins should be easily pip- or conda- installables, seemlessly unlock better performance for scikit-learn estimators, should conform to the same specifications and the same quality standards than scikit-learn default engines, and be swappables so that all users can keep porting and sharing their estimators without regard to the compute backend it has been trained with. Several plugins are being experimented with currently, such as the `sklearn_numba_dpex` project that uses the OneAPI-based toolchain.",,,30,1,en,False,False,,"NEUMLP, LPYNCD","Olivier Grisel, Franck Charras",Aula,2023-08-17T13:30:00+00:00,2023-08-17T16:00:00+02:00,2.00,2.0,,Could scikit-learn future be GPU-powered ? The talk discusses the performance improvements that GPU computing could bring to existing scikit-learn algorithms,Other,,,,,some,some,yes,,https://github.com/soda-inria/sklearn-numba-dpex
UX8CTK,Keynote on polars,confirmed,,Keynote,,2023-06-28T20:01:56.050606+00:00,,"Polars is the ""relatively"" new fast dataframe implementation that redefines what DataFrames are able to do on a single machine, both in regard to performance and dataset size.
In this talk, we will dive into polars and see what makes them so efficient. It will touch on technologies like Arrow, Rust, parallelism, data structures, query optimization and more.","Polars is the ""relatively"" new fast dataframe implementation that redefines what DataFrames are able to do on a single machine, both in regard to performance and dataset size.
In this talk, we will dive into polars and see what makes them so efficient. It will touch on technologies like Arrow, Rust, parallelism, data structures, query optimization and more.
Come to this keynote to learn more about the DataFrame world.",,,60,1,en,False,False,,37ZAQC,Ritchie Vink,Aula,2023-08-17T07:00:00+00:00,2023-08-17T10:00:00+02:00,,,,Keynote on the Polars DataFrame library and query engine,Parallel Computing,Learning and Teaching Scientific Python,Supervised Learning,Astronomy,Data Analysis and Data Engineering,none,some,,,
V7KNNE,Integrating Ethics in ML: From Philosophical Foundations to Practical Implementations,confirmed,,Keynote,,2023-06-28T19:54:56.781234+00:00,,"In the rapidly evolving landscape of Machine Learning (ML), significant advancements like Large Language Models (LLMs) are gaining critical importance in both industrial and academic spheres. However, the rush towards deploying advanced models harbors inherent ethical tensions and potential adverse societal impacts. The keynote will start with a brief introduction to the principles of ethics, viewed through the lens of philosophy, emphasizing how these fundamental concepts find application within ML. Grounding our discussion in tangible realities, we will delve into pertinent case studies, including the BigScience open science initiative, elucidating the practical application of ethical considerations. Additionally, the keynote will touch upon findings from my recent research, which investigates the synergy between ethical charters, legal tools, and technical documentation in the context of ML development and deployment.","In the rapidly evolving landscape of Machine Learning (ML), significant advancements like Large Language Models (LLMs) are gaining critical importance in both industrial and academic spheres. However, the rush towards deploying advanced models harbors inherent ethical tensions and potential adverse societal impacts. The keynote will start with a brief introduction to the principles of ethics, viewed through the lens of philosophy, emphasizing how these fundamental concepts find application within ML. Grounding our discussion in tangible realities, we will delve into pertinent case studies, including the BigScience open science initiative, elucidating the practical application of ethical considerations. Additionally, the keynote will touch upon findings from my recent research, which investigates the synergy between ethical charters, legal tools, and technical documentation in the context of ML development and deployment.",,,60,1,en,False,False,,KGWFV8,Giada Pistilli,Aula,2023-08-16T07:00:00+00:00,2023-08-16T10:00:00+02:00,,,,Integrating Ethics in ML: From Philosophical Foundations to Practical Implementations,Parallel Computing,Learning and Teaching Scientific Python,Supervised Learning,Astronomy,Data Analysis and Data Engineering,none,some,,,
WBSYCM,"Predictive survival analysis with scikit-learn, scikit-survival and lifelines",confirmed,,Tutorial,Machine and Deep Learning,2023-05-14T16:34:39.441721+00:00,,"This tutorial will introduce how to train machine learning models for time-to-event prediction tasks (health care, predictive maintenance, marketing, insurance...) without introducing a bias from censored training (and evaluation) data.","Tutorial notebooks:

- https://github.com/soda-inria/survival-analysis-benchmark

According to Wikipedia:

Survival analysis is a branch of statistics for analyzing the expected duration of time until one event occurs, such as deaths in biological organisms and failure in mechanical systems. [...]. Survival analysis attempts to answer certain questions, such as what is the proportion of a population which will survive past a certain time? Of those that survive, at what rate will they die or fail? Can multiple causes of death or failure be taken into account? How do particular circumstances or characteristics increase or decrease the probability of survival?

In this tutorial we will deep dive into a practical case study of predictive maintenance using tools from the scientific Python ecosystem. Here is a tentative agenda:

- What is time-censored data and why it is a problem to train time-to-event regression models.
- Single event survival analysis with Kaplan-Meier using scikit-survival.
- Evaluation of the calibration of survival analysis estimators using the integrated brier score (IBS) metric.
- Predictive survival analysis modeling with Cox Proportional Hazards, Survival Forests using scikit-survival, GradientBoostedIBS implemented from scratch with scikit-learn.
- How to use a trained GradientBoostedIBS model to estimate the median survival time and the probability of survival at a fixed time horizon.
- Inspecting the learned statistical association between input features and survival probabilities using partial dependence plot.

The tutorial notebooks also contain additional material that we probably won't have time to present in 90 min, namely:

- Competing risks modeling with Nelson–Aalen, Aalen-Johansen using lifelines.
- Estimation of the cause-specific cumulative incidence function (CIF) using our GradientBoostedIBS model.
- Extracting implicit failure data from operation logs using sessionization with Ibis and DuckDB.

Target audience: good familiarity with machine learning concepts, with prior experience using scikit-learn (you know what cross-validation means and how to fit a Random Forest on a Pandas dataframe).",An 4h30 version of this tutorial was presented at JupyterCon 2023.,,90,1,en,True,False,,"NEUMLP, QCVBZD","Olivier Grisel, Vincent Maladiere",Aula,2023-08-14T13:30:00+00:00,2023-08-14T17:00:00+02:00,2.00,2.0,,"Survival Analysis with scikit-learn, scikit-survival and lifelines",,,Supervised Learning,,,expert,some,,https://github.com/soda-inria/survival-analysis-benchmark,
WJTLSW,What-not to expect from NumPy 2.0,confirmed,,Maintainer track,Scientific Applications,2023-05-14T21:19:53.804143+00:00,,"NumPy is planning a 2.0 release early next year replacing the 1.X release.  While we hope that the release will not be disruptive to most users we do plan some larger changes that may affect many.  These changes include modifications to the Python and C-API, for example making the NumPy promotion rules more consistent around scalar values.","The release of a NumPy 2.0 has long been avoided since NumPy tries to have a high threshold for breaking changes.  But due to its age, NumPy has also numerous issues which are difficult to change through a slow deprecation process.
We have finally reached the decision to release a NumPy 2.0 in order to address some of these issues.  A main issue is adoption of NEP 50 to change the scalar promotion rules which will make them more consistent and also is part of pushing towards Array-API adoption.  Further clean-ups of the Python API and changes such as making 64bit integers the default on 64bit windows are also planned.
While our C-API will hopefully get some additions to start supporting our new API around ufuncs and DTypes, we also plan to remove or change API to allow evolution and simplify it.  Let's review some of these changes and open discussion about these or other changes.

While it is good to move forward, it is also very important that a majority of users will not have difficulties with updating or transitioning.  Let's discuss potential issues and solutions.",,,45,1,en,False,False,,QKVYNA,Sebastian Berg,HS 119 - Maintainer track,2023-08-16T12:15:00+00:00,2023-08-16T15:00:00+02:00,2.00,2.0,,What-not to expect from NumPy 2.0:  Let's discuss what is planned for NumPy 2.0 and what is not.,,,,Other,,none,expert,,https://numpy.org/neps/,https://github.com/numpy/numpy
WPEXXC,Generating Data Frames for your test - using Pandas stratgies in Hypothesis,confirmed,,Tutorial,Machine and Deep Learning,2023-04-30T11:38:19.967487+00:00,,"Do you test your data pipeline? Do you use Hypothesis? In this workshop, we will use Hypothesis - a property-based testing framework to generate Pandas DataFrame for your tests, without involving any real data.","In this short 90 mins workshop, we will first go through the basics of hypothesis and what is property-based testing. After that, we will introduce the strategies for Pandas objects - available via the extras in Hypothesis. We will have a glimpse of what the strategies are doing to generate the testing object, including Pandas Series and DataFrames. In the end, we will apply what we learn in real testing applications - testing a data pipeline that involves DataFrames.

## Outline
- Introduction of Property-based testing (15 mins)
- Introduction and basic use of Hypothesis exercises (30 mins)
- Deep dive into Pandas strategies (20 mins)
- Do it yourself - apply property-based testing to data pipelines (20 mins)
- Conclusion (5 mins)

## Prerequisits
No prior knowledge of property-based testing or hypothesis is required. However, we assume the attendee has experience using Pandas and has a basic understanding of Pandas objects. Knowledge about Numpy array and typing would also be beneficial in understanding the Pandas Strategies.

## Goal
We hope the attendee will learn about property-based testing and see how it can benefit their work involved data - especially those that use Pandas. After the workshop, attendees should be able to understand how the Pandas strategies in Hypothesis works and to use Hypotheses to test codes that involve Pandas Series or DataFrame input.","I am a contributor to Hypothesis, especially with the Numpy and Pandas support",,90,1,en,False,False,,8EGVC9,Cheuk Ting Ho,Aula,2023-08-15T11:30:00+00:00,2023-08-15T15:00:00+02:00,2.00,2.0,,"Do you test your data pipeline? Do you use Hypothesis? In this workshop, we will use Hypothesis - a property-based testing framework to generate Pandas DataFrame for your tests, without involving any real data.",,,Reproducible Machine Learning,,,some,some,,,https://github.com/HypothesisWorks/hypothesis/
X8LYJY,Introduction to Geospatial Machine Learning with SRAI,confirmed,,Tutorial,Scientific Applications,2023-05-14T21:07:37.043901+00:00,,"This tutorial offers a thorough introduction to the srai library for Geospatial Artificial Intelligence. Participants will learn how to use this library for geospatial tasks like downloading and processing OpenStreetMap data, extracting features from GTFS data, dividing an area into smaller regions, and representing regions in a vector space using various spatial features. Additionally, participants will learn to pre-train and fine-tune models for downstream tasks.","In this tutorial, we intend to provide a comprehensive introduction to the Spatial Representations for Artificial Intelligence (srai) library. Participants will learn how to utilize this library for various geospatial applications, such as downloading and processing OpenStreetMap data, extracting features from GTFS data, splitting a given area into smaller regions, and embedding regions into a vector space based on different spatial features. Moreover, users will learn how to pre-train a model of their choice and fine-tune existing pre-trained models for use in downstream tasks.

By the end of the tutorial, attendees will be able to:
1. Install and set up the SRAI library.
2. Use SRAI to download and process geospatial data.
3. Apply various regionalization and embedding techniques to geospatial data.
4. Utilize pre-trained embedding models for clustering and similarity search.
5. Pre-train available models from scratch.
6. Fine-tune pre-trained models to use them in downstream tasks.
7. Understand the potential applications and future enhancements of the SRAI library.",,,90,1,en,True,False,,"NEMRFE, DLQCAA, CEEVGF, AEKH3Q, PHTF7Q","Szymon Woźniak, Piotr Szymański, Piotr Gramacki, Kamil Raczycki, Kacper Leśniara",Aula,2023-08-14T08:30:00+00:00,2023-08-14T12:00:00+02:00,2.00,2.0,,"Join us for an in-depth tutorial on the srai library, a powerful tool for GeoAI. Learn to handle geospatial data, create regions representations, and fine-tune specialized models",,,,Geo Science,,some,some,,https://srai-lab.github.io/srai,https://github.com/srai-lab/srai/
XPHSMW,From ab-initio to scattering experiments: A Python workflow for constructing neuroevolution potentials,confirmed,,Poster,Scientific Applications,2023-04-19T11:12:45.224453+00:00,,"Computer simulations such as Molecular Dynamics (MD) are an effective tool in studying a broad range of structural and dynamical properties in atomic-scale systems, but the connection to X-ray or neutron scattering experiments is not always straightforward. 
In this work, we present a Python workflow that starts with the construction of machine learned potentials and ends with the prediction of experimental observables, such as the dynamical structure factor.
Specifically, the Python package `Calorine` is used to train neuro-evolution potentials (NEP) with ab-initio level accuracy using the software package `GPUMD`, whilst still retaining a familiar interface based around the Python framework `ASE`. 
The trained potentials are then used to run MD within GPUMD, also done through `Calorine`.
Finally, the resulting MD trajectories are analyzed with the Python package `Dynasor` in order to calculate dynamical structure factors, which in turn may be convoluted with instrument parameters in order to compare to experiments.
We apply our workflow to crystalline benzene as a prototype system, but the workflow may be readily extended to different systems.","Increased understanding of materials is paramount for transitioning to a sustainable society. In particular, energy materials for use in solar cells are key in such a transition. In order to maximize the efficacy of these materials, one needs to optimize their optical properties, which are governed by quantum mechanics. 

Quantum mechanical simulations, using tools such as Density Functional Theory (DFT), are well suited for this task. They yield detailed and accurate understanding of the system at the level of atoms. However, the computational cost of DFT scales prohibitively with the number of atoms in the system, making them intractable for many systems and materials of great interest. 

One example of a class of such systems are liquid chromophores, which are organic molecules renowned for their optical properties, with uses in solar cells, solvent-free dyes and light emitting diodes. These properties are affected by supramolecular interactions, for instance with the molecules forming stacks, and understanding of how these structures are formed is important to be able to optimize these chromophore systems. Simulations of these stacks requires system sizes of tens of thousands of atoms, placing them out of reach of what is possible with DFT.

Machine Learning (ML) offers a possible solution to this problem. Specifically, by using ML models to approximate DFT, so called ML force fields, accurate simulations of sufficiently large systems can be performed at a fraction of the cost of DFT. 

In this work, we present a workflow for developing ML force fields starting from reference data generated with DFT, using the Python package Calorine. We construct Neuroevolution Potentials (NEP), which are highly accurate and efficient neural network potentials implemented in the GPUMD package. We then demonstrate how to use the models to predict experimentally relevant observables, such as dynamical structure factors which are of particular interest in neutron scattering experiments, using the Python package Dynasor. Here, we study crystalline benzene as a prototype system, but the work can be readily extended to other systems and materials.",,,90,1,en,False,False,,NASG3L,Eric Lindgren,,,,2.00,1.8,,Machine Learning enables accurate simulations of important materials for the transition to a sustainable future. We present a workflow for efficiently creating and using such models.,,,,"Simulations (e.g., Physics, CFD, ESMs)",,some,some,,https://calorine.materialsmodeling.org/,https://calorine.materialsmodeling.org/
YP9N8H,GPT generated text detection: problems and solution in the scientific publishing,confirmed,,Talk (15 mins + Q&A),Machine and Deep Learning,2023-06-02T12:06:58.420044+00:00,,"Since its release, ChatGPT is now widely adopted as ""the"" text generation tool used across all industries and businesses. This also includes the domain of scientific research where we do observe more and more scientific papers partially or even fully generated by AI. The same also applies to the peer-reviews reports created while reviewing a paper.

What are the guidelines in the scientific research world? What is now the meaning of the written word and how do we build a model that can identify whether a text is AI-generated? What are the potential solutions to solve this important issue?

Within this talk, we are discussing on how to detect AI-generated text and  how to create a scalable architecture integrating this tool.","ChatGPT and its Open Source alternatives are nowadays being integrated in different text writing workflows. The scientific research, being also heavenly impacted by those new technologies, creates a lot of debate about whether or not these technologies should be used for scientific papers writing. The COPE guidelines are very clear about it, however, there is currently no effective tools available that can  efficiently identify whether a text is AI-generated.

There are already some promising principles on how to solve this problem that can be categorized as: watermarking, likelihood detection, and classification. How can these algorithms be used within the scientific writing to detect AI-generated text and how to integrate them into the AI-infrastructure using Python and other modern tools (such as vector search engine for example)?","We have submitted two talks, this one entitled ""GPT generated text detection: problems and solution in the scientific publishing"" and a previous one entitled ""Building an AI-Infrastructure, that works, at reasonable scale"". Please chose one of the two talks than best matches the conference. There is also a possibility we present both talks, as an option.",,20,1,en,True,False,,"PP3NZ9, SBADQX","Dr. Milos Cuculovic, Andrea Guzzo",HS 120,2023-08-16T09:40:00+00:00,2023-08-16T12:00:00+02:00,1.00,1.0,,GPT generated text detection: problems and solution in the scientific publishing,,,Other,,,some,some,yes,,
YQNNB7,MLEES - An Initial Guide through the Thickets of Machine Learning Research in Earth and Environmental Sciences,confirmed,,Poster,"Community, Education, and Outreach",2023-05-02T18:52:41.709655+00:00,,"Students in earth and environmental sciences generally have limited time dedicated to learning programming. As machine learning (ML) techniques become more widespread in our fields, we find that providing course materials that introduce the concepts behind ML techniques and materials showcasing the use of techniques in recent literature helps students develop the skills necessary to apply ML to their own research. We tackle each ML concept through notebooks that use benchmark datasets and take inspiration from renowned references. We then focus on reproduction of the work in over 5 papers/preprints that successfully applied ML to climate science, emphasizing the importance of reproducibility and establishing trust in ML research. We also provide a platform to highlight the result of our students' work via final projects in which they apply the course’s concepts in their Master's research. These further show how the course promotes work at the intersection of environmental science and elementary ML methods. The materials are open source, hosted on GitHub, and further developments will increase the variety in deep learning algorithms presented throughout the course. We also provide materials covering an introduction to the Python skills required to follow the notebooks.  Finally, part of the materials have already been used in a Massively Open Online Course (MOOC) developed by ECMWF, we hope to continue with development of a more robust Jupyter book and to develop our own MOOC.","Students in earth and environmental sciences generally have a very limited amount of time dedicated to learning how to program. As the use of machine learning techniques becomes more widespread in our fields, we find that providing course materials that introduce the concepts behind classical and novel ML techniques as well as accompanying materials that showcase the use of techniques in recent literature helps students develop the skills necessary to apply ML to their own research. We tackle each machine learning concept through pedagogical notebooks that rely on benchmark datasets (e.g., MNIST and the Palmer's Penguins Dataset) and that take inspiration from well known references. Then, we focus on reproduction of the work in over 5 papers and preprints, emphasizing the importance of reproducible work and ways of understanding and establishing trust in ML research. To do this, we have developed a collection of Jupyter Notebooks that rely on the use of Python and a number of fundamental libraries (e.g., Scikit-Learn, Pandas, PyTorch, TensorFlow, and Matplotlib) to guide students through understanding the difference between classification and regression algorithms, performance benchmarking and the use of baselines, and approaches to using modern deep learning algorithms for research. We also provide a platform to highlight the result of our students' work in the course via a final project that is meant to help the students apply the concepts from the course in their Master's research. The final projects, archived at https://wp.unil.ch/dawn/ml_ees_projects/, demonstrate that the course's current version promotes work at the intersection of environmental science and elementary machine learning methods. The materials themselves are open source and made available on GitHub, and further developments will increase the variety in deep learning algorithms presented throughout the course. In addition, we provide materials covering a brief introduction to the programming skills in Python required to follow the notebooks.  Finally, given that part of the course materials have already been used in a Massively Open Online Course (MOOC) developed by ECMWF (lms.ecmwf.int), we hope to continue with development of a more robust Jupyter book and eventually develop our own MOOC.","Collaborators: Tom Beucler, Iat Hin-Tam, Jingyan Yu

The poster is meant to provide a complete overview of the course, QR codes and descriptions for each of the notebooks and key figures of what they cover.",,90,1,en,True,False,,JZ3LXH,Milton Gomez,,,,2.00,2.0,,A Guided Tour of Machine Learning for Environmental Sciences,,Learning and Teaching Scientific Python,,,,some,some,,,
YTW3UF,Pandas 2.0 and beyond,confirmed,,Talk (25 mins + Q&A),Data Science and Visualisation,2023-05-14T20:35:10.141011+00:00,,Pandas has reached a 2.0 milestone in 2023. But what does that mean? And what is coming after 2.0? This talk will give an overview of what happened in the latest releases of pandas and highlight some topics and major new features the pandas project is working on,"The pandas 2.0 release is targeted for the first quarter of 2023. This is a major milestone for the pandas project, and this talk will start with an overview of this release. Pandas 2.0 includes some new (experimental) features, but mostly means enforcing deprecations that have been accumulated in the 1.x series, along with some necessary breaking changes.

But that doesn’t mean there are no interesting features to talk about! The main part of the presentation will showcase some new features, both already released as opt-in features or to come in future releases.
Support for non-nanosecond resolution datetimes, allowing time spans ranging over a billion of years. Improved support for nullable data types, including easy opt-in options for I/O functions. Experimental integration with pyarrow to back columns of a DataFrame (beyond the string dtype).
A major change that is under way is a change to the copy and view semantics of operations in pandas (related to the well-known (or hated) SettingWithCopyWarning). This is already available as an experimental opt-in to test and use the new behaviour, and will probably be a highlight of pandas 3.0.",,,30,1,en,False,False,,7VUXWM,Joris Van den Bossche,Aula,2023-08-16T09:05:00+00:00,2023-08-16T11:35:00+02:00,2.00,2.0,,Pandas has reached a 2.0 milestone in 2023. But what does that mean? And what is coming after 2.0? This talk will give an overview of what happened in the latest releases of pandas and highlight some topics and major new features the pandas,,,,,Data Analysis and Data Engineering,some,none,no,,https://github.com/pandas-dev/pandas/
ZA3FUY,Timing and Benchmarking Scientific Python,confirmed,,Talk (25 mins + Q&A),Data Science and Visualisation,2023-05-07T08:02:58.284914+00:00,,"Scientific code is often complex, resource-intensive, and sensitive to performance issues, making accurate timing and benchmarking critical for optimising performance and ensuring reproducibility. However, benchmarking scientific code presents several challenges, including variability in input data, hardware and software dependencies, and optimisation trade-offs. In this talk, I discuss the importance of timing and benchmarking for scientific code and outline strategies for addressing these challenges. Specifically, I emphasise the need for representative input data, controlled benchmarking environments, appropriate metrics, and careful documentation of the benchmarking process. By following these strategies, developers can effectively optimise code performance, select efficient algorithms and data structures, and ensure the reliability and reproducibility of scientific computations.","Scientific code plays a crucial role in advancing scientific research and discovery, but its complexity, resource-intensiveness, and sensitivity to performance issues make accurate timing and benchmarking critical for optimal performance and reproducibility. To this end, this talk addresses the importance of timing and benchmarking for scientific code, and outlines the challenges and strategies associated with it.

One of the main challenges in benchmarking scientific code is the variability of input data, which can influence the benchmarking results. To overcome this, it is essential to use representative input data that accurately reflects real-world scenarios. In addition, it is crucial to establish a controlled benchmarking environment to minimise the impact of external variables on the results. This includes running benchmarks on the same hardware and software configurations, using the same input data, and running multiple trials to ensure consistency.

Another challenge is the choice of appropriate metrics to measure performance. Depending on the specific requirements of the application, this may involve measuring execution time, memory usage, or other metrics. In addition, optimisation trade-offs can also affect benchmarking results, highlighting the importance of carefully balancing performance with other factors such as accuracy and maintainability.

To ensure reproducibility, careful documentation of the benchmarking process is necessary, including the input data, hardware and software configurations, and benchmarking methodology. By following best practices such as these, developers can effectively optimise code performance, select efficient algorithms and data structures, and ensure the reliability and reproducibility of scientific computations.

In summary, this talk highlights the significance of accurate timing and benchmarking for scientific code, and presents strategies and best practices for overcoming the challenges associated with it. By implementing these strategies, researchers and developers can accelerate scientific progress and drive innovation through robust and reliable scientific computations.",,,30,1,en,True,False,,EQJWS9,Kai Striega,Aula,2023-08-16T11:30:00+00:00,2023-08-16T14:00:00+02:00,2.00,2.0,,"Timing and benchmarking are critical for scientific code, but pose challenges such as input variability and optimisation trade-offs. Using representative input data, controlled environments, and appropriate metrics can help.",,,,,Data Analysis and Data Engineering,none,some,no,,
ZMLBES,Streamline Your Scientific Work with AutoML,confirmed,,Poster,Machine and Deep Learning,2023-05-05T11:28:07.348703+00:00,,"How can AutoML technology be used in scientific research? Take a sneak peek at where your data analysis can be easier with mljar-supervised. By automating every step of the pipeline, from preprocessing to algorithm selection and explanation, researchers can easily advance their work. We will showcase the various fields in which scientists have implemented AutoML technology","MLJAR AutoML is one of the open-source data science tools that help to make the analytic process easier. Researchers can easily go forward with their work by automating each step of making the pipeline, such as preprocessing, choosing the correct algorithm, and explanation. The poster will show fields in which scientists were using AutoML technology (such as. medicine, pharmacology, environment, and finance). Some of they's work described in articles were printed in journals such as Nature or Science. We will also show the most valuable features of the MLJAR AutoML framework and its modes of work. Some of the advantages of using AutoML:
- explaining and understanding your data (Automatic Exploratory Data Analysis),
- trying many different machine learning models (Algorithm Selection and Hyper-Parameters tuning),
- creating Markdown reports from analysis with details about all models (Automatic-Documentation),
- saving, re-running, and loading the analysis and ML models.","This poster can also be a presentation. If you accept our topic, we can also present it as a talk.",,90,1,en,True,False,,"JWK7RV, B9JJ3N","Piotr Płoński, Aleksandra Płońska",,,,2.00,1.8,,Streamline Your Scientific Work with #AutoML #machinelearning #python #opensource,,,Supervised Learning,,,some,some,,https://github.com/mljar/mljar-supervised,https://github.com/mljar/mljar-supervised
ZNFWD3,Building divserve open source communities - learnings from PyLadies Berlin’s monthly open source hack nights,confirmed,,Talk (15 mins + Q&A),"Community, Education, and Outreach",2023-05-14T14:34:48.831987+00:00,,"Today state of the art scientific research as well as industrial software development strongly depend on open source libraries. The demographic of the contributors to these libraries is predominantly white and male. In order to increase participation of groups who have been historically underrepresented in this domain PyLadies Berlin, a volunteer run community group focussed on helping marginalised people to professionally establish themselves in tech, has been running hands on monthly open source hack nights for more than a year. After some initial challenges the initiative yielded encouraging results. This talk summarises the learnings and teaches how they can be applied in the wider open source community.","Contributing to open source projects is a highly rewarding activity that benefits not only society as whole but also individual contributors. For example, open source contributions provide a great opportunity for skill development and building a public portfolio. However, contributing to open source projects can be challenging especially for people belonging to underrepresented groups in tech because the majority of these people are career changers. In order to lower the barriers of contributing to open source projects PyLadies Berlin are running monthly open source hack nights during which participants are guided through making contributions to open source projects. After facing some hurdles in the early phase of this initiative, the hack nights have become popular events that attract people from all across Germany and even Europe. In this talk Dr Maren Westermann, the creator of this event series will walk the audience through the work involved behind the scenes, the challenges, and the lessons learned to make this initiative a success.",,,20,1,en,True,False,,R9KUCJ,Maren Westermann,Aula,2023-08-17T08:55:00+00:00,2023-08-17T11:15:00+02:00,2.00,1.3,,Learn about strategies for building divserve #opensource communities - learnings from PyLadies Berlin’s monthly open source hack nights presented by @MarenWestermann,,Diversity and Inclusion,,,,none,none,yes,,
ZX7PLY,Sprint EuroSciPy 2023 - Afternoon Session,confirmed,,Sprint,,2023-08-08T18:48:31.006285+00:00,,"A one-day sprint is organized on August 18. Open-source sprints are sessions where maintainers and contributors (including first-time contributors) meet and work together in improving the open-source projects. This can include bug fixes, new features, improvements to the documentation and others.","# Sprint

### Project sprint

A one-day sprint is organized on August 18. Open-source sprints are sessions where maintainers and contributors (including first-time contributors) meet and work together in improving the open-source projects. This can include bug fixes, new features, improvements to the documentation and others.

Sprints are a great way to get started in open-source development, learn, and meet like-minded people. All EuroSciPy attendees are welcome and encouraged to join the sprints.

The definitive schedule is not yet known and more sprints will be added when they are proposed, and some will be improvised in the same day. If you want to propose a sprint, please send us a message to maintainers@euroscipy.org .

### List of confirmed sprints:

- scikit-learn
     - GPU programming for scikit-learn with sklearn-numba-dpex and Array API.
     - Addressing issues from the GitHub issue tracker.
- pandas
- VizPy
- braindecode
     - Restructuring the documentation to make it more accessible for new users.
     - Re-implement and optimization the DataLoaders.
     - Re-implement the cropped training.


### Sprints Location

Address: 
Roshoffgasse 2,
CH-4051 Basel,
Switzerland",,,180,1,en,False,False,,,,,,,,,,Coding Sprint at EuroSciPy 2023 for Afternoon Session,Parallel Computing,Learning and Teaching Scientific Python,Supervised Learning,Astronomy,Data Analysis and Data Engineering,none,some,,,
ZZWYAT,Network Analysis Made Simple (and fast!),confirmed,,Tutorial,Data Science and Visualisation,2023-05-02T20:07:10.325049+00:00,,"Through the use of NetworkX's API, tutorial participants will learn about the basics of graph theory and its use in applied network science. Starting with a computationally-oriented definition of a graph and its associated methods, we will build out into progressively more advanced concepts (path and structure finding). We will also discuss new advances to speed up NetworkX Code with dispatching to alternate computation backends like GraphBLAS. This will be a hands-on tutorial, so stretch your muscles and get ready to go through the exercises!","Have you ever wondered about how those data scientists at Facebook and LinkedIn make friend recommendations? Or how epidemiologists track down patient zero in an outbreak? If so, then this tutorial is for you. In this tutorial will cover the basic of network analysis, we will use a variety of datasets to help you understand the fundamentals of network thinking, with a particular focus on constructing, summarizing, and visualizing complex networks. We will also cover recent changes in NetworkX which enables users to dispatch their code to more efficient backends like GraphBLAS to speed up their code.

By the end of the tutorial, participants will have learned how to use network thinking to better understand relationship problems while analyzing data. They will also be comfortable using the NetworkX API to model their data.

Part 1: Introduction (30 min)

- Networks of all kinds: biological, transportation, web.
- Representation of networks, NetworkX data structures
- Introduction to NetworkX API for modelling and graph operations.

Part 2: Hubs and Paths (30 min)

- Finding important nodes; applications
- Pathfinding algorithms and their applications
- Hands-on: implementing path-finding algorithms
- Visualize degree and betweenness centrality distributions.

Part 3: Speed up your code with NetworkX dispatching (30 min)

- Quick introduction to GraphBLAS
- Moving between GraphBLAS and NetworkX.
- Speed up your NetworkX code by changing one line of code!","The tutorial material can be found on GitHub at https://github.com/ericmjl/Network-Analysis-Made-Simple. We will either add a section to this tutorial about GraphBLAS or directly to the NetworkX documentation examples before the tutorial at EuroSciPy. We will use these Jupyter Notebooks for the tutorial and exercises.  This tutorial builds on top of previous years tutorial. This year, we will also add a section on dispatching to GraphBLAS, which will speed up the analysis pipeline for NetworkX users by a big margin.

All material are also freely available online at [https://ericmjl.github.io/Network-Analysis-Made-Simple/index.html](https://ericmjl.github.io/Network-Analysis-Made-Simple/index.html).

No setup is necessary, as tutorial participants can use Binder for computation (and if mybinder.org isn't available we will try to get something up before the tutorial). Participants will also be provided instructions if they wish to set up the environment locally.",,90,1,en,True,False,,UAM73R,Mridul Seth,Aula,2023-08-14T06:30:00+00:00,2023-08-14T10:00:00+02:00,2.00,2.0,,Learn to use NetworkX to speed up your network science problems!,,,,,Data Analysis and Data Engineering,none,some,,https://ericmjl.github.io/Network-Analysis-Made-Simple/,github.com/networkx/networkx
