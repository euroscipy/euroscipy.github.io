ID,Proposal title,Proposal state,Pending proposal state,Session type,Track,created,Tags,Abstract,Description,Session image,Speaker IDs,Speaker names
38L9P9,Interactive Image Annotation with plotly and Dash,confirmed,,Guest Talk,,2022-08-23T20:12:24.257882+00:00,,"Automatic image processing is a common task in many scientific and technological fields such as life sciences (with medical imaging), satellite imaging, etc. While machine learning is often used for efficient processing of such data sets, building a high-quality training set is an important task. Specialized software (such as rootpainter, ilastik) exist in different communities to build such training sets thanks to user annotations drawn on images. 

In this talk, I will show how to use the open-source libraries plotly and dash to build custom interactive applications for interactive image annotation, and how to combine these tools with libraries such as scikit-image or machine learning/deep learning libraries for building a whole image processing pipeline.","Automatic image processing is a common task in many scientific and technological fields such as life sciences (with medical imaging), satellite imaging, etc. While machine learning is often used for efficient processing of such data sets, building a high-quality training set is an important task. Specialized software (such as rootpainter, ilastik) exist in different communities to build such training sets thanks to user annotations drawn on images. 

In this talk, I will show how to use the open-source libraries plotly and dash to build custom interactive applications for interactive image annotation, and how to combine these tools with libraries such as scikit-image or machine learning/deep learning libraries for building a whole image processing pipeline.",,UUPVUN,Emmanuelle Gouillart
38NBJR,JAX and Flax: Function Transformations and Neural Networks,confirmed,,Keynote,,2022-08-05T09:40:29.381308+00:00,,"Modern accelerators (graphics processing units and tensor processing units) allow for high performance computing at massive scale. JAX traces computation in Python programs through the familiar numpy API, and uses XLA to compile programs that run efficiently on these accelerators. A set of composable function transformations allows for expressing versatile scientific computing with an elegant syntax.

Flax provides abstractions on top of JAX that make it easy to handle weights and other states that is required for solving problems using neural networks.

This talk first presents the basic JAX API that allows for computing gradients, compiling functions, or vectorizing computation. It then proceeds to cover other parts of the JAX ecosystem commonly used for neural network programming, such as basic building blocks and optimizers.","Modern accelerators (graphics processing units and tensor processing units) allow for high performance computing at massive scale. JAX traces computation in Python programs through the familiar numpy API, and uses XLA to compile programs that run efficiently on these accelerators. A set of composable function transformations allows for expressing versatile scientific computing with an elegant syntax.

Flax provides abstractions on top of JAX that make it easy to handle weights and other states that is required for solving problems using neural networks.

This talk first presents the basic JAX API that allows for computing gradients, compiling functions, or vectorizing computation. It then proceeds to cover other parts of the JAX ecosystem commonly used for neural network programming, such as basic building blocks and optimizers.",,PKTXYS,Andreas Steiner
7SNGMW,BioConvert: a comprehensive format converter for life sciences,confirmed,,Poster,,2022-05-25T16:59:49.671226+00:00,,"Life science uses many different formats. They may be old, or with complex syntax and converting these formats may be challenging for scientists. Bioconvert aims to provide a standard tool/interface to convert life science data formats from one to another.

Many conversion tools already exist but they may be dispersed, focused on a few specific formats, difficult to install, or not optimised. With Bioconvert, we plan to cover a wide spectrum of format conversions; we will re-use existing tools when possible and provide an interface to compare different conversion tools or methods via benchmarking. New implementations are provided when considered better than existing ones.

Bioconvert is developed in Python using continuous integration, a test suite and extensive Sphinx documentation. In March 2022, we had 48 formats, 98 direct conversions (125 different methods).","In this talk, we will present Bioconvert and what it provides to the bioinformatics community. However, we will also put an emphasis on the way the project was led. First, as an introduction to the Python language. Then, as a set of sessions on Sphinx documentation, testing, continuous integration and packaging. Finally, starting with a single conversion, we quickly added tens of conversions based on expert knowledge from the users. This methodology where expert knowledge combined with experts in Python led to a project that was quite efficient. We will also emphasise the difficulties faced by the group with deployment and reproducibility.",,US7D9R,thomas cokelaer
7W7PPC,Discover Pythran through 10 code samples,confirmed,,Talk (long),,2022-06-03T08:29:09.325494+00:00,,"The [Pythran](https://pythran.readthedocs.io)  compiler is used to speed-up generic Python scientific kernels across the world. Through ten code samples taken from scipy, scikit-image codebase and stack overflow snippets, this talks is going to demonstrate the major features of the compiler, as well as some technical nits!","The [pythran](https://pythran.readthedocs.io) compiler is used to speed-up generic Python scientific kernels across the world, including scipy and scikit-image ones.

It has the advantage of taking pure Python code as input, with type annotations on function entry points, and generate efficient native code in exchange.

But there's much more to it! Through 10 commented real-world examples taken from stackoverflow, scipy and scikit learn code base, this talk is going to present some of the technical nits Pythran has to offer: OpenMP support, SIMD instruction usage, a featurful type signature description, high-level code optimization, Python syntax sugar support, distutils integration (oops, is that even a feature?)",,ZJ3KYE,Serge « sans » Paille
89WNRG,Industrial Strength DALLE-E: Scaling Complex Large Text & Image Models,confirmed,,Talk,,2022-07-07T07:39:10.695108+00:00,,"Identifying the right tools to enable for high performance machine learning may be overwhelming as the ecosystem continues to grow at break-neck speed. This becomes particularly emphasised when dealing with the ever growingly popular large language and image generation models such as GPT2, OTP and DALL-E, between others. In this session we will dive into a practical showcase where we will be productionising the large image generation model DALL-E, and showcase some optimizations that can be introduced as well as considerations as the use-cases scale. By the end of this session practitioners will be able to run their own DALL-E powered applications as well as integrate these with functionalities from other large language models like GPT2, etc. We will be leveraging key tools in the Python ecosystem to achieve this, including Pytorch, HuggingFace, FastAPI and MLServer.","Identifying the right tools to enable for high performance machine learning may be overwhelming as the ecosystem continues to grow at break-neck speed. This becomes particularly emphasised when dealing with the ever growingly popular large language and image generation models such as GPT2, OTP and DALL-E, between others. In this session we will dive into a practical showcase where we will be productionising the large image generation model DALL-E, and showcase some optimizations that can be introduced as well as considerations as the use-cases scale. By the end of this session practitioners will be able to run their own DALL-E powered applications as well as integrate these with functionalities from other large language models like GPT2, etc. We will be leveraging key tools in the Python ecosystem to achieve this, including Pytorch, HuggingFace, FastAPI and MLServer.",,EQMGKH,Alejandro Saucedo
8JT9QA,Sequana: a set of Next Generation Sequencing pipelines,confirmed,,Poster,,2022-05-25T17:28:25.469558+00:00,,"Sequana software is developed within a Sequencing platform at Institut Pasteur. It provides a Python library dedicated to Next Generation Sequencing (NGS) analysis including visualisation of NGS formats. 
Sequana is also a project that provides (i) a set of pipelines dedicated to NGS in the form of Snakefiles (Makefile-like with Python syntax based on Snakemake framework), (ii) tools to help in the creation of such pipelines, (iii) a graphical interface for Snakemake framework, (iv) standalone applications for NGS analysis. Pipelines can be run locally or on HPC clusters. Common user interface is provided to ease user interface. These NGS pipelines are ready for production and have been applied on hundreds of projects including Covid variant detection, genomic, transcriptomics, etc","In this talk we will present the Sequana project illustrated with several real-life examples related to health with viral or bacterial genomes as test-cases. We will show how Python could be used to multiply the number of pipelines deployed for production within a Sequencing Platform. One key point for production, especially in the fields of life sciences, is reproducibility. We will emphasise the importance of test-suite but also the difficulties in the NGS field due to large data sets being handled. Finally, we will show how the project started from a single monolithic library to a set of independent pipelines.",,US7D9R,thomas cokelaer
8RAJX7,"Increase citations, ease review & collaboration – Making machine learning in research reproducible",confirmed,,Talk (long),,2022-08-28T20:49:56.411505+00:00,,"Every scientific conference has seen a massive uptick in applications that use some type of machine learning. Whether it’s a linear regression using scikit-learn, a transformer from Hugging Face, or a custom convolutional neural network in Jax, the breadth of applications is as vast as the quality of contributions.

This tutorial aims to provide easy ways to increase the quality of scientific contributions that use machine learning methods. The reproducible aspect will make it easy for fellow researchers to use and iterate on a publication, increasing citations of published work. The use of appropriate validation techniques and increase in code quality accelerates the review process during publication and avoids possible rejection due to deficiencies in the methodology. Making models, code and possibly data available increases the visibility of work and enables easier collaboration on future work.

This work to make machine learning applications reproducible has an outsized impact compared to the limited additional work that is required using existing Python libraries.","One of the tenets of science is to be reproducible. 

But if we always did what we’re supposed to, the world would be a better and easier place. However, there are benefits to making science reproducible that directly benefit researchers, especially in computational science and machine learning. These benefits are like the title says:

- Easier review cycles
- More citations
- More collaboration

But it goes further. Reproducibility is a marketable skill outside of academia. When we work in companies that apply data science or machine learning, these companies know that technical debt can slowly degrade a code base and in some cases like Amazon and Google, the machine learning system has to be so reproducible that we expect the entire training and deployment to work automatically on a press of a button. Technical debt is also a problem in academia, but here it is more framed in the devastating prospect of the only postdoc leaving that knows how to operate the code base.

Luckily, we have a lot of work cut out for us already!

These benefits, and a few others, like making iteration, and therefore frequent publication easier, do not come at a proportional cost. Most of the methods to increase code quality in machine learning projects of applied scientists are in fact fairly easy to set up and run!

So how do we actually go about obtaining these goals?

## Model Evaluation

Applying machine learning in an applied science context is often method work. We build a prototype model and expect want to show that this method can be applied to our specific problem. This means that we have to guarantee that the insights we glean from this application generalize to new data from the same problem set.

This is why we usually import `train_test_split()` from scikit-learn to get a validation set and a test set. But in my experience, in real-world applications, this isn’t always enough. In science, we usually deal with data that has some kind of correlation in some kind of dimension. Sometimes we have geospatial data and have to account for Tobler’s Law, i.e. things that are closer to each other matter more to each other than those data points at a larger distance. Sometimes we have temporal correlations, dealing with time series, where data points closer in time may influence each other.

Not taking care of proper validation, will often lead to additional review cycles in a paper submission. It might lead to a rejection of the manuscript which is bad enough. In the worst case scenario, our research might report incorrect conclusions and have to be retracted. No one wants rejections or even retractions.

So we’ll go into some methods to properly evaluate machine learning models even when our data is not “independent and identically distributed”.

## Benchmarking

Another common reason for rejections of machine learning papers in applied science is the lack of proper benchmarks. This section will be fairly short, as it differs from discipline to discipline.

However, any time we apply a superfancy deep neural network, we need to supply a benchmark to compare the relative performance of our model to. These models should be established methods in the field and simpler machine learning methods like a linear model, support-vector machine or a random forest.

## Model Sharing

Some journals will require the sharing of code or models, but even if they don’t we might benefit from it.

Anytime we share a model, we give other researchers the opportunity to replicate our studies and iterate upon them. Altruistically, this advances science, which in and of itself is a noble pursuit. However, this also increases the citations of our original research, a core metric for most researchers in academia.

In this section, we explore how we can export models and make our training codes reproducible. Saving a model from scikit-learn is easy enough. But what tools can we use to easily make our training code adaptable for others to import and try out that model? Specifically, I want to talk about:

- Automatic Linters
- Automatic Formatting
- Automatic Docstrings and Documentation
- Docker and containerization for ultimate reproducibility

## Testing

Machine learning is very hard to test. Due to the nature of the our models, we often have soft failures in the model that are difficult to test against.

Writing software tests in science, is already incredibly hard, so in this section we’ll touch on 

- some fairly simple tests we can implement to ensure consistency of our input data
- avoid bad bugs in data loading procedures
- some strategies to probe our models

## Interpretability

One way to probe the models we build is to test them against the established knowledge of domain experts. In this final section, we’ll explore how to build intuitions about our machine learning model and avoid pitfalls like spurious correlations. These methods for model interpretability increase our trust into models, but they can also serve as an additional level of reproducibility in our research and a valuable research artefact that can be discussed in a publication.

This part of the tutorial will also go into some considerations why the feature importance of tree-based methods can serve as a start but often shouldn’t be used as the sole source of truth regarding feature interpretation of our applied research.

This section will introduce tools like `shap`, discuss feature importance, and manual inspection of models.

## Ablation Studies

Finally, the gold standard in building complex machine learning models is proving that each constituent part of the model contributes something to the proposed solution. 

Ablation studies serve to dissect machine learning models and evaluate their impact.

In this section, we’ll finally discuss how to present complex machine learning models in publications and ensure the viability of each part we engineered to solve our particular problem set.

## Conclusion

Overall, this tutorial is aimed at applied scientists that want to explore machine learning solutions for their problems.

This tutorial focuses on a collection of “easy wins” that scientists can implement in their research to avoid catastrophic failures and increase reproducibility with all its benefits.",,HCWQZW,Jesper Dramsch
8XBVXQ,How to increase diversity in open source communities,confirmed,,Talk,,2022-06-03T16:14:23.182362+00:00,,"Today state of the art scientific research strongly depends on open source libraries. The demographic of the contributors to these libraries is predominantly white and male [1][2][3][4]. In recent years there have been a number of various recommendations and initiatives to increase the participation in open source projects of groups who are underrepresented in this domain [1][3][5][6]. While these efforts are valuable and much needed, contributor diversity remains a challenge in open source communities [2][3][7]. This talk highlights the underlying problems and explores how we can overcome them.","In this talk we’ll first examine the problems encountered by people belonging to marginalised groups in open source as well as by project maintainers with respect to contributing to and increasing the diversity of open source projects, respectively [1][2][3][4][5][6]. Building on this overview, we’ll go over what kind of actions have been taken to increase diversity in open source projects, with special focus on scientific libraries, and the effects they have had [1][6][7]. Lastly, we’ll look at ideas that are currently being tested and next steps. By the end of this talk, the audience will have a good understanding of why contributor diversity is low in open source, the efforts that have been made so far to address this problem, and what can further be done to increase the presence of underrepresented groups in technology in general, and in open source in particular.",,R9KUCJ,Maren Westermann
99R3C7,Interoperability in the DataFrame landscape: DataFrame API & PyArrow Update,confirmed,,Maintainer track,,2022-08-29T09:01:18.011626+00:00,,"In this session, we want to share some updates on the DataFrame ecosystem: the DataFrame interchange protocol (https://data-apis.org/dataframe-protocol/latest/purpose_and_scope.html) and Arrow C Data interface (https://arrow.apache.org/docs/format/CDataInterface.html), and the integration of those interoperability protocols with different libraries. Further, we want to have an open conversation about challenges and requirements related to DataFrame interoperability and supporting multiple DataFrame libraries in projects.","In this session, we want to share some updates on the DataFrame ecosystem: the DataFrame interchange protocol (https://data-apis.org/dataframe-protocol/latest/purpose_and_scope.html) and Arrow C Data interface (https://arrow.apache.org/docs/format/CDataInterface.html), and the integration of those interoperability protocols with different libraries. Further, we want to have an open conversation about challenges and requirements related to DataFrame interoperability and supporting multiple DataFrame libraries in projects.",,7VUXWM,Joris Van den Bossche
9GUNEZ,A Primer to Maintainable Code,confirmed,,Talk (long),,2022-06-09T19:44:26.862802+00:00,,"In this talk, I'll give an overview of software quality and why it's important - especially for scientists. Provide best practices and libraries to dive deeper into, hypes to ignore, and simple guidelines to follow to write code that your peers will love.

After the talk, the audience will have a guide on how to develop better code and be aware of potential blind spots.","In this talk I will provide an overview of best practices for software quality. Practices and libraries to look deeper into included, hypes to be ignored providing simple guidelines to follow for writing code your peers will love.

Jupyter notebooks are often messy, scripted applications like to contain redundant code and like to fail just before the result - wasting precious time. The code works for one-time use, but is difficult to maintain, reuse, or read by colleagues.

In this talk I will present best practices to make code:
- more readable
- better to maintain
- re-usable

Flag potentially bad practices as:
- closures
- using too many third libraries

Practices how to best design applications including:
- refactoring
- versioning
- DRY
- when to write tests
- documentation

Provide an overview of the habitats production-ready code likes to live in like CI/CD pipelines.

After the talk the audience will have a guideline on how to develop better code, and be aware of potential blind spots.

Software quality is important - especially for research! This talk provides an overview of best practices and libraries to dive deeper into, hypes to ignore, and simple guidelines to follow to write code that your peers will love.",,8F38DV,Alexander CS Hendorf
9LMAXM,Revolutionalise Data Visulization with PyScript,confirmed,,Talk (long),,2022-06-06T13:33:15.874772+00:00,,"Since the announcement of PyScript, it has gained lots of attention and imagination about how we can run applications of Python in the browser. Out of everything that I have come across, most of the use cases are data visualisation. Let's see how we can up our data viz game with PyScript.","At the beginning of this talk, we will go over what PyScript is and how to use it: no need to install it, just put a CDN URL in an HTML file. This part will cover some basic knowledge about it, why is it unique and why it can revolutionize data visualisation, for those who have not heard much about PyScript.

After that, we will go over 3 examples: one interactive data plot, one interactive map visualization and one integration with D3. Besides showing the result, we will also have a deeper look at the code, the key to making similar visualization so you can learn a few tricks to make your own PyScript data visualization as well.

This talk is for anyone who needs a simple and no fuzz way to present data findings and visualization. As PyScript runs only on the browser, it will be an optimal way to share data findings through visualization.",,8EGVC9,Cheuk Ting Ho
9QBKCE,Conda Store : easy environments management & reproducibility for Teams and Enterprises,confirmed,,Poster,,2022-06-05T07:54:41.928927+00:00,,"End users think in terms of environments, not packages. Conda Store makes it easy for data scientists to define their environments, ensures reproducibility, productionizing, easing collaboration, and reduces friction and latency between developers and IT.","Conda Store is a tool designed with multiple goals in mind : 

- Easy to use : makes it simple for data scientists to define and build their environments. Thanks to Conda Store's UI, no need to write yaml anymore  - but power-users still can.

- Facilitate collaboration : No more working isolated on locally-hosted environments. No more divergent solves when sharing an environment.yaml file with your team. Actually, you don't need to share a yaml file anymore. Conda Store empowers teams to let them define, build and share their environments collaboratively. Also, role-based access control (RBAC) makes it very flexible to define accesses and permissions across people and teams.

- More autonomy : data scientists don't need to wait anymore for IT to add the package they need. Click, Build, Done.

- Reproducibility and Productionizing : easy rollback to a previous specification, environments available in multiple formats : yaml, lockfile, conda tarball, docker image. You can even serve them on AWS S3.",,DSY8ES,Pierre-Olivier Simonard
9VCPM3,Real-time estimation of an heat pump I/O state with IoT data.,confirmed,,Talk (long),,2022-06-09T15:08:24.423412+00:00,,"In the present time, we are facing a continuous growing of the energy price. It is then important to optimize the use of heat pumps, both in domestic and industrial environments. Using an opportunely labeled dataset of accelerometer, speed or relative position over time coming from a cheap sensor it is possible to estimate the I/O state of any heating or cooling engine. This new real-time measure allows then to compute the energy consumption and to study the most cheap usage scheme.
In this presentation we will show a real-case implementation of some fast binary classifiers, from basic statistics to machine learning, assessing the performance of each method in terms of computational time, precision and accuracy levels.","In the present time, we are facing a continuous growing of the energy price. It is then important to optimize the use of heat pumps, both in domestic and industrial environments. Using an opportunely labeled dataset of accelerometer, speed or relative position over time coming from a cheap sensor it is possible to estimate the I/O state of any heating or cooling engine. This new real-time measure allows then to compute the energy consumption and to study the most cheap usage scheme.
In this presentation we will show a real-case implementation of some fast binary classifiers, from basic statistics to machine learning, assessing the performance of each method in terms of computational time, precision and accuracy levels.",,TTUQ3P,Davide Poggiali
9VKWZP,Pragmatic Panel: Build and Deploy Complex Data-Driven WebApps,confirmed,,Talk,,2022-06-05T07:20:35.676186+00:00,,"Panel is one of the leading choices for building dashboards in Python. In this talk, we discuss the practical aspects of complex data-driven dashboards. There are tutorials and guides available which help teach new users the basics, but this talk focuses on the challenges of building more complex, industry-ready, deployed dashboards. There are a variety of niche issues which arise when you push the limits of complexity, and we will share the solutions we have developed. We will demonstrate these solutions as we walk through the entire lifecycle from data ingestion, though exploratory analysis to deployment as a finished website.","Panel is a mature tool for quickly creating custom, interactive web apps and dashboards by connecting user-defined widgets to plots, images, tables, or text. Panel allows scientists to create their own apps for data exploration quickly and easily without requiring them to know javascript or work with an external developer team. It provides the framework for building much more complex dashboards which look and feel like a mature web app. By allowing prototyping inside Jupyter and deployment outside Jupyter, it bridges the gap between exploration and production.

In this talk, we discuss the practical aspects of complex data-driven dashboards. There are tutorials and guides available which help teach new users the basics, but this talk focuses on the challenges of building and sharing more complex, industry-ready, applications. There are a variety of issues which arise when you build fully featured applications, and we will share the solutions we have developed. We will demonstrate these solutions as we walk through the entire lifecycle from data, though exploratory analysis to deployment as a finished website.

Examples of the topics we will cover include: best practices; creating multi-page apps where you may not know the number of pages ahead of time; Debugging cascading effects of user interactions; Deployment in air-gapped environments; using templates and customizing Panel’s look and feel. 

These lessons were distilled from Panel apps we created for clients in finance, manufacturing and science. The datasets and code for these examples in the talk will be made available via a github repository.",,DSY8ES,Pierre-Olivier Simonard
AFXCXH,Discrete event simulations of 'all electric' mines,confirmed,,Talk,,2022-05-27T22:43:34.915021+00:00,,"How a discrete event simulation can help mining companies reduce their dependence on diesel as a source of fuel for their large haulage trucks. Using open source software, mining environments are modeled, and helps decision making for building an all electric mine, where diesel powered vehicles are made obsolete.","A typical 200 tonne payload mining truck can burn through more than 3000 liters of diesel fuel per day. Mining companies around the globe are searching for pathways to reduce their environmental impact, and clearly, reducing their dependence on diesel fuel is be a step in the right direction. 

Trials using diesel/electric hybrid, batteries, hydrogen and other energy sources are currently underway to serve as an alternative energy source to power these vehicles. The technology required for powering this class of large haulage vehicle is still in development, leaving mining companies unsure about the path they should take.

Using the Python Simpy and NetworkX libraries to serve as the core, a discrete event simulation of real mining environments is created. Electric haulage trucks, electric trolley lines and more are modeled, and provide results aimed to help mining companies move to a more sustainable operation.",,8MJW7J,Nicholas Hall
ALLCZ7,JupyterLab 4 and the future of Jupyter Notebook,confirmed,,Poster,,2022-06-03T19:56:50.610684+00:00,,"JupyterLab is a powerful computational environment for interactive data science in the browser, and the new version 4 release comes with many new features and improvements.

The Jupyter Notebook project decided to base its next major version 7 on JupyterLab components and extensions, which means many JupyterLab features are also available to Jupyter Notebook users. 

In this presentation, we will demo all the features coming in these new versions and how users can seamlessly switch from one notebook interface to another.","JupyterLab 4 is packed with new features and improvements:

- More reliable and stable Real Time Collaboration feature to collaborate with peers
- Accessibility improvements
- Enhanced code completions with the improved Language Server Protocol integration
- Commenting on notebooks and files from the JupyterLab interface
- Improved debugging experience, with the variable explorer and rich mime type rendering
- A new settings UI to more easily tweak and customize the environment
- Performance improvements

The JupyterLab plugin system also enables other distributions to be created from JupyterLab building blocks. Which is actually the case for the next major version of the Jupyter Notebook application.

Jupyter Notebook 7 is based on the JupyterLab codebase, but provides an equivalent user experience to the current (version 6) application. Notebook 7 keeps the document-centric user experience at its core, and brings many new features that were not previously available:

- Debugger
- Real-time collaboration
- Theming and dark mode
- Internationalization
- Improved Web Content Accessibility Guidelines (WCAG) compliance
- Support for many JupyterLab extensions, including Jupyter LSP (Language Server Protocol) for enhanced code completions

This talk will be about demoing the new features coming to JupyterLab 4 and Notebook 7, and how these two frontends provide a coherent package for productive interactive computing in the browser.",,"KA8ZHH, ZYLSXW","Jeremy Tuloup, Frédéric Collonval"
APTJNF,The Beauty of Zarr,confirmed,,Poster,,2022-06-01T23:19:16.101328+00:00,,"This poster showcases Zarr, an open-source data format for storing chunked, compressed N-dimensional arrays. We present a systematic approach to understanding and implementing Zarr by showing how it works and the need for using it. Zarr is based on an open technical specification, making implementations across several languages possible. The focus here is on Zarr’s Python implementation and its interoperability with the existing libraries in the PyData stack.","Zarr is a data format for storing chunked, compressed N-dimensional arrays. Zarr is based on open-source technical specification and has implementations in several languages, with Python the most used one. Zarr is NumFOCUS’s sponsored project and is under their umbrella.

### Outline:


First, I’d be talking about:

### What’s, Why’s, and How’s of Zarr (15 mins.)

- How does Zarr work?
    - Talking about the motivation and functionality of Zarr
- What’s the need for using Zarr?
    - When, where and why to use it?
- Pluggable compressors and file-storage
    - Talking about several compressors and file-storage systems available in Zarr
- Managing(selection, resizing, writing, reading) chunked arrays using Zarr functions
    - Using inbuilt functions to manage compressed chunks
- How Zarr is different when compared to other storage formats?
    - Talking briefly about technical specification, which allows Zarr to have implementations in several languages
    - Pros and cons when compared to other storage formats
- Zarr community
    - What is the Zarr community, and how do we do things?


Then, I’d be doing a hands-on session, which would cover:

### Hands-on (10 mins.)

- Creating and using Zarr arrays
    - Using inbuilt functions to create Zarr arrays and reading and writing data to it
- Looking under the hood
    - Use store functions to explain how your Zarr data is stored
- Consolidating metadata
    - Consolidating the metadata for an entire group into a single object
- Writing and reading from Cloud object storage
    - Using S3/GCS/Azure to create Zarr arrays and writing data to it
- Showing how Zarr interoperates with the PyData stack
    - How Zarr interoperates with the PyData stack(NumPy, Dask and Xarray) and how you can write data to your Zarr chunks at incredibly high speed in parallel using Dask


I’d be closing the talk by: 

### Conclusion(5 mins.)

- Key takeaway
- How you can contribute to Zarr?
- QnA

This talk aims to address the audience who works with large amounts of data and are in search of a data format which is transparent, easy to use and friendly to the environment. Zarr is also reasonably used in bioimaging, geospatial and research communities. So, Zarr is your one-stop solution if you’re from a community or an organisation dealing with high-volume data. Also, anyone who is curious and wants to learn about Zarr and how to use it is most welcome.

The tone of the talk is set to be informative, along with a hands-on session. Also, I’m happy to adjust the style according to the audience in the room.

Intermediate knowledge of Python and NumPy arrays is required for the attendees to attend this talk.

### After this talk, you’d learn:

- Basic use cases for Zarr and how to use it
- Understand the basics of data storage in Zarr
- Understand the basics of compressors and file-storage systems in Zarr
- Take a better and informed decision on what data format to use for your data",,"A7ACFE, YWKXWU","Sanket Verma, Jonathan Striebel"
B3AGKG,"scikit-learn and fairness, tools and challenges",confirmed,,Talk (long),,2022-06-03T15:21:10.999119+00:00,,"Fairness, accountability, and transparency in machine learning have become a major part of the ML discourse. Since these issues have attracted attention from the public, and certain legislation are being put in place regulating the usage of machine learning in certain domains, the industry has been catching up with the topic and a few groups have been developing toolboxes to allow practitioners incorporate fairness constraints into their pipelines and make their models more transparent and accountable. Some examples are fairlearn, AIF360, LiFT, fairness-indicators (TF), ...

This talk explores some of the tools existing in this domain and discusses work being done in scikit-learn to make it easier for practitioners to adopt these tools.","On the machine learning side, scikit-learn has been one of the most commonly used libraries which has been extended by third party libraries such as imbalanced-learn and scikit-lego. However, when it comes to incorporating fairness constraints in a usual scikit-learn pipeline, there are challenges and limitations related to the API, which has made developing a scikit-learn compatible fairness focused package challenging and hampering the adoption of these tools in the industry.

In this talk, we start with a common classification pipeline, then we assess fairness/bias of the data/outputs using disparate impact ratio as an example metric, and finally mitigate the unfair outputs and search for hyperparameters which give the best accuracy while satisfying fairness constraints.

This workflow will expose the limitations of the API related to passing around feature names and/or sample metadata in a pipeline down to the scorers. We discuss certain workarounds and then talk about the work being done to address these issues and show how the final solution would look like. After this talk, you will be able to follow the related discussions happening in these open source communities and know where to look for them.

The code and the presentation will be publicly available on github.",,HGSWKF,Adrin Jalali
B9N3U9,Continuous and on demand benchmarking,confirmed,,Talk,,2022-06-06T21:52:14.490835+00:00,,"We all know and love our carefully designed CI pipelines, which tests our code and makes sure by adding some code or fixing a bug we aren’t introducing a regression in the codebase. But we often don’t give the same treatment to benchmarking as we give to correctness. The benchmarking tests are usually one off scripts written to test a specific change. In this talk, we will discuss various strategies to test our code for performance regressions using ASV (airspeed velocity) for python projects.","In this talk, we will discuss how we can use ASV (airspeed velocity) and on demand cloud providers to mimic the usual setup of dedicated servers to run benchmarking tests. Currently, a lot of projects use dedicated servers to run their benchmarking suite, which usually is not economical as the majority of the time the hardware is unused. We can get rid of the setup of dedicated hardware + ongoing costs associated with it by only using resources when required.

Outline
5 mins:
- Why benchmarking?
- The usual ways of running the benchmarking suite on dedicated hardware.
- Using GitHub Actions reliably for ""relative benchmark"" testing (A quick shout out to the blog post by Quansight) 

10 mins:
- How we can use container technology to setup our tests and run them on demand on ""dedicated"" hardware to cut down on costs and give a lower barrier to entry to set up a benchmarking suite for projects.",,WTK33D,Mridul Seth
BAL88N,Scipp plot: modular interactive plotting from graph nodes,confirmed,,Poster,,2022-05-29T14:27:09.400175+00:00,,"We present the plotting framework of the Scipp package (https://scipp.github.io/) for multi-dimensional arrays.
Based on the Model View Controller pattern, it uses a set of nodes connected in a graph to represent a sequence of processing steps that can be applied to the data before plotting it onto the figure axes.
A common example of this could be a 2D scatter plot, accompanied by 1D histograms of the data points on the top and right hand side of the scatter axes.
The histogramming nodes, that lie below the original root data node in the graph, perform a histogram operation in each of the X and Y dimensions, and their results get sent to the top and right plotting axes.

The use of a graph of connected nodes opens up the opportunity for a very modular way of creating interactive plots.
For instance, using a library of widgets (such as ipywidgets), we can change the input to one of the nodes, which notifies all the nodes below it about the change.
This means that modifying a parameter of the scatter data with e.g. a slider, would automatically update not only the main scatter plot, but also the histograms on the sides.

Any function (smoothing, fitting, filtering …) can be used inside a node, and any number of axes (or views) can be attached to a given node. This flexibility allows users to create complicated interactive visualizations with just a few lines of code.","We present the plotting framework of the Scipp package (https://scipp.github.io/) for multi-dimensional arrays.
Based on the Model View Controller pattern, it uses a set of nodes connected in a graph to represent a sequence of processing steps that can be applied to the data before plotting it onto the figure axes.
A common example of this could be a 2D scatter plot, accompanied by 1D histograms of the data points on the top and right hand side of the scatter axes.
The histogramming nodes, that lie below the original root data node in the graph, perform a histogram operation in each of the X and Y dimensions, and their results get sent to the top and right plotting axes.

The use of a graph of connected nodes opens up the opportunity for a very modular way of creating interactive plots.
For instance, using a library of widgets (such as ipywidgets), we can change the input to one of the nodes, which notifies all the nodes below it about the change.
This means that modifying a parameter of the scatter data with e.g. a slider, would automatically update not only the main scatter plot, but also the histograms on the sides.

Any function (smoothing, fitting, filtering …) can be used inside a node, and any number of axes (or views) can be attached to a given node. This flexibility allows users to create complicated interactive visualizations with just a few lines of code.

Scipp is developed by the European Spallation Source, a neutron scattering facility under construction in Lund, Sweden. Scipp is available under the open source BSD-3 licence from PyPI (`pip install scipp`) or via conda (`conda install -c conda-forge -c scipp scipp`).",,WAV8NL,nvaytet
BEHTMU,Machine learning with missing values,confirmed,,Talk,,2022-06-06T19:41:32.753750+00:00,,"This talk will cover how to build predictive models that handle well missing values, using scikit-learn. It will give on the one side the statistical considerations, both the classic statistical missing-values theory and the recent development in machine learning, and on the other side how to efficiently code solutions.","In many data-science applications, the data may come with missing values. There is a rich statistical literature on performing analysis with missing values. However, machine learning brings new tradeoffs: how to deal with missing-values at test time? Should we really care about recovering the model suitable for fully-observed data? I will cover both the classic theory and recent theoretical advances. I will show how scikit-learn can be used to implement various solutions, and how these illustrate the theory.

Tentative outline:
- The classic statistical view on missing values
   - Missing at Random Settings: why it is important
   - Imputation, and corresponding scikit-learn tools
- Prediction for missing values
   - Simple predictors need very good predictors
   - Rich predictors work with simple imputers, even outside Missing at Random settings",,MXCVQL,Gaël Varoquaux
BYCNN7,Scaling scikit-learn performances: introducing new computational foundations,confirmed,,Talk (long),,2022-06-13T19:44:07.693957+00:00,,"scikit-learn is an open-source scientific library for machine learning in Python. In this talk, we will present the recent work carried over by the scikit-learn core-developers team to improve its native performance.","scikit-learn is an open-source scientific library for machine learning in Python.

Since its first release in 2010, the library gained a lot of traction in education, research and the wider society, and has set several standards for API designs in ML software. Nowadays scikit-learn is of one the most used scientific library in the world for data analysis. It provides reference implementations of many methods and algorithms to a userbase of millions.

With the renewed interest in machine-learning based methods in the last years, other libraries providing efficient and highly optimised methods (such as for instance LightGBM and XGBoost for Gradient-Boosting-based methods) have emerged. Those libraries have encountered a similar success, and have put performance and computational efficiency as top priorities.

In this talk, we will present the recent work carried over by the scikit-learn core-developers team to improve its native performance.

This talk will cover elements of the PyData ecosystem and the CPython interpreter with an emphasis on their impact on performances. Computationally expensive patterns will then be covered before presenting the technical choices associated with the new foundational implementations, keeping the project requirements in mind. At the end, we will take a quick look at the future work and collaborations on hardware-specialised computational kernels.",,P8VJPK,Julien Jerphanion
BZDJXH,"CLAIMED - An open source unified platform for batch, streaming and microservices based data science",confirmed,,Poster,,2022-07-04T17:07:32.428916+00:00,,"Data are processed in pipelines – either an entire data set, in batches or one by one. A variety of programming languages, frameworks and libraries exists. In CLAIMED – the component library for AI, Machine Learning, ETL and Data Science – we provide an opinionated set of coarse grained components implemented as jupyter notebooks. Through C3, the claimed component compiler those can be (as of now) transformed into Kubeflow Pipeline Components, Airflow Operators or simple (docker) container images to be executed on Knative. An adapter implemented as side car transforms those into either streaming components (currently http(s) and Kafka) or micro services – with scale to zero support. Using the jupyter lab Elyra pipeline editor and CLAIMED, anybody can create data science pipelines without programming skills. But the source code is only one click away. The jupyter notebook backing the component is available for review, adjustments or improvements of the components.","Data are processed in pipelines – either an entire data set, in batches or one by one. A variety of programming languages, frameworks and libraries exists. In CLAIMED – the component library for AI, Machine Learning, ETL and Data Science – we provide an opinionated set of coarse grained components implemented as jupyter notebooks. Through C3, the claimed component compiler those can be (as of now) transformed into Kubeflow Pipeline Components, Airflow Operators or simple (docker) container images to be executed on Knative. An adapter implemented as side car transforms those into either streaming components (currently http(s) and Kafka) or micro services – with scale to zero support. Using the jupyter lab Elyra pipeline editor and CLAIMED, anybody can create data science pipelines without programming skills. But the source code is only one click away. The jupyter notebook backing the component is available for review, adjustments or improvements of the components.",,NDMWWC,Romeo Kienzler
E7Z3VY,"Increase citations, ease review & collaboration – Making machine learning in research reproducible",confirmed,,Tutorial,,2022-06-05T15:42:08.873583+00:00,,"Every scientific conference has seen a massive uptick in applications that use some type of machine learning. Whether it’s a linear regression using scikit-learn, a transformer from Hugging Face, or a custom convolutional neural network in Jax, the breadth of applications is as vast as the quality of contributions.

This tutorial aims to provide easy ways to increase the quality of scientific contributions that use machine learning methods. The reproducible aspect will make it easy for fellow researchers to use and iterate on a publication, increasing citations of published work. The use of appropriate validation techniques and increase in code quality accelerates the review process during publication and avoids possible rejection due to deficiencies in the methodology. Making models, code and possibly data available increases the visibility of work and enables easier collaboration on future work.

This work to make machine learning applications reproducible has an outsized impact compared to the limited additional work that is required using existing Python libraries.","One of the tenets of science is to be reproducible. 

But if we always did what we’re supposed to, the world would be a better and easier place. However, there are benefits to making science reproducible that directly benefit researchers, especially in computational science and machine learning. These benefits are like the title says:

- Easier review cycles
- More citations
- More collaboration

But it goes further. Reproducibility is a marketable skill outside of academia. When we work in companies that apply data science or machine learning, these companies know that technical debt can slowly degrade a code base and in some cases like Amazon and Google, the machine learning system has to be so reproducible that we expect the entire training and deployment to work automatically on a press of a button. Technical debt is also a problem in academia, but here it is more framed in the devastating prospect of the only postdoc leaving that knows how to operate the code base.

Luckily, we have a lot of work cut out for us already!

These benefits, and a few others, like making iteration, and therefore frequent publication easier, do not come at a proportional cost. Most of the methods to increase code quality in machine learning projects of applied scientists are in fact fairly easy to set up and run!

So how do we actually go about obtaining these goals?

## Model Evaluation

Applying machine learning in an applied science context is often method work. We build a prototype model and expect want to show that this method can be applied to our specific problem. This means that we have to guarantee that the insights we glean from this application generalize to new data from the same problem set.

This is why we usually import `train_test_split()` from scikit-learn to get a validation set and a test set. But in my experience, in real-world applications, this isn’t always enough. In science, we usually deal with data that has some kind of correlation in some kind of dimension. Sometimes we have geospatial data and have to account for Tobler’s Law, i.e. things that are closer to each other matter more to each other than those data points at a larger distance. Sometimes we have temporal correlations, dealing with time series, where data points closer in time may influence each other.

Not taking care of proper validation, will often lead to additional review cycles in a paper submission. It might lead to a rejection of the manuscript which is bad enough. In the worst case scenario, our research might report incorrect conclusions and have to be retracted. No one wants rejections or even retractions.

So we’ll go into some methods to properly evaluate machine learning models even when our data is not “independent and identically distributed”.

## Benchmarking

Another common reason for rejections of machine learning papers in applied science is the lack of proper benchmarks. This section will be fairly short, as it differs from discipline to discipline.

However, any time we apply a superfancy deep neural network, we need to supply a benchmark to compare the relative performance of our model to. These models should be established methods in the field and simpler machine learning methods like a linear model, support-vector machine or a random forest.

## Model Sharing

Some journals will require the sharing of code or models, but even if they don’t we might benefit from it.

Anytime we share a model, we give other researchers the opportunity to replicate our studies and iterate upon them. Altruistically, this advances science, which in and of itself is a noble pursuit. However, this also increases the citations of our original research, a core metric for most researchers in academia.

In this section, we explore how we can export models and make our training codes reproducible. Saving a model from scikit-learn is easy enough. But what tools can we use to easily make our training code adaptable for others to import and try out that model? Specifically, I want to talk about:

- Automatic Linters
- Automatic Formatting
- Automatic Docstrings and Documentation
- Docker and containerization for ultimate reproducibility

## Testing

Machine learning is very hard to test. Due to the nature of the our models, we often have soft failures in the model that are difficult to test against.

Writing software tests in science, is already incredibly hard, so in this section we’ll touch on 

- some fairly simple tests we can implement to ensure consistency of our input data
- avoid bad bugs in data loading procedures
- some strategies to probe our models

## Interpretability

One way to probe the models we build is to test them against the established knowledge of domain experts. In this final section, we’ll explore how to build intuitions about our machine learning model and avoid pitfalls like spurious correlations. These methods for model interpretability increase our trust into models, but they can also serve as an additional level of reproducibility in our research and a valuable research artefact that can be discussed in a publication.

This part of the tutorial will also go into some considerations why the feature importance of tree-based methods can serve as a start but often shouldn’t be used as the sole source of truth regarding feature interpretation of our applied research.

This section will introduce tools like `shap`, discuss feature importance, and manual inspection of models.

## Ablation Studies

Finally, the gold standard in building complex machine learning models is proving that each constituent part of the model contributes something to the proposed solution. 

Ablation studies serve to dissect machine learning models and evaluate their impact.

In this section, we’ll finally discuss how to present complex machine learning models in publications and ensure the viability of each part we engineered to solve our particular problem set.

## Conclusion

Overall, this tutorial is aimed at applied scientists that want to explore machine learning solutions for their problems.

This tutorial focuses on a collection of “easy wins” that scientists can implement in their research to avoid catastrophic failures and increase reproducibility with all its benefits.",,HCWQZW,Jesper Dramsch
EFJC8C,Open Source Mission Support System for research aircraft missions,confirmed,,Talk,,2022-06-06T16:33:46.132790+00:00,,"The Mission Support System Software (MSS) is a client/server application developed in the community to collaboratively create flight plans based on model data. Through conda-forge, the components of MSS can be used on different platforms.","Examining the atmosphere by research aircraft is a costly and highly collaborative effort involving a large community of scientists, their one-of-a-kind measurement instruments and a very limited amount of available flight-hours.
The MSS Software (mss.rtfd.io) is used for planning research aircraft missions. Such missions involve the measurement of interesting atmospheric situation by research aircraft. These missions typically involve a wide range of one-of-a-kind instruments designed and operated by different scientific institution, with different requirements and operational conditions.
The software project contains several components. Scripts for automated download and process of weather forecast data for the server components. This model forecast data is needed to guide the aircraft to the location of interest. The data is visualized by an extended OGC webmap server capable of visualising the big data generated by complex 3-D atmospheric simulations in a highly configurable manner and delivering the resulting small PNG images over the internet. Another server based on socketIO enables collaborative working from different locations on the same flightpath simultaniously. The flightpath editor is a QT UI that allows to overlay the pictures produced by our server. The flightpath can be changed accordingly vertically and horizontally. We provide also a view based on the simulated data to visualize data similar to an insitu measurement along the flightpath.",,BY73RP,Reimar Bauer
EXQHBU,Discovering Mathematical Optimization with Python,confirmed,,Talk,,2022-06-13T21:41:22.792633+00:00,,"Mathematical optimization is the selection of the best alternative with respect to some criterion, among a set of candidate options. 

There are multiple applications of mathematical optimization. For example, in investment portfolio optimization, we search for the best way to invest capital given different alternatives. In this case, an optimization problem will allow us to choose a portfolio that minimizes risk (or maximizes profit), among all possible allocations that meet the defined requirements.

In most cases, mathematical optimization is used as a tool to facilitate decision-making. Sometimes these decisions can be made automatically in real-time.

This talk will explore how to formulate and solve mathematical optimization problems with Python, using different optimization libraries.","Mathematical optimization is an important tool in decision-making. With it, it is possible to optimize the economic benefit, time, distance, or any desired variable. 

The first step in optimization is the construction of a model. A good choice of model is essential. If the model is too simple, it will not provide useful information about the problem. If it is too complex, it may be too difficult to solve. After the model has been created, it is possible to solve the problem, usually with the help of a computer.

It is important to note that there is no universal optimization algorithm; rather, there are different algorithms that are adapted to different optimization problems. The correct choice of the right algorithm for a specific application usually rests with the user. This choice is important, as it can determine whether the problem is solved quickly or slowly and, indeed, whether the solution is found. 

In this talk we will learn how to solve mathematical optimization problems, using Python and different optimization libraries.",,HSUYYR,Pamela Alejandra Bustamante Faúndez
F399CM,Emergent structures in noisy channel message-passing,confirmed,,Talk,,2022-06-13T08:03:18.100369+00:00,,"In this [blog post](https://ichko.github.io/emergent-structures-in-robust-message-passing), we will explain a mechanism for generating neural network glyphs, like the glyphs we use in human languages. Glyphs are purposeful marks, images with 2D structure used to communicate information. We will use neural networks to generate those structured images, by optimizing for robustness.",[Emergent structures in noisy channel message-passing](https://ichko.github.io/emergent-structures-in-robust-message-passing) is a blog post I wrote explaining a few experiments I did with neural network image generation. The idea is that we want to generate images that are robust under some noise. So we generate images from random representations we perturb them and we try to decode the initial representation. This leads to the generator learning to create images with 2D structures.,,Z7UGMZ,Iliya Zhechev
GMBXCA,Python in Storm,confirmed,,Poster,,2022-06-06T02:38:56.711355+00:00,,"The advancement and development in the field of Technology and Communication call for a need for Real-time data processing that is fast and fault-tolerant. Apache Storm provides an epoch platform to develop applications that can process a multitude of data in real-time. Being distributed, Storm is predominantly fast and maintains high accuracy with its topological analysis and task completion checks.","The advancement and development in the field of Technology and Communication call for a need for Real-time data processing that is fast and fault-tolerant. Apache Storm is a distributed framework for the real-time processing of Big Data. Parallelism is the principle of the storm. Here the same code is executed on multiple nodes with different input data. Apache Storm utilizes Zookeeper for coordination. This enables Storm to start right from where it left even after the restart. Moreover, it is free, reliable, scalable, and fault-tolerant.

Outline 
1. Introduction to Apache Storm
- Design and Working, Real-time stream processing, Storm vs Hadoop, Advantages.
2. How Python effortlessly interfaces with Storm
- Python module to interface with the storm.
3. Demo and Q/A

Join in to find out how python persists in the storm world.",,KD8LS3,Soundharya Khanapur
GSQFSV,Introduction to scikit-learn II,confirmed,,Tutorial,,2022-07-13T22:59:38.388576+00:00,,"This tutorial will provide a beginner introduction to scikit-learn. Scikit-learn is a Python package for machine learning.

This tutorial will be subdivided into three parts. First, we will present how to design a predictive modeling pipeline that deals with heterogeneous types of data. Then, we will go more into detail in the evaluation of models and the type of trade-off to consider. Finally, we will show how to tune the hyperparameters of the pipeline.","This tutorial will provide a beginner introduction to scikit-learn. Scikit-learn is a Python package for machine learning.

This tutorial will be subdivided into three parts. First, we will present how to design a predictive modeling pipeline that deals with heterogeneous types of data. Then, we will go more into detail in the evaluation of models and the type of trade-off to consider. Finally, we will show how to tune the hyperparameters of the pipeline.",,"HRFVLY, FQUEVZ","Arturo Amor, Arkadiusz Trawiński, PhD"
HPTWUT,"Sliding into Causal Inference, with Python!",confirmed,,Talk (long),,2022-06-04T08:17:39.809937+00:00,,"What would the world look like if Russia had won the cold war? If the Boston Tea Party never happened? And where would we all be if Guido van Rossum had decided to pursue a career in theatre? Unfortunately we don't have the technology to slide into parallel worlds and explore alternative histories. However it turns out we *do* have the tools to simulate parallel realities and give decent answers to intriguing 'what if' questions. This talk will provide a gentle introduction to these tools, professionally known as Causal Inference.","The talk is aimed at data practitioners, preferably with basic knowledge of Python and statistics. That said, the focus of the talk is to nurture an intuitive understanding of the subject first, and implementation second. By the end of the talk I hope audience members could identify causal inference problems, have an intuitive understanding of the different tools they can apply to these problems, and have the appetite to further their learning!


The talk will cover the problem of answering causal questions (The Fundamental Problem of Causal Inference) and the main tools to address it. The emphasis will be on intuitive understanding how the different tools work, rather than pesky underlying assumptions, complex notation or convoluted literature. Just enough theory and lines of code to get the message across.

Outline:
 - *Introduction to parallel universes and ""what if?"" questions?* [2 mins]
 - *The golden standard for causal inference.*  We'll discuss randomised controlled experiments and also set the scene for cases these aren't possible. [6 mins]

Three key tools:
- *Differences-in-Differences* [3 mins]
- *Propensity score methods* [4 mins]
- *Synthetic Controls* (or: creating an alternate universe on your machine) [5 mins]

What's next: [5 mins]
A wrap up that includes:
- *What we didn't cover* (a few words about other techniques, DAGs, etc.)
- *Quick overview of Python tools for causal inference*
- *Where do we go from here* - resources, curriculums, readings and communities.",,7999Z9,Alon Nir
HYLCNT,Introduction to scikit-learn I,confirmed,,Tutorial,,2022-07-13T22:57:57.043315+00:00,,"This tutorial will provide a beginner introduction to scikit-learn. Scikit-learn is a Python package for machine learning.

This tutorial will be subdivided into three parts. First, we will present how to design a predictive modeling pipeline that deals with heterogeneous types of data. Then, we will go more into detail in the evaluation of models and the type of trade-off to consider. Finally, we will show how to tune the hyperparameters of the pipeline.","This tutorial will provide a beginner introduction to scikit-learn. Scikit-learn is a Python package for machine learning.

This tutorial will be subdivided into three parts. First, we will present how to design a predictive modeling pipeline that deals with heterogeneous types of data. Then, we will go more into detail in the evaluation of models and the type of trade-off to consider. Finally, we will show how to tune the hyperparameters of the pipeline.",,"HRFVLY, FQUEVZ","Arturo Amor, Arkadiusz Trawiński, PhD"
HYZRBT,Introduction to Audio & Speech Recognition,confirmed,,Tutorial,,2022-06-03T14:15:00.681069+00:00,,"The audio (& speech) domain is going through a massive shift in terms of end-user performances. It is at the same tipping point as NLP was in 2017 before the Transformers revolution took over. We’ve gone from needing a copious amount of data to create Spoken Language Understanding systems to just needing a 10-minute snippet. 

This tutorial will help you create strong code-first & scientific foundations in dealing with Audio data and build real-world applications like Automatic Speech Recognition (ASR) Audio Classification, and Speaker Verification using backbone models like Wav2Vec2.0, HuBERT, etc.","Unlike general Machine Learning problems where we either classify i.e. segregate a data point into a pre-defined class or regress around a continuous variable, audio related problems can be slightly more complex. Wherein, we either go from an audio representation to a text representation (ASR) or separate different layers of audio (Diarization) and so on. This tutorial will not only help you build applications like these but also unpack the science behind them using a code-first approach.

Every step of the way we’ll first write and run some code and then take a step back and unpack it all till it makes sense. We’ll make science *fun* again :)

The tutorial will be divided into 3 key sections:

1. Read, Manipulate & Visualize Audio data
2. Build your very own ASR system (using pre-trained models like Wav2Vec2.0) & deploy it
3. Create an Audio Classification pipeline & infer the model for other downstream audio tasks 

At the end of the tutorial, you’ll develop strong intuition about Audio data and learn how to leverage large pre-trained backbone models for downstream tasks. You’ll also learn how to create quick demos to test and share your models.

Libraries: HuggingFace, SpeechBrain, PyTorch & Librosa",,DHPEJJ,Vaibhav Srivastav
HZYDMX,Privacy Enhancing Technologies for Data Science,confirmed,,Keynote,,2022-08-23T13:17:31.076082+00:00,,"Privacy is becoming an increasingly pressing topic in data collection and data science. Thankfully, Privacy Enhancing Technologies (or PETs) are maturing alongside the growing demand and concern. In this keynote, we’ll explore what possibilities emerge when using Privacy Enhancing Technology like differential privacy, encrypted computation and federated learning and investigate how these technologies could change the face of data science today.","Privacy is becoming an increasingly pressing topic in data collection and data science. Thankfully, Privacy Enhancing Technologies (or PETs) are maturing alongside the growing demand and concern. In this keynote, we’ll explore what possibilities emerge when using Privacy Enhancing Technology like differential privacy, encrypted computation and federated learning and investigate how these technologies could change the face of data science today.",,KZSBEH,Katharine Jarmul
JBPYFQ,"Education - Materials, methods, tools",confirmed,,Maintainer track,,2022-08-29T08:55:56.550497+00:00,,"This session focuses on issues related to education in the ecosystem, from three different aspects, and during the session, we focus on recent advances and existing and upcoming challenges.

 * Materials: how are projects dealing with documentation and education materials
 * Methods: What should we do to make our materials more accessible to underrepresented and/or historically marginalised groups?
 * Tools: What are the existing tools in the ecosystem helping us achieve the above goals, and what do we need to develop?

We will give an overview of these different aspects.","This session focuses on issues related to education in the ecosystem, from three different aspects, and during the session, we focus on recent advances and existing and upcoming challenges.

 * Materials: how are projects dealing with documentation and education materials
 * Methods: What should we do to make our materials more accessible to underrepresented and/or historically marginalised groups?
 * Tools: What are the existing tools in the ecosystem helping us achieve the above goals, and what do we need to develop?

We will give an overview of these different aspects.",,S7MRUC,Mx Chiin-Rui Tan
JNXQPB,Introduction to NumPy,confirmed,,Tutorial,,2022-07-13T16:31:15.349772+00:00,,"This tutorial will provide an introduction to the NumPy library intended for beginners.

NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.","This tutorial will provide an introduction to the NumPy library intended for beginners.

You are encouraged to type along with me. For this you bring your laptop with a Firefox 90+ or Chromium 89+ installed. We will work through this repository: https://github.com/maikia/numpy-demo

NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.

This tutorial will notably introduce the following aspects:

- n-dimensional arrays (`ndarray`)
- indexing of `ndarray`
- operations on `ndarray`",,RJ3U9R,Maria Teleńczuk
JS97H9,How to make the most precise measurement,confirmed,,Talk,,2022-06-06T21:55:26.581924+00:00,,"Computer chips are created using photolithography. Today's lithography machines are highly complex machines containing ultra-high precision optics. How do you create and in particular measure these optics? That's easy, you build the world's best interferometer. But what if that's not enough?","Structures on computer chips are getting smaller and smaller. To be able to print these nanometer-sized structures correctly, the surfaces of the used lithography optics have to be manufactured to sub-nanometer precision. We have now reached technical limits on how exactly one can measure these. This talk explains how we can get beyond that.

Measurement precision and accuracy is all about separting the desired signal from any background. When all technical and signal processing techniques are exhausted, the only chance left is to know your background exactly. If you know everything about your measurement machine, you can calculate your background and thus separate signal from backgound. This talk describes how we at Carl Zeiss build complex simulations of our high-end measurement machines to enable the next generations of computer chips.",,8QK3TH,Markus Gruber
K8DEQK,Scientific Python / SPECs,confirmed,,Maintainer track,,2022-08-29T08:48:46.991946+00:00,,"The Scientific Python project aims to better coordinate the ecosystem and grow the community. This session focuses on our efforts to better coordinate project development, and to improve shared infrastructure. In this session together we will discuss project goals and recent technical work.

The Scientific Python project’s vision is to help pave the way towards a unified, expanded scientific Python community. It focuses its efforts along two primary axes: (i) to create a joint community around all scientific projects and (ii) to support maintainers by building cross-cutting technical infrastructure and tools. In this session we mostly focus on the second aspect.

The project has already launched a process whereby projects can, voluntarily, adopt reference guidelines; these are known as SPECs or Scientific Python Ecosystem Coordination documents. SPECs are similar to projects specific guidelines like PEPs, NEPs, SLEPs, and SKIPs, to name a few. The distinction being that SPECs have a broader scope, targeted at all (or most) projects from the scientific Python ecosystem.

The project also provides and maintains tools to help maintainers. This includes a theme for the project websites (used on, e.g., numpy.org and scipy.org), a self-hosted privacy-friendly web analytics platform, a community discussions forum, a technical blog, and project development statistics.

We present these tools, discuss various upcoming SPECs, and highlight the project’s future potential.","The Scientific Python project aims to better coordinate the ecosystem and grow the community. This session focuses on our efforts to better coordinate project development, and to improve shared infrastructure. In this session together we will discuss project goals and recent technical work.

The Scientific Python project’s vision is to help pave the way towards a unified, expanded scientific Python community. It focuses its efforts along two primary axes: (i) to create a joint community around all scientific projects and (ii) to support maintainers by building cross-cutting technical infrastructure and tools. In this session we mostly focus on the second aspect.

The project has already launched a process whereby projects can, voluntarily, adopt reference guidelines; these are known as SPECs or Scientific Python Ecosystem Coordination documents. SPECs are similar to projects specific guidelines like PEPs, NEPs, SLEPs, and SKIPs, to name a few. The distinction being that SPECs have a broader scope, targeted at all (or most) projects from the scientific Python ecosystem.

The project also provides and maintains tools to help maintainers. This includes a theme for the project websites (currently used on, e.g., numpy.org and scipy.org), a self-hosted privacy-friendly web analytics platform, a community discussions forum, a technical blog, and project development statistics.

We present all these tools, discuss various upcoming SPECs, and highlight the project’s future potential.

The Scientific Python project is already supported by eight core projects: IPython, Matplotlib, NetworkX, NumPy, pandas, scikit-image, scikit-learn, and SciPy. The organization has spent the last several months working on the infrastructure, and is now ready to engage more widely to help grow and support the community.",,XF3MEH,Jarrod Millman
KD3D7W,Introduction to pandas,confirmed,,Tutorial,,2022-07-13T16:45:22.860763+00:00,,"This tutorial is an introduction to pandas intended for beginners.

pandas is one of Python's core packages for data science. pandas organizes data into DataFrames and provides powerful methods for manipulating them. The library is built on top of NumPy. It'll be helpful for the tutorial if you have some experience with NumPy arrays, for example, by following the Introduction to NumPy tutorial.","You'll learn how to convert your existing Python lists and dictionaries to DataFrames and how to read data from files into DataFrames. pandas provides powerful methods for transforming data, and you'll get experience with these in hands-on exercises.

The tutorial covers the following topics:

- DataFrames as Panels of Data
- Create DataFrames
- Work With Tidy Data
- Transform DataFrames
- Share Results and Insights

The workshop consists of 90 minutes of live code demonstrations and hands-on exercises.",,CSTAZX,Geir Arne Hjelle
KWFH9N,Informative and pleasant dataviz with Raincloud plot,confirmed,,Poster,,2022-06-09T15:03:16.489391+00:00,,"Categorical plots offer a variety of plotting styles that allows the user to picture even large datasets showing some summary statistics of the data. In some case graphs can be misleading, either unwilling or on purpose. The reader can be confused or even get an incorrect idea of the phenomenon underlying the data.
In this talk we introduce the Raincloud plot, a multi-language plotting style aimed to create charming and informative graphical representations of a dataset. After some introduction, we will offer a simple tutorial that will cover different use cases. We will then compare the Raincloud plots with some other plot styles, showing that some data misunderstanding can be avoided with a sufficiently detailed plot.","Categorical plots offer a variety of plotting styles that allows the user to picture even large datasets showing some summary statistics of the data. In some case graphs can be misleading, either unwilling or on purpose. The reader can be confused or even get an incorrect idea of the phenomenon underlying the data.
In this talk we introduce the Raincloud plot, a multi-language plotting style aimed to create charming and informative graphical representations of a dataset. After some introduction, we will offer a simple tutorial that will cover different use cases. We will then compare the Raincloud plots with some other plot styles, showing that some data misunderstanding can be avoided with a sufficiently detailed plot.",,TTUQ3P,Davide Poggiali
LF777M,Network Science with Python,confirmed,,Tutorial,,2022-06-05T09:43:11.508726+00:00,,"This workshop is for data scientists and other programmers who want to add another tool in their data science toolkit. Modelling, analysing and visualising data as networks! Network Science deals with analysing network data, and the data can come from different fields like politics, finance, computer science, law and even Game of Thrones!","In this workshop we will cover the basics of network theory and network thinking, then we will go over some algorithms used to analyse network data. We will take a quick detour to understand the story between linear algebra and networks. We will then jump on to some real world datasets and how to use our newly acquired skills to tackle the data problems.

This will be a hands-on and interactive tutorial, so get ready to code your way till the end!

By the end of the workshop you should be comfortable with working with network data using the PyData ecosystem (NetworkX, pandas, numpy!).

We will roughly follow the following timeline during the workshop (30 mins each)

Part A: Introduction to Graphs and the NetworkX API

Part B: Graph Algorithms
- Hubs: Which nodes are the important nodes in our data?
- Paths: Where should I jump next to find my destination?
- Structures: Who should I be friends with?

Part C: Linear Algebra and Network Science
- What do matrices have to do with nodes and edges?",,WTK33D,Mridul Seth
LYBCHK,Data-Driven Thresholding for Extreme Event Detection in Geosciences,confirmed,,Talk,,2022-06-05T17:22:50.847852+00:00,,"Extreme weather events are a well known source of human suffering, loss of life, and financial hardship. Amongst these, tropical cyclones are notoriously impactful, leading to significant interest in predicting the genesis, tracks, and intensity of these storms - a task which continues to present significant challenges. In particular, tropical cyclogenesis (TCG) can be described as “a needle in a haystack” problem, and steps must be taken to make predictions tractable. Previously, the filtering of non-genesis points by thresholding predictive variables has been described, with thresholds being selected to reduce the number of discarded TCG cases. In the art, this thresholding has often been carried out empirically, that while effective relies on domain knowledge. This talk instead proposes the development of a systematic, machine-learning-based approach implemented in Python. The method is designed to be interpretable to the point of becoming transparent machine learning. Threshold values that minimize the false-alarm rate and maintain a high recall are found, and then combined in a forward selection algorithm. As other extreme events in the geosciences are considered needle in the haystack problems, the described approach can be of use in reducing the variable space in which to study and predict the events. Finally, the transparent nature of the proposed approach can provide simple insight into the conditions in which these events occur.","Extreme weather events are a well known source of human suffering, loss of life, and financial hardship. Amongst these, tropical cyclones are notoriously impactful, leading to significant interest in predicting the genesis, tracks, and intensity of these storms - a task which continues to present significant challenges. In particular, tropical cyclogenesis (TCG) can be described as “a needle in a haystack” problem, and steps must be taken to make predictions tractable. Previously, the filtering of non-genesis points by thresholding predictive variables has been described, with thresholds being selected to reduce the number of discarded TCG cases. In the art, this thresholding has often been carried out empirically, that while effective relies on domain knowledge. This talk instead proposes the development of a systematic, machine-learning-based approach implemented in Python. The method is designed to be interpretable to the point of becoming transparent machine learning. Threshold values that minimize the false-alarm rate and maintain a high recall are found, and then combined in a forward selection algorithm. As other extreme events in the geosciences are considered needle in the haystack problems, the described approach can be of use in reducing the variable space in which to study and predict the events. Finally, the transparent nature of the proposed approach can provide simple insight into the conditions in which these events occur.",,JZ3LXH,Milton Gomez
P7FQ3Y,Deep learning at the Radiology & Nuclear Medicine Clinic / University Hospital Basel,confirmed,,Talk,,2022-05-31T10:40:51.550853+00:00,,Deep learning can assist radiology doctors in interpreting and analyzing radiology images. We will present use cases which are used today in clinical practice. These range from organ segmentation to image classification.,"We will present how python and deep learning is used at the radiology department at the University Hospital Basel. The talk should give an overview of how the use cases are initially designed, developed and brought to production. One use case will be how deep learning helped quantifying the severity of COVID-19 infected lungs. Another use case will be how it can help to detect vertebrae fractures.",,"E8ZC8Z, SMGL7F","Joshy Cyriac, Jakob Wasserthal"
PA7NUF,Contributor Experience & Diversity,confirmed,,Maintainer track,,2022-08-29T09:04:02.276834+00:00,,"Most of us have been hearing about Diversity Equity and Inclusion (DEI) for some years now, and even had access to many resources by now. Our projects have codes of conduct, and some have been doing sprints and mentorships. But how much has fundamentally changed?

Let’s meet for an honest conversation about the challenges of DEI actions, and culture change. How do we achieve long-term impact? What are low-hanging fruit? We can share hard-to-ask questions, effective tools, experiences that shaped our approach, and see if we can all nudge each other forward a little.

Inclusion happens at the community level, also when we want to address DEI itself. So, we will need to create a safe space for hard questions and leave judgment at the door.

Thanks to our grant to advance an inclusive culture in the scientific Python ecosystem, we have created the contributor experience lead role. We have been working with NumPy, SciPy, Matplotlib, and pandas to learn how to integrate this new role to a project, and how to introduce contributor hospitality techniques. We are working on creating widely available resources, and we would benefit from hearing from maintainers from the wider community.","Most of us have been hearing about Diversity Equity and Inclusion (DEI) for some years now, and even had access to many resources by now. Our projects have codes of conduct, and some have been doing sprints and mentorships. But how much has fundamentally changed?

Let’s meet for an honest conversation about the challenges of DEI actions, and culture change. How do we achieve long-term impact? What are low-hanging fruit? We can share hard-to-ask questions, effective tools, experiences that shaped our approach, and see if we can all nudge each other forward a little.

Inclusion happens at the community level, also when we want to address DEI itself. So, we will need to create a safe space for hard questions and leave judgment at the door.

Thanks to our grant to advance an inclusive culture in the scientific Python ecosystem, we have created the contributor experience lead role. We have been working with NumPy, SciPy, Matplotlib, and pandas to learn how to integrate this new role to a project, and how to introduce contributor hospitality techniques. We are working on creating widely available resources, and we would benefit from hearing from maintainers from the wider community.",,AHQBCP,Noa Tamir
PFQPTF,Renku-Python: Reproducible and Reusable Workflows,confirmed,,Poster,,2022-05-30T15:20:15.478301+00:00,,"Renku is a platform that bundles together various tools for reproducible and collaborative data analysis projects. Here we take a deep dive into the Python CLI and library component of the Renku platform, highlighting its functionality for recording and executing workflows both locally and remotely, as well as its architecture for storing recorded metadata in a knowledge graph and how this can be extended by third-party plugins.","Renku is a platform that bundles together various tools for reproducible and collaborative data analysis projects. Here we take a deep dive into the Python CLI and library component of the Renku platform, highlighting its functionality for recording and executing workflows both locally and remotely, as well as its architecture for storing recorded metadata in a knowledge graph and how this can be extended by third-party plugins.

The Python component of Renku supports tracking workflows, datasets and projects. This talk will focus on the workflow functionality, which allows recording, composing and reusing workflows on the command line and executing them with different execution backends (such as CWL, Toil or Argo), be it locally, on HPC or in the cloud.  
It allows reusing the same workflow with a range of parameters, tracking every execution and recording provenance of all used inputs and produced outputs. This makes it easier than ever to share your process with others and have them reproduce and reuse it. Additionally, third-party tools and plugins can enrich the Renku metadata with arbitrary data of their own, allowing Renku to support a multitude of use-cases and to integrate with different domains.

The integrated Python API allows easy access to this metadata, making it simple to extend and build on top of the Renku platform. The system for storing this metadata integrates with Git and normal development workflows and uses an efficient implementation that allows translation from internally-stored metadata to other formats, such as JSON-LD to integrate as part of our larger knowledge graph or exporting to third-party workflow languages.

We will also discuss planned future changes and how we plan to support interoperability with other workflow systems as well as easy integration with remote storage options such as S3 for large amounts of data, as well as how these features integrate with and streamline working with the Renku platform as a whole.",,VJ9CZQ,Ralf Grubenmann
PRYDKM,dirty_cat : a Python package for Machine Learning on Dirty Categorical Data,confirmed,,Poster,,2022-06-09T15:16:51.460538+00:00,,"In this talk, we will introduce ""dirty_cat"", a Python library for encoding dirty, non-curated categorical features into numerical features while preserving similarities.
We will focus on a few methods implemented in the similarity encoder, the Gamma-Poisson encoder, the min-hash encoder and the super-vectorizer.","Machine learning models are used in many applications to predict a target variable given features that provides additional information. For instance, we can be interested in predicting the salary of an employee (target) given its job title and experience level (features). However, machine learning algorithms only operate on numerical features. When dealing with categorical features like job titles, a preprocessing step must be performed in order to encode them into numerical features.

A naive way to encode such data into numerical features is one-hot encoding: each job title is assigned a binary vector that is orthogonal to those of other jobs. This simple method encodes each category (job title) independently from the others, and therefore fails to capture similarities between them. For instance, ""Police Captain"" is more similar to ""Police Cadet"" than ""Property Manager II"" and thus they are more likely to have similar salaries. Preserving this information when encoding categories is crucial to the performance of the machine learning model.

Dirty-cat is a Python package that provides tools to easily encode categorical data into numerical features while capturing similarities between categories: after encoding, similar categories are described by vectors with close coefficients, thus improving prediction. The package provides a user-friendly interface to easily transform a raw table with string features into a clean table that can be directly leveraged by machine learning models.

We will focus on methods developed in ""Similarity encoding for learning with dirty categorical variables"" and ""Encoding high-cardinality string categorical variables"" by Patricio Cerda et al., and implemented in the Gamma-Poisson encoder, the similarity encoder, the min-hash encoder, and the super-vectorizer.",,YRSL8P,Lilian Boulard
QG8WUR,Optimizing inference for state of the art python models,confirmed,,Talk,,2022-06-13T17:13:22.533809+00:00,,"This talk will take state of the art python models and show how, through advanced inference techniques, we can drastically increase the performance of the models at runtime. You’ll learn about the open source MLServer project and see live how easily it helps serve python-based machine learning models.","Machine learning models are often created with an emphasis on how they run during training but with little regard for how they’ll perform in production. In this talk, you’ll learn what those issues are and how to address them using some state of the art models as an example. We’ll introduce the open source project, MLServer, and look at how features, such as multi-model serving and adaptive batching, can optimize performance for your models. Finally, you’ll learn how using an inference server locally can speed up the time to deployment when moving to production.",,GBEPWN,Ed Shee
QHUSYT,Python in the browser,confirmed,,Maintainer track,,2022-08-30T09:04:39.344626+00:00,,"Recently it became possible to run Python and the scientific Python packages in the browser thanks to WebAssembly and [Emscripten](https://emscripten.org/). This is done in particular in the [Pyodide](https://github.com/pyodide/pyodide) and [emscripten-forge](https://github.com/emscripten-forge/recipes) projects. It allows for a scientific Python application, or a compute environment such as [JupyterLite](https://github.com/jupyterlite/jupyterlite), to be seamlessly accessible to a large number of users with very little effort or infrastructure requirements.

At the same time, the scientific Python ecosystem did not evolve with the web in mind. We will discuss some of the challenges package maintainers may face when trying to run their package in the browser, and what could be done to overcome these.","Recently it became possible to run Python and the scientific Python packages in the browser thanks to WebAssembly and [Emscripten](https://emscripten.org/). This is done in particular in the [Pyodide](https://github.com/pyodide/pyodide) and [emscripten-forge](https://github.com/emscripten-forge/recipes) projects. It allows for a scientific Python application, or a compute environment such as [JupyterLite](https://github.com/jupyterlite/jupyterlite), to be seamlessly accessible to a large number of users with very little effort or infrastructure requirements.

At the same time, the scientific Python ecosystem did not evolve with the web in mind. We will discuss some of the challenges package maintainers may face when trying to run their package in the browser, and what could be done to overcome these.",,TAQFCQ,Roman Yurchak
QRAUXL,Array expressions and symbolic gradients in SymPy,confirmed,,Talk,,2022-05-30T21:59:05.471566+00:00,,"SymPy is an open source computer algebra system (CAS) written in Python.

The recent addition of the array expression module provides an alternative to the matrix expression module, with generalized support to higher dimensions (matrices are constrained to 2 dimensions).

Given the importance of multidimensional arrays in machine learning and mathematical optimization problems, this talk will illustrate examples of tensorial expressions in mathematics and how they can be manipulated using either module or in the index-explicit way.

Conversion tools have been provided to SymPy to allow users to switch an expression between the array form and either the matrix or index-explicit form. In particular, the conversion from array to matrix form attempts to represent contractions, diagonalizations and axis-permutations with operations commonly used in matrix algebra, such as matrix multiplication, transposition, trace, Hadamard and Kronecker products. 

A gradient algorithm for array expressions has been implemented, returning a closed-form array expression equivalent to the derivative of arrays by arrays. The derivative algorithm for matrix expressions now uses this algorithm, attempting to convert the array back to matrix form if trivial dimensions can be dropped.","SymPy supports various ways to represent multidimensional mathematical expressions. For example, the matrix product M N where both M and N are matrix symbols, may be represented as:

- `M*N` ==> matrix expression
- `ArrayContraction(ArrayTensorProduct(M, N), (1, 2))` ==> array expression equivalent to matrix multiplication
- `Sum(M[i, j]*N[j, k], (j, ...))` ==> index-explicit expression with summation equivalent to matrix multiplication

Conversion from the array expression form is implemented in SymPy. Conversions generally work, except for some higher-dimensional array expression forms that cannot be converted to matrix expressions.

The derivative of an array expressions by another array symbol returns an array expression. In the index-explicit form it is performed by using Kronecker delta, while the matrix form currently relies on the conversions to the array form.",,NYVKBG,Francesco Bonazzi
RANUF3,Scipp: Multi-dimensional data arrays with labeled dimensions for dense and binned data,confirmed,,Poster,,2022-05-27T12:29:36.942627+00:00,,"Inspired by Xarray, Scipp (https://scipp.github.io/) enriches raw NumPy-like multi-dimensional arrays of data by adding named dimensions and associated coordinates. For an even more intuitive and less error-prone user experience, Scipp furthermore adds physical units to arrays and their coordinates. Scipp data arrays additionally support a dictionary of masks, basic propagation of uncertainties, and bin-edge coordinates.

On top of the above, Scipp's key feature is support for multi-dimensional non-destructive binning of record-based ""tabular"" data into arrays of bins. The use of labeled arrays with coordinates to represent the table of records allows for clear conceptual association of a record's metadata with dimensions and coordinates of the array of bins. Based on this, Scipp can provide fast, flexible, and efficient binning, rebinning, and filtering operations, all while preserving the original individual records.

Scipp ships with data display and visualization features for Jupyter notebooks, including a powerful plotting interface.","Scipp (https://scipp.github.io/) is in part inspired by [Xarray](https://docs.xarray.dev/en/stable/).
It enriches raw NumPy-like multi-dimensional arrays of data by adding named dimensions and associated coordinates.
Multiple arrays can be combined into datasets.
On top of this, Scipp provides:

- Physical units are stored with each value or coordinate array and are handled in arithmetic operations.
- Support for record-based ""tabular"" data and non-destructive binning thereof, referred to as binned data.
- Support for masks stored alongside data.
- Direct support for histograms, i.e., bin-edge coordinates which are by 1 longer than the data extent.
- Basic propagation of uncertainties.
- Data display and visualization features for Jupyter notebooks, including a powerful plotting interface.

Physical units, dimension labels, and dictionaries of coordinate arrays are used to verify that operands are compatible prior to an operation.
Dimension labels are used for safe and automatic broadcasting and transposition.
In contrast to NumPy, equivalent Scipp functions for array manipulation use dimension labels instead of axis indices.
Selecting slices based on coordinate values is also supported.
See [Data Structures](https://scipp.github.io/user-guide/data-structures.html), [Slicing](https://scipp.github.io/user-guide/slicing.html), and [Computation](https://scipp.github.io/user-guide/computation.html) for more details.

Scipp's support for binned data is conceptually similar to an [Awkward Array](https://awkward-array.readthedocs.io/en/latest/) of records.
While Scipp is less flexible and feature complete than Awkward, Scipp's use of labeled arrays with coordinates (and masks) allows for clear conceptual association of a record's metadata with coordinates of the parent array.
Based on this, Scipp can provide flexible and efficient binning, rebinning, and filtering operations, all while preserving the original individual records.
See [Binned Data](https://scipp.github.io/user-guide/binned-data/binned-data.html) for more details.

Unlike NumPy's masked arrays (`numpy.ma` module), Scipp keeps a dictionary of masks.
Each mask can have a different subset of the data's dimensions, allowing for space-efficient storage of, e.g., a mask of ""rows"" and a mask of ""columns"" of 2-D data.
Scipp propagates masks in binary operations and applies them automatically to data where necessary, e.g., when summing over a dimension.
See [Masking](https://scipp.github.io/user-guide/masking.html) for more details.

In many applications for the physical sciences, coordinates of data need to be transformed into different coordinates.
These transformations can turn into complex multi-step processes involving multiple inputs and multiple outputs.
Dealing with record-based data partitioned into bins further complicates the situation.
Therefore, Scipp provides a graph-based mechanism for defining such transformations based on simple functions implementing individual coordinate transformation steps.
Applying the transformation is then performed by built-in Scipp functionality.
See [Coordinate Transformations](https://scipp.github.io/user-guide/coordinate-transformations.html) for more details.

Scipp aims to provide wrappers for common SciPy functionality, for a safer and higher-level user interface.
For example, instead of positional axis indices Scipp's SciPy wrapper use dimension labels, and Y and X arrays are implicitly given by an arrays values and a coordinate of the array.

While Scipp is mainly intended as a Python library, it is built around a core C++ library, which uses TBB for multi-threading, providing good out-of-the-box performance.

Scipp is developed by the European Spallation Source, a neutron scattering facility under construction in Lund, Sweden.
Scipp is available under the open source BSD-3 licence from PyPI (`pip install scipp`) or via conda (`conda install -c conda-forge -c scipp scipp`).",,FXPCAL,Simon Heybrock
RHYT3R,What is Contributor Experience?,confirmed,,Talk (long),,2022-06-01T14:47:53.760538+00:00,,"In my current work as a contributor experience lead, I am supporting and growing Matplotlib’s and Pandas’ communities by organizing events, meetings, and proactive engagement with a focus on equity and inclusion of historically marginalized groups. In my talk I’ll give an introduction to this new role, the grant that supports it, and some of the work done so far…

I will share takeaways for maintainers, and contributors; from simple changes that can be implemented relatively easily, to bigger topics, which one might want to learn more about, and slowly yet proactively, facilitate changes to tweak the contributor experience for a project.","In my current work as a contributor experience lead, I am supporting and growing Matplotlib’s and Pandas’ communities by organizing events, meetings, and proactive engagement with a focus on equity and inclusion of historically marginalized groups. In my talk I’ll give an introduction to this new role, the grant that supports it, and some of the work done so far…

I will share takeaways for maintainers, and contributors; from simple changes that can be implemented relatively easily, to bigger topics, which one might want to learn more about, and slowly yet proactively, facilitate changes to tweak the contributor experience for a project.",,AHQBCP,Noa Tamir
RRQVRN,Introduction to PyTorch,confirmed,,Tutorial,,2022-07-28T10:23:52.039233+00:00,,"In this tutorial we will go through the main features of the `PyTorch` framework for Deep Learning. 
We will start by learning how to build a neural network from the ground up, deep diving into `torch.tensor`,  `Dataset` and `optimisers`. 
We will analyse data cases from different domains (e.g. _numerical_, _images_), introducing different neural network layers and architecture. Last but not least, a few tips from a pure Data science-y perspective will be shared, to appreciate the wonderful integration PyTorch has with the Python Data model!","In this tutorial we will go through the main features of the `PyTorch` framework for Deep Learning. 
We will start by learning how to build a neural network from the ground up, deep diving into `torch.tensor`,  `Dataset` and `optimisers`. 
We will analyse data cases from different domains (e.g. _numerical_, _images_), introducing different neural network layers and architecture. Last but not least, a few tips from a pure Data science-y perspective will be shared, to appreciate the wonderful integration PyTorch has with the Python Data model!",,YQKWBT,Valerio Maggio
S7NHQE,ReservoirPy: Efficient Training of Recurrent Neural Networks for Timeseries Processing,confirmed,,Poster,,2022-06-06T18:35:37.902944+00:00,,"ReservoirPy is a simple user-friendly library based on Python scientific modules. It provides a flexible interface to implement efficient Reservoir Computing (RC) architectures with a particular focus on Echo State Networks (ESN). Advanced features of ReservoirPy allow to improve computation time efficiency on a simple laptop compared to basic Python implementation, with datasets of any size.

Some of its features are: offline and online training, parallel implementation, sparse matrix computation, fast spectral initialization, advanced learning rules (e.g. Intrinsic Plasticity) etc. It also makes possible to easily create complex architectures with multiple reservoirs (e.g. deep reservoirs), readouts, and complex feedback loops. Moreover, graphical tools are included to easily explore hyperparameters with the help of the hyperopt library. It includes several tutorials exploring exotic architectures and examples of scientific papers reproduction. Moreover, graphical tools are included to easily explore hyperparameters with the help of the hyperopt library. ReservoirPy is available on GitHub https://github.com/reservoirpy/reservoirpy with the open source MIT license, it includes a detailed documentation https://reservoirpy.readthedocs.io and a pypi package for easy installation.","From weather to language, extracting information from data streams is a key issue in Artificial Intelligence. Reservoir Computing (RC) is particularly well suited to take these temporal dynamics into account. It is a machine learning paradigm on sequential data where an artificial neural network is only partially trained. One of the major interests of these recurrent neural networks is their reduced computational cost and the possibility to learn both in on-line and off-line fashion. They have been successfully applied on a wide variety of tasks, from prediction/generation of chaotic timeseries to discrimination of audio sequences, such as bird song recognition. They offer de facto a fast, simple yet efficient way to train RNNs. This ""reservoir of computations"" works thanks to random projections in large dimensions, and is thus similar to temporal Support Vector Machines (SVM).

We present a library that facilitate the creation of RC architectures, from simplest to most complex, based on the Python scientific stack (NumPy, Scipy, Matplotlib). This library offers memory and time efficient implementations for both online and offline training paradigms, such as FORCE learning or parallel ridge regression. The flexibility of the API allows to quickly design ESNs including re-usable and customizable components. It enables to build models such as DeepESNs as well as other advanced architectures with complex connectivity between multiple reservoirs with feedback loops. Extensive documentation and tutorials both for newcomers and experts are provided through GitHub and ReadTheDocs websites.

The presentation will introduces intuitions on the Reservoir Computing paradigm followed by the main features of the ReservoirPy library, illustrated with code examples covering popular RC techniques from the literature.

We argue that such flexible dedicated library will ease the creation of more advanced architectures while guarantying its correct implementation and reproducibility across the RC community. Related projects are also included in the ReservoirPy organisation https://github.com/reservoirpy such as an interface with R language to ReservoirPy.

Github: https://github.com/reservoirpy/reservoirpy

Documentation: https://reservoirpy.readthedocs.io/

Related projects: https://github.com/reservoirpy

R interface to ReservoirPy: https://github.com/reservoirpy/reservoirR",,KWE7XB,Xavier Hinaut
SSZFWQ,Introduction to Python for scientific programming,confirmed,,Tutorial,,2022-07-13T16:19:26.090323+00:00,,"This tutorial will provide an introduction to Python intended for beginners.

It will notably introduce the following aspects:

- built-in types
- controls flow (i.e. conditions, loops, etc.)
- built-in functions
- basic Python class","This tutorial will provide an introduction to Python intended for beginners.

It will notably introduce the following aspects:

- built-in types
- controls flow (i.e. conditions, loops, etc.)
- built-in functions
- basic Python class

We introduce here the Python language. Only the bare minimum necessary for getting started with Numpy and Scipy is addressed here. To learn more about the language, consider going through the excellent tutorial https://docs.python.org/tutorial.",,KSWEHL,Mojdeh Rastgoo
SYA7GA,Interactive Data Science in the browser with JupyterLite and Emscripten Forge,confirmed,,Talk (long),,2022-06-04T18:00:32.859822+00:00,,"JupyterLite is a Jupyter distribution that runs entirely in the web browser, backed by in-browser language kernels including WebAssembly powered Jupyter Xeus kernels and Pyodide.

JupyterLite enables data science and interactive computing with the PyData scientific stack, directly in the browser, without installing anything or running a server.

JupyterLite leverages the Emscripten and Conda Forge infrastructure, making it possible to easily install custom packages with binary extensions in the browser, such as numpy, scipy and scikit-learn.","This will be a functional talk to present JupyterLite with concrete examples and live demos in the web browser.

The structure of the presentation is as follows:

- Introduction
- Easy interactive computing in the browser
- A lightweight Jupyter Frontend running in the browser
  - Boots in seconds
  - Deployment and scalability made easy
- Demos of the Features:
  - the WebAssembly powered Python kernel
  - Support for existing JupyterLab Extensions, IPython, Jupyter Widgets, interactive visualizations
  - Support for Real Time Collaboration
  - Working with files and notebooks
- A quick overview of the underlying architecture
  - Plugin-based as a Lumino application
  - Extensible by design: most JupyterLab extensions are compatible by default and new kernels can easily be added
- Emscripten Forge
  - Presentation of the distribution that enables the emscripten platform for the mamba (conda) package manager
  - Allows freezing in time a given WebAssembly environment. This will enable a new level of computational reproducibility.
  - Based on the WebAssembly standard supported by all web browsers
  - Demo of using a custom set of Python packages in a JupyterLite deployment
- A wide range of use cases
  - Embed a live Python console on your website 🚀
  - Powering the numpy and sympy documentation to let anyone try the libraries in the browser without installing anything
  - Education: easy access to computing environment without the trouble of installing anything
  - Reducing the load on public services like mybinder.org
- What's coming up next",,"KA8ZHH, JW8BQY, TDTZCJ","Jeremy Tuloup, Martin Renou, Thorsten Beier"
TXX8MQ,"conda-forge, mamba, boa and quetz - the evolution of package management for data science and beyond",confirmed,,Talk,,2022-06-01T18:47:35.550348+00:00,,"Mamba is a fast, cross-platform and language independent package manager that is fully compatible with conda packages. 
It has enabled the conda-forge project to scale way beyond what was previously possible. 
In this talk we present further innovations in the mamba ecosystem, including boa, a new build tool based on mamba and quetz, an open-source and extensible package server for conda packages.","Mamba is widely used in the conda-forge community, a binary distribution of scienitific open-source packages for Windows, macOS and Linux. Today, mamba powers hundreds of automated builds for Python, C, C++ and many other programming languages every day. In this talk we will go into some of the details that make the conda-forge project and the mamba integration so successful.

Furthermore, we will highlight some interesting new innovations coming to the mamba project. Thanks to a CZI grant we currently work on the sandboxing of environments on Linux and macOS, zchunk support for repodata, package metadata signing, OCI and S3 support and more.

Additionally, we will take a look at a new build tool called boa. Boa comes with a new recipe format that is cleaner and faster to evaluate and makes building new packages a smooth  process. It comes with an interactive mode to make packages easily debuggable. We will show how we have used boa to produce an experimental WASM distribution called ""emscripten-forge"" that builds WebAssembly enabled conda-packages that can readily be used in the web-browser with JupyterLite.

Lastly, we will show recent improvements in quetz, the open-source conda package server. It is fully extensible, both in the front- and backend – something we want to use with the conda-forge project to add more features for users and package maintainers such as fast (metadata) package search.",,M7CWJZ,Wolf Vollprecht
TXYGUK,Time Series Forecasting with scikit-learn's Quantile Gradient Boosted Regression Trees,confirmed,,Tutorial,,2022-06-06T20:53:06.167426+00:00,,"This tutorial will introduce how to leverage scikit-learn's powerful
**histogram-based gradient boosted regression trees** with various loss functions
(Least squares, **Poisson** and the **pinball loss** for quantile estimation) on a time
series forecasting problem. We will see how to leverage pandas to build **lag and
windowing features** and [scikit-learn](https://scikit-learn.org) time-series cross-validation tools and other
model evaluation tools.","This tutorial is intended for an audience with some familiarity with data
science tools and machine learning concepts. It will start from practical
considerations on how to manipulate the data and fit simple yet powerful
models and progressively move to more advanced considerations on model
evaluation.

The main focus is to show how to **cast a time series forecasting problem into a
supervised machine learning problem** (non-linear regression) using basic
pandas-based feature engineering, time series aware cross-validation and
highlighting the impact of the choice of the loss function of gradient boosting
models.

We will compare this forecasting to a baseline that only leverages instantaneous
contextual variables as predictors using scikit-learns feature engineering tools
(column transformers and pipelines) with a particular emphasis how to build
**cyclic time-derived features** and categorical variables.

We will then dive deeper into model evaluation assessing various performance metrics with time-series aware cross-validation.

We will compare uncertainty bounds from quantile regression with conformal prediction methods from MAPIE.

Finally we will explore how to deal with the auto-regressive setting to predict forecast for a multi-step horizon with `sktime`.

The tutorial will be available as a Jupyter notebook and the audience will be
encouraged to develop there how intuitions by experimenting interactively with
the teaching material.

If time allows, we will also compare this approach with alternative solutions
based on neural networks or linear models trained on rich spline-based features.",,NEUMLP,Olivier Grisel
UCDNLV,Effectively using matplotib,confirmed,,Tutorial,,2022-07-13T16:47:26.210003+00:00,,This tutorial explains the fundamental ideas and concepts of matplotlib. It's suited for complete beginners to get started as well as existing users who want to improve their plotting abilities and learn about best practices.,"Matplotlib is one of the most-used and powerful visualization libraries for python. Nevertheless, there has been and still is some confusion on how use it properly. This has a number of reasons ranging from an evolution of the API and lack of good documentation to the complexity that comes with the large feature set and flexibility. But these issues can be overcome.

This tutorial will explain the main concepts and intended usage patterns of matplotlib. Knowing these, lets you effectively use high-level functions for most of the cases. But you will be able to go into the details if you need to fine-tune certain aspects of the plot. We'll also touch some nowadays discouraged ways of working from the past (you should know what not to do - even though that's still found in lots of examples on the web) and we may get a glimpse into the future.

Tim Hoffmann joined the matplotlib core development team almost two years ago with the mission to make matplotlib easier to use.

Requirements and set up instructions:
Jupyter plus any recent (>=3.0) matplotlib version will do. To be on the safe side, you may set up a new conda environment using conda create -n using-mpl matplotlib jupyterlab pandas ipympl.

Notebooks: https://github.com/timhoffm/using-matplotlib",,X8YQJR,Tim Hoffmann
UNTLSR,"conda-forge: supporting the growth of the volunteer-driven, community-based packaging project",confirmed,,Talk (long),,2022-06-13T18:53:12.325772+00:00,,"The conda-forge project is one of the fastest growing Open Source communities out there – and most data scientists have probably heard of it. In this talk we explain the inner workings of conda-forge, its relationship to conda and PyPI, and we will explain how everyone can package software with conda-forge.","The conda-forge project offers binary packages for Linux, macOS and Windows. It has greatly simplified software distribution for millions of users. As a volunteer-run Open Source project with build instructions for a plethora of software packages that are most often used in the multidisciplinary community of scientific computing, conda-forge has seen an unprecedented growth, not the least because of the success of Python in that field.

In this talk we’ll take a look at the past, present and future of the project and showcase a number of projects in the overarching conda ecosystem that help with scaling conda-forge.

The talk will go into detail how the conda-forge project is maintained by volunteers and where collaboration between participating organizations is a key aspect of shaping its future.
We will also answer the question “Why not just use PyPI and the Python packaging stack?” and show how easy it is to create and submit packages to conda-forge.",,"M7CWJZ, LEG8KZ, 9YF3VD","Wolf Vollprecht, Jannis Leidel, Jaime Rodríguez-Guerra"
UWGJXA,Evaluating your machine learning models: beyond the basics,confirmed,,Tutorial,,2022-06-06T19:48:29.574408+00:00,,"This tutorial will guide towards good evaluation of machine-learning models, choosing metrics and procedures that match the intended usage, with code examples using the latest scikit-learn's features. We will discuss how good metrics should characterize all aspects of error, e.g. on the positive and negative class; the probability of a detection, or the probability of a true event given a detection; as they may need to catter for class imbalance. Metrics may also evaluate confidence scores, e.g. calibration. Model-evaluation procedures should gauge not only the expected generalization performance, but also its variations.","Model evaluation is a crucial aspect of machine-learning, to choose the best model, or to decide if a given model is good-enough for production. This tutorial will give didactic introductions to the various statistical aspects of model evaluation: what aspects of model prediction are important to capture, and how different metrics available in scikit-learn captures them. How to devise a model-evaluation procedure that is best suited to select the best model, or control that a model is suited for usage. This tutorial goes beyond mere application of scikit-learn and we expect even experts to learn useful considerations.

The tutorial will be loosely based on the following preprint https://hal.archives-ouvertes.fr/hal-03682454, but with code examples for each important concept. A tentative outline is as follows:

### Performance metrics

#### Metrics for classification

- Binary classification
    - Confusion matrix
    - Simple summaries and their pitfalls
    - Probability of detection given true class, or vice versa?
    - Summary metrics for low prevalence
    - Metrics for shifts in prevalence
    - Multi-threshold metrics
    - Confidence scores and calibration
- Multi-class classification
    - Adapting binary metrics to multi-class settings
    - Multilabel classification

#### Metrics for regression

- R2 score
- Absolute error measures
- Assessing the distribution of errors

### Evaluation strategies

#### Evaluating a learning procedure

- Cross-validation strategies
- Driving model choices: nested cross-validation
- Statistical testing
    - Sources of variance
    - Accounting for benchmarking variance

#### Evaluating generalization to an external population

- The notion of external validity
- Confidence intervals for external validation",,"MXCVQL, HRFVLY","Gaël Varoquaux, Arturo Amor"
VFU9JL,Lessions learned from 10 years of Python in industrial reseach and development,confirmed,,Talk (long),,2022-06-06T20:46:11.801104+00:00,,"This talk explains why Python is a good choice for research and development. It spans the arch from a conceptual, almost philosophical, understanding of the software needs of reseach and development up to concrete organiziational strategies.","At the semiconductor division of Carl Zeiss it's our mission is to continuously make computer chips faster and more energy efficient. To do so, we go to the very limits of what is possible, both physically and technologically. This is only possible through massive research and development efforts.

In this talk, we tell the story of how Python became a central part in our R&D activities. We explain what worked and what didn't. This leads to a conceptual understanding of how one should regard software in R&D contexts and what is different from classical production software. Based on this we understand how we should structure our software and how we need to set up our organization to support this.",,X8YQJR,Tim Hoffmann
W7WDKW,Getting started with JupyterLab,confirmed,,Tutorial,,2022-06-08T16:26:34.409726+00:00,,"JupyterLab is very widely used in the Python scientific community. Most, if not all, of the other tutorials will use Jupyter as a tool. Therefore, a solid understanding of the basics is very helpful for the rest of the conference as well as for your later daily work.
This tutorial provides an overview of  important basic Jupyter features.","# Outline

## Introduction

* Terminology: JupyterLab, Notebook, IPython (10 min)
* Notebook approach - cells, code, markdown and more (15 min)

## Tools

* Help system and history (10 min)
* Magic functions basics (15 min)

## Development

* Runtime measurements and profiling (20 min)
* Exceptions and debugging (20 min)

The tutorial will be hands on.
While the students will receive a comprehensive PDF with all course content,
I will not distribute pre-filled Notebooks.
Instead, I will start with a blank Notebook for each topic and develop the
content step-by-step.
The participants are encouraged to type along.
My typing speed is usually appropriate and allows participants to follow.
In addition, the supplied PDF contains all needed code and commands to get back
on track, if I should be too fast.
I also explicitly ask for feedback if I am too fast or things are unclear.
I encourage questions at any time.
In fact, questions and my answers are often an important part of my teaching,
making the learning experience much more lively and typically more useful.

### Software Requirements

You need to have Python and JupyterLab installed. I will use Python 3.10. Older versions such as 3.8. or 3.9 should work too. If you use Anaconda, you should be all set. Otherwise, if you use conda install with `conda install -c conda-forge jupyterlab` (or use `mamba` instead of `conda`); if you use `pip` install with `pip install jupyterlab`.",,9KSJ3K,Mike Müller
WBLY9B,pyLife – a python package for mechanical lifetime assessment,confirmed,,Poster,,2022-06-02T08:00:47.013370+00:00,,"pyLife is a Python package covering state of the art algorithms of mechanical lifetime assessment and material fatigue. In this talk we will see a very quick glance of mechanical lifetime estimation and how we can combine classical methods from mechanical engineering with methods from data science. We will see how pyLife's modules can be used to build versatile solutions for the engineer's desktop as well as server based solutions for manufacturing and quality assurance with a high degree of automation. As pyLife is an Open Source project, everyone is welcome to collaborate. We are curious if we can establish a developer community in the realm of mechanical engineering. We are aiming especially towards university teachers using pyLife for teaching and research purposes.","Progress of recent decades in data science and machine learning but also more general in computing power and data availability have a big impact also on classical engineering disciplines like mechanical engineering. Engineers have always gone beyond text book calculation flows in order to design their products as good and effective as possible. This means that nowadays programming skills are one essential part of an engineer's skill set.

The classical software commercially available, however does not help the engineer in this. They are most often proprietary closed software packages that implement only workflows foreseen by the software manufacturer. Even those that come with a plugin architecture hardly support the engineer with an accessible library that implements basic domain specific functionality.

Into this gap we aim to step in with pyLife, an Open Source Python package, dealing with material fatigue and mechanical lifetime estimation. Originating from a purely in house development we decided to continue the development in public, sharing the efforts with others. Our aim is a Python package that compiles state of the art calculation methods and algorithms of the realm of material fatigue and mechanical lifetime estimation, so that engineers can build their custom workflows on top of it.

As pyLife builds on the well known numpy/scipy/pandas stack it is easy to incorporate modern mathematics like Bayesian statistics using packages like GPy as well as database connections and all kinds of front end solutions.

pyLife can be an interesting project not only for engineers but also for scientists, who develop new calculation methods or material fatigue model as well as for university teachers and students for practical application of text book algorithms like for example rain flow counting. We are highly welcome collaboration with universities to integrate newly developed algorithms into pyLife.",,PJ3VMA,Johannes Mueller
WNEGE7,Introduction to geospatial data analysis with GeoPandas,confirmed,,Tutorial,,2022-08-03T18:19:55.118122+00:00,,"This tutorial is an introduction to geospatial data analysis, with a focus on tabular vector data using GeoPandas. It will show how GeoPandas and related libraries can improve your workflow (importing GIS data, visualizing, joining and preparing for analysis, exploring spatial relationships, …).","This tutorial is an introduction to geospatial data analysis in Python, with a focus on tabular vector data using GeoPandas. The content focuses on introducing the participants to the different libraries to work with geospatial data and will cover munging geo-data and exploring relations over space. This includes importing data in different formats (e.g. shapefile, GeoJSON), visualizing, combining and tidying them up for analysis, and will use libraries such as pandas, geopandas, shapely, pyproj, matplotlib, folium, …

Please find the material and installation instructions at https://github.com/jorisvandenbossche/geopandas-tutorial",,7VUXWM,Joris Van den Bossche
X8DUTF,Decision making under uncertainty,confirmed,,Talk (long),,2022-05-23T20:10:31.110025+00:00,,"Python is the most popular programming language in the data space and is one of the major driver of many advancements in machine learning. However, it's much less know that the Python library Pyomo is a great tool for solving mathematical optimization problems common in operations research. 

In this talk I'm demonstrating how Pyomo can be used to find optimal decisions when data is uncertain and how to combine data driven forecasts with optimal decision making.","Mathematical optimization is widely used to solve challenging decision problem. 

Stochastic programming is a subfield of mathematical optimimization that involves uncertainty. In a stochastic programm some or all problem parameter are uncertain but follow a known probability distribution, whereas in determinstic optimization all problem parameters are assumed to be known exactly.
The goal in stochastic programming is to find a policy that is feasible for all possible data instances and maximizes the expectation of some function of the decisions and the random variables.
Because many real-world decisions involve uncertainty, stochastic programming has found applications in a broad range of areas ranging from finance to transportation to energy optimization.

The Python library Pyomo is a great tool to solve mathematical optimization problems as it supports a wide range of problem types in mathematical optimization.

In this talk we will see how to use Pyomo to build and solve decision models when data is assumed to be known exactly.
We see different ways to include incertainty in an optimization model and how this can be implemented using the Pyomo.
Moreover we see how we can combine data driven forecasts with optimal decision making.",,LNHS3G,Christian Barz
XAGVJZ,Scientific Python in the browser with Pyodide,confirmed,,Talk (long),,2022-06-05T10:58:36.403048+00:00,,"In this talk, we will look at the growing Python in the browser ecosystem, with a focus on the Pyodide project. We will discuss the remaining challenges as well as new possibilities it offers for scientific computing, education, and research.","Python can run in diverse environments, including most recently in the browser.                                                         

In this talk, we will look at the growing Python in the browser ecosystem, with a focus on the Pyodide project. We will discuss the remaining challenges as well as new possibilities it offers for scientific computing, education, and research.
                                                                                                                                        
Pyodide is a Python distribution for the browser and Node.js. It includes a build of CPython 3.9 for WebAssembly/Emscripten, a number of common Python packages (including the core scientific stack) as well as the ability to install pure Python packages from PyPI. Pyodide comes with a robust Javascript ⟺ Python foreign function interface so that you can mix these two languages in your code with minimal friction.",,TAQFCQ,Roman Yurchak
XM3Q9S,Memory maps to accelerate machine learning training,confirmed,,Talk,,2022-06-01T18:48:15.254091+00:00,,"Memory-mapped files are an underused tool in machine learning projects, which offer very fast I/O operations, making them suitable for storing datasets during training that don't fit into memory.
In this talk, we will discuss the benefits of using memory maps, their downsides, and how to address them.","When working on a machine learning project, one of the most time-consuming parts is the model's training.

But a big part of the model's training is usually filled with filesystem I/O, which is very slow, especially in the context of computer vision.

In this talk, we will focus on using memory maps for storing the datasets during training - which allows you to significantly reduce the training time of your model.

We will also compare using memory maps to other ways to store the dataset during training, such as: in-memory datasets, one image per file, hdf5 file, etc. and will describe the strong and weak sides of the different approaches. Colab notebooks will be provided, and practical examples on significant performance improvements of popular online tutorials will be shown.

We will also show how to address common shortcomings and painpoints of using memory maps in machine learning projects.",,88H9KZ,Hristo Vrigazov
XSNHMB,Image processing with scikit-image,confirmed,,Tutorial,,2022-06-06T19:44:27.557927+00:00,,"Image data are used in many scientific fields such as astronomy, life sciences or material sciences. This tutorial will walk you through image processing with the scikit-image library, which is the numpy-native image processing library of the scientific python ecosystem.

The first hour of the tutorial will be accessible to beginners in image processing (some experience with numpy array is a pre-requisite), and will focus on some basic concepts of digital image manipulation and processing (filters, segmentation, measures). In the last half hour, we will focus on more advanced aspects and in particular Emma will speak about performance and acceleration of image processing.","Image data are used in many scientific fields such as astronomy, life sciences or material sciences. This tutorial will walk you through image processing with the scikit-image library, which is the numpy-native image processing library of the scientific python ecosystem.

The goal of the tutorial is to give confidence to beginners in image processing to get started processing their images with scikit-image, to understand some basic concepts of image processing and to understand how to find help and documentation to go further after the tutorial. For more advanced users, a last part will focus on performance aspects.

The first hour of the tutorial will be accessible to beginners in image processing. Some experience with manipulating numpy arrays is a pre-requisite. We will first explain how to manipulate image pixels as elements of numpy arrays, and how to make basic transformation of images through numpy arrays. Then we will move to some basic concepts of digital image processing:
- image histogram and contrast
- image filtering: transformations of an image resulting in a new image of similar size (for example, thresholding, edge enhancement, etc.)
- image segmentation: partitioning an image into several regions (objects)
- measures on binary images
This part will be hands-on with several exercises, and we will show how to use the scikit-image documentation to find relevant information.

In the last half hour, Emma will focus on more advanced aspects and in particular will speak about performance and acceleration of image processing.",,"UUPVUN, K8U8GS","Emmanuelle Gouillart, Lars Grüter"
ZPLK87,"napari: a multi-dimensional image visualization, annotation, and analysis platform in Python",confirmed,,Poster,,2022-05-31T15:35:22.604444+00:00,,"Napari is an interactive, GPU-accelerated, nD image viewer written in python. It displays images in a 2D or 3D canvas, then provides sliders for any additional dimensions in a dataset. It can also overlay associated data such as segmentations, points, polygons, surfaces, vectors, and tracks. Finally, napari is well-integrated with the scientific python ecosystem: NumPy arrays are the primary data structure used for visualization, and other standard arrays (such as Zarr or Dask arrays) are also supported. This makes it easy to insert interactive visualization, curation, and annotation steps into any workflow using standard SciPy libraries such as NumPy, SciPy, dask, and scikit-image. In this talk, I will introduce napari and demonstrate how napari can be used for interactive image analysis.","[napari](https://napari.org/) is an interactive, GPU-accelerated, nD image viewer written in python. With napari, one can view images, annotate their data, and develop custom interactive workflows. napari is built upon the scientific python ecosystem including vispy for rendering and numpy and scipy for numerical calculations. This integration makes napari well-suited for viewing, curating, annotating, and analyzing scientific image data. Images can be analyzed interactively when napari is used in an interactive python environment such as jupyter notebook. Additionally, napari can be extended for particular applications using the plugin interface. Plugins have been developed by the napari community for applications include deep learning tasks such as image segmentation and GPU-accelerated image processing. Current plugins can be viewed at the [napari-hub](https://www.napari-hub.org/), which is the napari plugin index.

In this talk, I will introduce napari and demonstrate how napari can be used for interactive image analysis. In particular, we will discuss:
* the basics of using napari
* what types of data can be viewed with napari
* how to interactively analyze your image data with napari
* how napari can be extended via the plugin interface",,9NJ3PQ,Kevin Yamauchi
ZQJPNY,Introduction to SciPy,confirmed,,Tutorial,,2022-07-13T16:40:47.180038+00:00,,"This tutorial will provide an introduction SciPy intended for beginners.

SciPy is a collection of mathematical algorithms and convenience functions built on the NumPy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.","This tutorial will provide an introduction SciPy intended for beginners.

You are encouraged to type along with me. To do so, please bring your laptop with a Firefox 90+ or Chromium 89+ installed. We will work through this repository: https://github.com/Vincent-Maladiere/scipy-demo.

SciPy is a collection of mathematical algorithms and convenience functions built on the NumPy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.

The tutorial will introduce notably the following topics:

- routines for linear algebra
- optimization routines",,QCVBZD,Vincent Maladiere
