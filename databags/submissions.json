{"count":39,"next":null,"previous":null,"results":[{"code":"7BXY7G","title":"Breaking the Constraints of Linear Notebook Environments","speakers":["AHZXBV"],"submission_type":5566,"track":5542,"tags":[1413],"state":"confirmed","abstract":"The talk will explore the limitations of current interactive notebook paradigms and introduce [ANONYMIZED TOOL], an experimental alternative to Jupyter that reimagines interactive programming for scientific computing. The talk will explore the design philosophy, technical implementation, and potential impact on scientific computing workflows. [TOOL] is an open source and available at: github.com/[ANONYMIZED].","description":"The talk will explore the limitations of current interactive notebook paradigms and introduce [ANONYMIZED TOOL], an experimental alternative to Jupyter that reimagines interactive programming for scientific computing. The talk will explore the design philosophy, technical implementation, and potential impact on scientific computing workflows.\r\n\r\nUnlike linear notebooks, [TOOL] decouples code execution from outputs. This approach preserves execution history, allows multiple kernel instances, and provides a cleaner, more manageable computational environment. Users can also organize code hierarchically, enabling: batch subtree evaluation, hierarchically scoped memory spaces that prevent variable leakage, and more intuitive code structuring for complex scientific workflows. [TOOL] offers visibility into computational states, including detailed variable inspection and chronological output tracking.\r\n\r\n[TOOL] also provides Git-friendly notebook formats that minimize collaboration friction. Rust-based backend ensures performance and reliability and clean Python kernel implementation without loading Python modules and starting new Python threads.\r\n\r\n[TOOL] is an open source and available at: github.com/[ANONYMIZED].","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223042],"answers":[381117,381113,381114,381120]},{"code":"7XUYKH","title":"Python Profiling and Optimisation—A Training Course for Researchers","speakers":["CSFTCQ"],"submission_type":5566,"track":5541,"tags":[1409,1413],"state":"confirmed","abstract":"Most researchers writing software are not classically trained programmers. Instead, they learn Python organically, often developing unpythonic habits that negatively impact their software‘s performance.\r\n\r\nIn this talk, we present a new course on Python profiling and optimisation. We give an overview of the course contents, report on feedback from researchers at multiple universities who attended early versions of the course, and discuss our plans for developing the course further. Finally, we share how you can run the course at your own institution and contribute to it via the Software Carpentry Incubator program.","description":"Most researchers writing software are not classically trained programmers. Instead, they learn Python organically, often developing unpythonic habits that negatively impact their software‘s performance. As research software engineers working with researchers from different areas, we decided to develop training on Python profiling and optimisation to share best practices.\r\n\r\nIn the first part of that course, we introduce learners to different profiling approaches such as function-level profiling (using `cProfile` and `snakeviz`) or line-level profiling (using `line_profiler`) to identify which parts of a program run slowest. In the second part, we introduce a wide range of optimisations to speed up those sections of the program. Optimisations described in the course include effective usage of language features and the standard library, choosing suitable data structures, using NumPy and other scientific Python packages, and more. Using these optimisations in real-life workloads, we have observed significant performance improvements, sometimes by more than an order of magnitude.\r\n\r\nWe ran early versions of the course at multiple universities for audiences consisting of students, researchers and research software engineers, using feedback we have received from learners to guide further development of the course.\r\nAll course materials are available online via the [Carpentries Incubator](https://carpentries-incubator.org). We welcome contributions to the course materials and encourage attendees to run the course at their own institutions.","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223050],"answers":[381857,381853,381854,381860]},{"code":"8KK8UC","title":"Skrub: machine learning for dataframes","speakers":["KMDJAL","ZHUVWQ","E3S7GV"],"submission_type":5561,"track":5542,"tags":[],"state":"confirmed","abstract":"Machine-learning algorithms expect a numeric array with one row per observation. Typically, creating this table requires \"wrangling\" with Pandas or Polars (aggregations, selections, joins, ...), and to extract numeric features from structured data types such as datetimes. These transformations must be applied consistently when making predictions for unseen inputs, and choices must be informed by performance measured on a validation dataset, while preventing data leakage. This preprocessing is the most difficult and time-consuming part of many data-science projects.\r\n\r\nSkrub bridges the gap between complex tabular data stored in Pandas or Polars dataframes, and machine-learning algorithms implemented by scikit-learn estimators. It provides scikit-learn transformers to extract features from datetimes, (fuzzy) categories and text, and to perform data-wrangling such as joins and aggregations in a learning pipeline. Its pre-built, flexible learners offer very robust performance on many tabular datasets without manual tweaking. It can create complex pipelines that handle multiple tables, while easily describing and searching rich hyperparameter spaces. As interactivity and visualization are essential for preprocessing, Skrub also provides an interactive report to explore a dataframe, and its pipelines can be built incrementally while inspecting intermediate results.\r\n\r\nWe will give an overview of Skrub and demonstrate its features on realistic and challenging tabular learning scenarios","description":"In the tutorial we will teach how to use skrub to easily tackle datasets that would be challenging to analyze using only scikit-learn.\r\n\r\nFirst, we will consider a dataset containing a single table, with information about company employees such as hiring date and role description. While its structure is relatively simple, this dataset would be challenging without skrub due to the richness of the data types it contains, including dates, categories and text. We will show how skrub can create an interactive report to see a sample, data distribution plots, summary statistics and measurements of association between the different columns. We will then show that with one line of code we can already get a very good generalization performance, thanks to skrub's pre-built learner that makes the appropriate modelling choices for the different kinds of columns found in the data. Next, we will dive into the internals of that generic pipeline to show the different encoders it uses for dates, high- and low-cardinality categories, and which can be useful on their own.\r\n\r\nIn a second part, we will consider a dataset which could not be processed in a scikit-learn pipeline. The task is to detect fraud in e-commerce transactions and there are two tables with a one-to-many relationship: a table of orders and a table of products, where each order can contain multiple products. Even though it is a simple dataset, it would be difficult to handle correctly without skrub due to the need to vectorize the rich, heterogeneous data in the products table before it can be aggregated and joined to the orders (and thus the prediction targets). An upcoming addition to skrub (that will be merged in the next few weeks) allows us to easily build a pipeline that handles multiple tables and contains arbitrary dataframe transformations. Moreover, it allows tuning any of the choices we make with scikit-learn's grid or randomized search, without the need to manually construct a complex hyperparameter grid. We will explain the concepts behind these tools and illustrate them by building an effective learner for this dataset. We will also discuss the modelling choices involved and explore the available options through hyperparameter search.\r\n\r\nFinally, we will conclude with a brief summary of ongoing development and future enhancements to skrub.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1222985],"answers":[382195,382191,382192,382198,382194]},{"code":"8U3PB3","title":"How To Accelerate Molecular Insights - Efficient Distance Calculations In Python","speakers":["BZSA9C"],"submission_type":5566,"track":5567,"tags":[],"state":"confirmed","abstract":"In the rapidly evolving field of chemo- and bioinformatics, the efficient computation of molecular distances plays a crucial role in applications such as drug discovery, molecular clustering, and structure-activity relationship modeling. The ability to accurately and efficiently measure molecular similarity is essential for tasks ranging from virtual screening to predictive modeling. As molecular datasets continue to grow in size and complexity, scalable and computationally efficient distance metrics become increasingly necessary to facilitate large-scale analysis.\r\n\r\nIn this work, we explore how Python’s numerical computing capabilities can be leveraged to implement a diverse range of molecular distance metrics. We focus on optimizing computations for vectorized molecular representations, ensuring that performance remains competitive with highly optimized C++-based solutions. By utilizing efficient numerical libraries, we demonstrate that Python can achieve substantial execution speed while maintaining the flexibility and ease of implementation that make it a preferred choice for many researchers.\r\n\r\nBeyond implementation, we conduct a comprehensive performance evaluation by comparing our Python-based methods against state-of-the-art libraries written in C++. Our benchmarking includes assessments of computational efficiency, memory usage, and scalability on large molecular datasets. The results illustrate that, with appropriate optimizations, Python-based approaches can serve as","description":"The ability to efficiently compute distances between vectorized molecular representations is a backbone of both cheminformatics and bioinformatics. Molecular distance metrics serve as the foundation for a wide range of applications across these fields, with one of their most critical roles being in clustering tasks. Clustering is an essential method in molecular analysis, enabling researchers to identify structural similarities, predict biological activity, and facilitate virtual screening in drug discovery. As the volume of available molecular data continues to grow exponentially, the demand for scalable and computationally efficient methods to process and analyze these vast datasets has never been greater.\r\n\r\nTraditional approaches to molecular similarity computations often rely on highly optimized, low-level implementations written in languages such as  C++ to achieve maximum performance. These solutions leverage hardware-efficient operations and fine-tuned memory management to deliver exceptional computational speeds. However, despite their efficiency, such implementations can pose significant challenges for researchers who lack a strong background in computer science or software development. The complexity of writing and maintaining low-level code can create barriers to entry, making it difficult for scientists to experiment with or customize molecular analysis workflows.\r\n\r\nIn contrast, Python has emerged as a dominant force in the landscape of scientific computing, offering an extensive ecosystem of numerical and data-processing libraries, such as NumPy, SciPy, and scikit-learn. The language’s simplicity, readability, and rich functionality make it an attractive alternative for researchers seeking to implement computational methods without delving into the intricacies of low-level programming. Despite historical concerns about Python’s execution speed compared to compiled languages, recent advancements in just-in-time (JIT) compilation, vectorized operations, and parallel computing have significantly narrowed the performance gap.\r\n\r\nIn this work, we explore how Python’s modern computational capabilities can be harnessed to efficiently compute molecular distances while maintaining accessibility and usability. We focus on vectorized molecular representations, such as binary and count fingerprints, and incorporate sparse matrix representations to handle large molecular datasets efficiently. Sparse matrices enable us to store and process only the non-zero elements in molecular representations, dramatically reducing memory consumption and improving computation times for large-scale analyses. By leveraging bulk calculations and optimized numerical routines, we demonstrate that Python-based implementations can achieve near-C++ performance. Through careful optimization strategies, including the use of NumPy’s broadcasting, we show that  Python can handle the challenges of large molecular datasets effectively, maintaining a balance between performance and accessibility.\r\n\r\nBeyond implementation, we conduct a benchmarking study to evaluate the performance of our optimized Python-based methods. We compare them against state-of-the-art C++ libraries specifically designed for molecular similarity computations, assessing key factors such as computational efficiency, memory consumption, and scalability when applied to large molecular datasets. Our results indicate that, with appropriate optimizations, Python-based approaches can serve as practical alternatives, achieving a balance between performance, usability, and accessibility. We highlight the trade-offs involved, demonstrating how Python’s versatility enables efficient molecular distance computations without sacrificing interpretability or ease of integration within broader data analysis pipelines.\r\n\r\nBy showcasing the feasibility of high-performance molecular similarity computations in Python, our work lowers the barrier to entry for researchers and practitioners who may not have extensive experience with lower-level programming languages. This contribution enhances the accessibility of advanced molecular informatics tools, fostering broader adoption and enabling a wider range of scientists to leverage these computational techniques in their research. Ultimately, this work paves the way for more inclusive and reproducible computational chemistry and bioinformatics, empowering researchers across disciplines to engage with large-scale molecular data analysis using modern, user-friendly methodologies.","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223057],"answers":[382316,382317,382322]},{"code":"9F88K8","title":"Python for subsea engineering: A case study on seabed object detection using AI/ML","speakers":["HYA9JW"],"submission_type":5567,"track":5568,"tags":[],"state":"confirmed","abstract":"This talk explores the application of deep learning in automating object detection using high-resolution seabed images. I will discuss the challenges of working with seabed datasets, strategies for training AI models with limited labelled data, and key considerations when choosing a deep learning framework for geospatial analysis. Using offshore wind farm site assessments as a case study, I will provide practical insights on image pre-processing, model selection, and workflow integration to enhance efficiency in marine geospatial data analysis.","description":"Recent advancements in artificial intelligence are transforming seabed imaging and geospatial data analysis, enabling more efficient and accurate engineering hazard assessments. Traditionally, seabed object detection relied on manual interpretation of sonar and bathymetric datasets, a time-intensive and subjective process. By leveraging Convolutional Neural Networks (CNNs), we can now automate feature extraction, reducing processing time while improving consistency and scalability in offshore wind farm installations, oil and gas site assessments, and carbon sequestration projects.\r\n\r\nObjectives:\r\nThis talk will outline our experience in developing an AI-assisted seabed object detection workflow, addressing the unique challenges of underwater imagery. Unlike terrestrial datasets, subsea images suffer from noise, inconsistent lighting, and varying sensor resolutions, making deep learning adaptation non-trivial. I will share insights on choosing an appropriate deep learning framework,  based on usability, flexibility, and performance. Pre-processing underwater imagery is a critical step, requiring techniques to mitigate noise, distortion, and lighting variations in seabed survey data.\r\n\r\nTalk Structure:\r\n1. Introduction: Importance of seabed object detection and AI's role in offshore wind, marine, and geospatial analysis (3 Minutes)\r\n2. Data Acquisition & Preprocessing: Handling high-resolution seabed survey data and potential applications across other remote sensing using Python libraries (NumPy, Rasterio, GDAL, OpenCV, Scikit-Image) (5 Minutes)\r\n3. Deep Learning Approach: Model selection, training process, and feature extraction using TensorFlow, PyTorch, Scikit-learn, and XGBoost (6 Minutes)\r\n4. Validation & Accuracy Assessment: Comparing AI predictions with expert-labelled datasets (4 Minutes)\r\n5. Results & Visualization: Mapping classified seabed features with Geopandas and Matplotlib (5 Minutes)\r\n6. Applications & Future Scope: Expanding AI use in geospatial workflows, including aerial imaging, LiDAR, sonar analysis, and real-time object detection in underwater robotics (2 Minutes)","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223019],"answers":[381796,381797,381802]},{"code":"9LAVJW","title":"Understanding Dispatching Approaches in the Scientific Python Ecosystem","speakers":["QKVYNA","RMNGAJ"],"submission_type":5567,"track":5542,"tags":[1465],"state":"confirmed","abstract":"In recent years, many specialised libraries have emerged, implementing optimised subsets of algorithms from larger Scientific Python libraries-- supporting GPUs for acceleration, parallel processing, or distributed computing, or written in a lower-level programming language like Rust or C. These implementations offer significant performance improvements—but integrating them smoothly into existing workflows can be challenging. This talk explores different dispatching approaches that enable seamless integration of these faster implementations without breaking APIs or requiring users to switch libraries. We'll focus on the following two approaches:\r\n\r\n- **Backend library-based dispatching** : allowing existing library function calls to be routed to a faster backend implementation present in a separate backend library written for GPUs or in a different language, etc. , as adopted by projects like NetworkX and scikit-image.\r\n\r\n- **Array API standardization and adoption** : more specific to dispatching in array libraries. Based on the type of array that is passed into a numpy function, the call is dispatched to the appropriate array library such as Tensorflow, PyTorch, Dask, JAX, CuPy, Xarray, etc. This allows for the array consuming libraries like SciPy and Sklearn to be used in workflows that are using these other array libraries.\r\n\r\nThen we will go over how these approaches are different from each other and when to use which approach based on different use cases and requirements.","description":"# Description\r\n\r\nIn the first few minutes we will go over what dispatching is, why it's needed, what kind of projects may benefit from it, and the kinds of dispatching i.e the user decides which backend to use explicitly or the library implicitly dispatch to the \"right\" implementation for the user or implicitly dispatch based on the input type(s).\r\n\r\n## Array API standard and NumPy predecessors (10-12 mins)\r\n\r\nIn this section we will start with NumPy's old dispatching, briefly explaining how it works, and also showing what it can do, such as allowing to use a NumPy function with cupy.\r\n\r\nWe will then continue with the Array API which has momentum as it is being used by libraries such as SciPy and sklearn and with existing support for numpy, pytorch, JAX, CuPy, Xarray, etc.\r\nWe will highlight it's use by briefly show-casing the speed-up when a user switches e.g. from NumPy to pytorch. \r\n\r\nThese examples will help us expose key difference in type dispatching approaches both for it's users and for the implementation.\r\nA main difference being that the Array API is library orientated while the NumPy dispatching was user-orientated.\r\n\r\n## Backend-library based dispatching (10-12 mins)\r\n\r\nWe will then continue with the more general backend-library based dispatching approach that is implemented in libraries like NetworkX and scikit-image.\r\n\r\nThis approach is more general and opens up additional possibilities as it is based on Python [`entry_point`](https://packaging.python.org/en/latest/specifications/entry-points), which are generally used to extend the functionality of a package. We will understand this `entry-point` based dispatching through a quick demo.\r\n\r\nThen we will see a demo of how NetworkX speeds-up with [nx-parallel](https://github.com/networkx/nx-parallel) and [nx-cugraph](https://github.com/rapidsai/nx-cugraph) backends, and the different user APIs in NetworkX to dispatch a call: based on backend-specific graph's type, `backend=` kwarg, environment variables, global configurations, context manager.\r\n\r\nThen we will go over some of the learnings gathered while integrating this mechanism in [scikit-image](https://github.com/scikit-image/scikit-image) and challenges faced with dispatching arrays inputs.\r\n\r\n## Summary and Comparison\r\n\r\nWe will end with a summary of how dispatching approaches differ and when to use which ones, both as a developer and a user of a project. In this summary we wish to expand beyond the previous examples and also mention for example [DataFrame API standards](https://data-apis.org/dataframe-api/draft/) and [narwhals](https://github.com/narwhals-dev/narwhals). \r\n\r\n\r\n## Intended audience:\r\n\r\n- Contributors/Maintainers of Python libraries who are interested in providing faster algorithms without changing much of their codebase and user-API\r\n- People who work with large graph, image and array datasets\r\n- Anyone interested in API dispatching or any of the above stuff :)\r\n\r\nSome basic knowledge of Python is expected; should know what objects and classes are.\r\n\r\n\r\nThank you :)","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[9170,9175,9176,9177,9178,9179,9180,9181,9182,9183,9184,9185],"slots":[1223049],"answers":[381881,381882,381887]},{"code":"ASR3XL","title":"Predictive Modeling with Imbalanced Datasets Using Scikit-learn","speakers":["KMDJAL","NEUMLP"],"submission_type":5561,"track":5545,"tags":[1413,1636],"state":"confirmed","abstract":"Real-world applications use machine learning to aid decision-making and planning. Data scientists employ probabilistic models to connect input data with outcome predictions that guide operational decisions. A common challenge is working with \"imbalanced\" datasets, where the outcome of interest occurs rarely compared to total observations. Examples include disease detection in medical screening, fraud identification in transactions, and discovery of rare physical phenomena like the Higgs boson.\r\n\r\nThis tutorial examines methodological considerations for handling imbalanced datasets. We focus on resampling techniques that adjust the ratio between positive and negative outcomes. The tutorial explores: (i) how imbalanced data affects probability outcomes and classifier calibration; (ii) resampling's impact on model overfitting/underfitting and its connection to regularization; and (iii) the tradeoffs between computational and statistical performance when implementing resampling strategies.\r\n\r\nHands-on programmatic notebooks provide practical insights into these concepts.","description":"Real-world applications utilize machine learning (or artificial intelligence) to assist in decision-making or planning. In this context, probabilistic models represent the standard approach for data scientists to link input data with probabilistic predictions for possible outcomes, which serve as the foundation for subsequent operational decisions or actions. An additional challenge encountered in real-world applications is that the outcome of interest to be predicted often occurs rarely compared to the total number of observations. This scenario is commonly referred to as an \"imbalanced\" dataset. Several examples illustrate such applications: (i) medical screening where detecting a specific disease represents a rare outcome compared to the general population, (ii) fraud detection where fraudulent events constitute a small fraction of total transactions, and (iii) detection of physical phenomena such as the Higgs boson where observations confirming its existence represent rare events compared to all observations.\r\n\r\nThis tutorial addresses this particular problem and examines specific methodological considerations when approaching imbalanced datasets. One important consideration relates to resampling and its various effects: with numerous negative outcomes compared to positive outcomes, existing literature supports reducing the ratio between these event types. With these techniques in mind, we examine and study the following aspects: (i) the impact of imbalanced datasets on probability outcomes and classifier calibration; (ii) the effect of resampling on model overfitting and underfitting and its relationship to model regularization; and (iii) the impact of resampling on computational performance versus statistical performance.\r\n\r\nThis tutorial provides a hands-on approach through programmatic notebooks to offer insights into each of these concepts.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1222998],"answers":[382684,382680,382681,382687,382683]},{"code":"BTXXNU","title":"Women in HPC – Breaking Barriers and Shaping the Future","speakers":["XBBD7M"],"submission_type":5563,"track":5541,"tags":[],"state":"confirmed","abstract":"Why aren’t there more women in the High-Performance Computing (HPC) community? This simple question led to the creation of the international organisation Women in High Performance Computing (WHPC). The members of this network are committed to greater equality, diversity and integration in the HPC community. The initiative is active at major HPC conferences, offers workshops and mentoring programmes, and aims to raise awareness in the HPC community with the slogan “Diversity creates a stronger community”.\r\nThree years ago a group at Jülich Computing Centre decided that it is time to establish a local group of WHPC – Jülich Women in HPC (JuWinHPC) – to strengthen the community of women in HPC at Forschungszentrum Jülich and to promote diversity. This talk presents the activities of JuWinHPC, from casual lunch meetings to the organisation of conference sessions, and summarises experiences gained and lessons learned striving to establish a local network of women in HPC and to increase diversity, inclusion and female visibility within the community.","description":"Why aren’t there more women in the High-Performance Computing (HPC) community? This simple question led to the creation of the international organisation Women in High Performance Computing (WHPC). The members of this network are committed to greater equality, diversity and integration in the HPC community. The initiative is active at major HPC conferences, offers workshops and mentoring programmes, and aims to raise awareness in the HPC community with the slogan “Diversity creates a stronger community”.\r\nThree years ago a group at Jülich Computing Centre decided that it is time to establish a local group of WHPC – Jülich Women in HPC (JuWinHPC) – to strengthen the community of women in HPC at Forschungszentrum Jülich and to promote diversity. This talk presents the activities of JuWinHPC, from casual lunch meetings to the organisation of conference sessions, and summarises experiences gained and lessons learned striving to establish a local network of women in HPC and to increase diversity, inclusion and female visibility within the community.","duration":60,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223013],"answers":[409900,409898,409899,409903]},{"code":"CBPMEK","title":"GPU Python for the Real World: Practical GPU-Accelerated Python with RAPIDS","speakers":["EE7H7J"],"submission_type":5561,"track":5542,"tags":[],"state":"confirmed","abstract":"NVIDIA GPUs offer unmatched speed and efficiency for data processing and model training, significantly reducing the time and cost associated with these tasks. Using GPUs is even more tempting when you use zero-code-change plugins and libraries. You can use PyData libraries including pandas, polars and networkx without needing to rewrite your code to get the benefits of GPU acceleration. We can also mix in GPU native libraries like Numba, CuPy and pytorch to accelerate our workflows from end-to-end.\r\n\r\nHowever, integrating GPUs into our workflow can be a new challenge where we need to learn about installation, dependency management, and deployment in the Python ecosystem. When writing code, we also need to monitor performance, leverage hardware effectively, and debug when things go wrong\r\n\r\nThis is where RAPIDS and its tooling ecosystem comes to the rescue. RAPIDS, is a collection of open source software libraries to execute end-to-end data pipelines on NVIDIA GPUs using familiar PyData APIs.","description":"In this tutorial we will cover:\r\n- Introduction to cuDF, cuML and more that showcases a simple example of data processing and model training on GPUs.\r\n- Answers to questions like: “Where do I get a GPU?”, “How do I run a container on a VM with a GPU?”, “How do I install GPU packages into an existing environment?”, as well as follow along examples to get a GPU up and running.\r\n- Troubleshooting and monitoring:  Examples of performance analysis, diagnostics, and debugging.\r\n\r\nThis is a hands-on tutorial, with multiple examples to get familiarized with the RAPIDS ecosystem. Participants should ideally have some experience using Python, pandas and sci-kit learn. We'll use cloud-based VMs, so familiarity with the cloud and resource creation is helpful but not required. No prior GPU knowledge is needed.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223005],"answers":[382351,382347,382348,382354,382350]},{"code":"CHDNML","title":"How to become a software detective and perform security research","speakers":["WQZFDC"],"submission_type":5561,"track":5541,"tags":[1413,1637],"state":"confirmed","abstract":"Security research is crucial amid the rapid evolution of cybercrime, the prevalence of nation-state attacks, and over 40k CVEs reported last year. With plenty of learning resources online, it’s challenging to begin your own research. This talk explores fundamental approaches and techniques to discover existing vulnerabilities in software, focusing on practical aspects and essential tools to perform black box and white box analysis, use static analysis tools to understand application structure, and dynamic tools to analyze its behaviour. Additionally, we will exercise static analysis on a vulnerable Python application to apply new knowledge. The goal is to understand how to perform a security research.","description":"Participants of this tutorial will gain a solid foundation in software analysis, with a strong emphasis on security. We will explore the significance of security research in software development and consider various resources and tools to discover existing and new vulnerabilities - including static and dynamic analysis, signature matching, automated scanning and fuzzing. \r\n\r\nTo illustrate these concepts, we’ll perform static analysis with CodeQL, Bandit and Nuclei on a vulnerable Python library as a case study. Additionally, we’ll understand different approaches and techniques to security-oriented analysis. Participants will gain essential knowledge to identify vulnerabilities, find potential targets for analysis, and apply research methodology. \r\n\r\nThis tutorial will cover\r\n* Relevance of security research\r\n* Manual/dynamic software analysis - approaches, tools, techniques\r\n* Automated software analysis - SAST, DAST, other tools\r\n* Outline research methodologies and resources\r\n* How to perform security research and update your knowledge\r\n* Practical walkthrough of vulnerable software to test acquired skills\r\n\r\nKey takeaways \r\n* Basic concepts related to vulnerability research\r\n* Software analysis fundamentals \r\n* Security analysis tools\r\n\r\n(edited)","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223006],"answers":[380746,380747,380752,380749]},{"code":"CTCYRB","title":"Efficient and accurate models for peptide function prediction","speakers":["EVNBNX"],"submission_type":5567,"track":5567,"tags":[1465],"state":"confirmed","abstract":"Peptides are small proteins, regularing many important biological processes. They have significant therapeutic potential, thanks to their properties, e.g. microbial, antiviral, or anticancer. \r\nIn particular, they offer a promising alternative to traditional antibiotics, addressing the growing crisis of drug resistance.\r\nAccurately predicting peptide properties is essential for drug discovery, and recent research has explored deep learning approaches such as graph neural networks, protein language models, and multimodal ensembles.\r\nHowever, these methods are often overly complex and lack scalability. They are also brittle and their performance breaks down on new datasets or tasks.\r\nWe propose to use molecular fingerprints for this task. They are established feature extraction algorithms from chemoinformatics, primarily applied on small molecules. \r\nWe show that they obtain state-of-the-art results on peptide function prediction and can efficiently vectorize larger biomolecules.\r\nThis approach is simple, fast, and accurate. We comprehensively measure its robustness on 6 benchmarks and 126 datasets. This unlocks a novel venue in chemoinformatics-based approaches for peptide-based drug design.","description":"Peptides, as small proteins, play crucial roles in biological processes and offer immense therapeutic potential in areas such as antimicrobial resistance, cancer treatment, and antiviral therapies. While deep learning methods like graph neural networks (GNNs) and protein language models (PLMs) have been widely explored for peptide function prediction, they often face scalability challenges and require significant computational resources.\r\n\r\nWe present methods and results from our paper (https://arxiv.org/abs/2501.17901), introducing an alternative approach that leverages molecular fingerprints—well-established chemoinformatics techniques primarily used with smaller molecules—to predict peptide properties efficiently and accurately. Our research demonstrates that count-based variants of hashed molecular fingerprints, when paired with tree-based classifiers like LightGBM, outperform deep learning methods. We validate our approach across six benchmarks and 126 datasets, achieving state-of-the-art results in peptide function prediction. Our findings challenge the assumed necessity of long-range dependencies in peptides, showing that short-range molecular substructures capture information sufficient for accurate function prediction.\r\n\r\nAdditionally, we will present performance optimizations that enhance computational efficiency, including parallel implementation and sparse representations. Our work is encapsulated in an open-source Python library, scikit-fingerprints, providing a practical tool for researchers in machine learning and computational chemistry.\r\n\r\nThis presentation will offer insights into the broader applications of peptide-based drug discovery and highlight the importance of using molecular fingerprints in chemoinformatics with scalable machine learning frameworks. Attendees will gain an understanding of current chemoinformatics research on peptides and familiarize with graph vectorization methods. They will see how combining domain-specific feature extraction with tree ensembles can yield superior results compared to complex models, all at a fraction of the computational cost.","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223038],"answers":[380630,380629,380625,380626,380633]},{"code":"D78ZJP","title":"Using Cython and C++ kernels to speed up Python libraries","speakers":["98HJFS","37CLM9"],"submission_type":5561,"track":5542,"tags":[],"state":"confirmed","abstract":"Many high-performance Python frameworks, such as NumPy, scikit-learn, and PyTorch, rely on primitives implemented in Cython and C++ to achieve optimal performance.  \r\n\r\nIn this tutorial, we will explore how to implement custom kernels in Cython and C++ and integrate them into Python projects. Using Linear Regression model trained with Normal Equations method as an example, we will demonstrate how to accelerate numerical computations by writing efficient kernels in Cython and C++. We will also discuss when implementing custom kernels is beneficial and when existing optimized libraries offer the best performance.  \r\n\r\nThis tutorial is aimed at intermediate Python users. At the same time C++ knowledge is advantageous but not mandatory.","description":"The main goal of this tutorial is to give a broader audience an overview of how high-performance libraries in Python are developed. Attendees will work with ready-made code templates and fill in missing parts to learn by doing.\r\n\r\n### **Session Outline**  \r\n- **(5 min) Motivation:** Problems with performance in Python and what to do with them  \r\n- **(10 min) Understanding Linear Regression & Normal Equations:** A brief introduction to the mathematical background, deriving the formulas that would be used for training  \r\n- **(10 min) Implementing a Python Baseline:** Writing a reference implementation on Python  \r\n- **(20 min) Optimizing with Cython:** Implementing a fast Cython-based kernel  \r\n- **(20 min) Example of pybind11 use:** Example of pybind11 usage for integrating kernels implemented in C++ \r\n- **(15 min) Performance Comparison:** Benchmarking Python vs. Cython vs. C++ & pybind11 implementations\r\n- **(10 min) Q&A and Discussion**  \r\n\r\n\r\nBy the end of this session, attendees will:\r\n- Understand how Python libraries achieve high performance using Cython and C++  \r\n- Learn how to write and integrate custom Cython or C++ kernels  \r\n- Know when to optimize Python code and when to rely on existing libraries  \r\n\r\nWe also hope that this tutorial will inspire more people to contribute to open-source high-performance libraries.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223001],"answers":[382413,382414,382419,382416]},{"code":"EAAE9B","title":"Enhancing SymPy Algorithms with MatchPy's Efficient Pattern Matching","speakers":["NYVKBG"],"submission_type":5566,"track":5542,"tags":[1465],"state":"confirmed","abstract":"This presentation explores an experimental integration between SymPy (symbolic mathematics) and MatchPy (associative-commutative pattern matching), both open-source Python libraries. By leveraging MatchPy's efficient pattern matching, which allows for multiple matches with a single expression tree visit, the combined system enhances SymPy's ability to solve equations, compute derivatives and integrals, and handle differential equations. An experimental RUBI formula integration algorithm implementation demonstrates the practical benefits.","description":"SymPy and MatchPy are powerful, open-source Python libraries for symbolic mathematics and pattern matching, respectively.\r\n\r\nWhile SymPy excels in symbolic manipulation, its built-in pattern matching can be limiting for complex tasks. MatchPy, designed for associative-commutative (AC) pattern matching, offers a unique advantage: its data structures enable the simultaneous matching of multiple patterns against a symbolic expression with only a single tree traversal, significantly boosting efficiency.\r\n\r\nThis talk presents an experimental module that integrates these libraries, demonstrating how MatchPy's capabilities can enhance SymPy's algorithms. We will explore practical applications, including improved equation solving, derivative and integral computations, and differential equation solutions. Notably, we will showcase an experimental implementation of the RUBI integration algorithm, illustrating how MatchPy simplifies the application of intricate integration rules.\r\n\r\nThis work highlights the benefits of combining these tools, fostering the development of more robust and efficient symbolic computation workflows.\r\n\r\nSymPy website:\r\nhttps://sympy.org/\r\n\r\nMatchPy documentation:\r\nhttps://matchpy.readthedocs.io/en/latest\r\n\r\nSymPy paper:\r\nhttps://peerj.com/articles/cs-103/\r\n\r\nMatchPy paper:\r\nhttps://arxiv.org/abs/1710.06915","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223030],"answers":[382272,382268,382269,382275]},{"code":"ELXEN7","title":"Python Framework for Large-Scale Radar Data Generation and Visualization","speakers":["CLXJRK"],"submission_type":5566,"track":5543,"tags":[],"state":"confirmed","abstract":"The application of machine learning in automotive radar systems presents severe challenges, particularly due to the limited availability of raw radar data tailored to specific radar configurations and annotated datasets. In this presentation, we introduce a novel Python-based framework designed to address these challenges by enabling large-scale radar data generation and visualization.\r\n\r\nOur framework leverages existing radar detections from production systems, accumulating radar detections over multiple cycles to enhance resolution and minimize feature fluctuation. These accumulated features, referred to as pseudo scatter points, are treated as scatter centers to generate raw spectra for virtual radar systems with arbitrary antenna arrangements. This approach incorporates clutter in the simulation to achieve more representative results.\r\n\r\nKey features of our framework include:\r\n\r\n- GPU Acceleration: Utilizes GPU acceleration to handle the computational demands of large-scale radar data generation efficiently.\r\n- Inbuilt Visualizer: Provides an inbuilt visualizer for radar data, facilitating real-time analysis and debugging.\r\n- Specialized Data class: Implements a specialized data class to streamline the process of radar data generation and processing.","description":"The advancement of machine learning in automotive radar systems is significantly impeded by the limited availability of annotated raw radar data, specifically tailored to distinct radar sensor configurations. To overcome these limitations, we present a robust Python-based computational framework specifically designed to facilitate efficient large-scale radar data generation and comprehensive visualization.\r\n\r\nOur framework leverages radar point cloud data, which is stored in the HDF5 format and collected directly from automotive radar sensors deployed in real-world environments. The point cloud data undergoes sophisticated preprocessing where radar detections are accumulated over multiple measurement cycles. This accumulation process is carefully corrected for both ego-vehicle velocity and the velocities of detected objects, ensuring spatial and temporal consistency. Through this preprocessing, our method reliably identifies dominant scatter centers within the radar's operational environment, creating high-quality datasets, termed \"pseudo scatter points,\" that effectively represent persistent environmental features.\r\n\r\nThese pseudo scatter points serve as a foundation for generating synthetic raw radar data suitable for virtual radar systems with arbitrary antenna arrangements and configurations within the widely used 77 GHz automotive radar band. The framework implements advanced radar signal processing algorithms including multi-dimensional Fast Fourier Transforms (FFTs), digital beamforming, Constant False Alarm Rate (CFAR) detection algorithms in range-Doppler domains, specialized peak-finding algorithms in azimuth and elevation dimensions, and interpolation methods to enhance angular resolution. Additionally, synthetic clutter generation methods are integrated to further improve the realism of the generated radar datasets.\r\n\r\nA key strength of our framework is its adaptability and flexibility, achieved through the utilization of a specialized and extensible Python data class structure. This structure simplifies the step-by-step management of radar data processing pipelines, facilitating customization, scalability, and straightforward integration of additional processing steps or alternative radar parameter configurations.\r\n\r\nTo address the computational demands of generating radar data at scale, the framework employs GPU acceleration via CuPy, a CUDA-enabled implementation of NumPy, enabling significant performance improvements. Currently, it achieves processing rates of approximately 10,000 radar frames per day, depending on radar resolution, thus offering practical scalability for both research and commercial development scenarios.\r\n\r\nVisualization capabilities are seamlessly integrated, leveraging PyQt5 combined with OpenGL to deliver a powerful inbuilt visualization tool. This visualizer provides detailed, interactive representations of radar cubes in real time, significantly aiding debugging, data exploration, and deeper insight into radar signal characteristics and potential anomalies.\r\n\r\nPractical applications of our framework include the generation of extensive radar datasets to support the training and validation of advanced deep neural networks for automotive radar perception tasks, as well as early-stage validation and benchmarking against reference radar sensor systems. Validation experiments, conducted through direct comparison with evaluation radar sensor data collected under identical conditions, have demonstrated the framework's capability to reliably reproduce realistic radar spectra, highlighting its value as a cost-effective alternative or complementary solution to expensive field-testing campaigns or high-fidelity ray tracing methods.\r\n\r\nFrom a scientific research perspective, our framework directly addresses a critical bottleneck in deep learning-based radar applications—the scalability and availability of data needed to train sophisticated machine learning models. Its flexibility, ease of use, and high computational efficiency make it an attractive tool for researchers and developers aiming to push the boundaries of radar-based perception in automotive safety systems and advanced driver assistance systems (ADAS).\r\n\r\nWe welcome feedback and collaboration from the scientific community at EuroSciPy 2025 to further refine and extend the capabilities of our framework, exploring its broader applicability in scientific computing and automotive research domains.","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223021],"answers":[382064,382065,382070]},{"code":"FPYRSB","title":"Compress, Compute, and Conquer: Python-Blosc2 for Efficient Data Analysis","speakers":["BMFCA3","WFFNSW"],"submission_type":5561,"track":5542,"tags":[],"state":"confirmed","abstract":"Have you ever experienced the frustration of not being able to analyze a dataset because it's too large to fit in memory? Or perhaps you've encountered the memory wall, where computation is hindered by slow memory access? In this hands-on tutorial, you'll learn how to overcome these common challenges using Python-Blosc2.\r\n\r\nPython-Blosc2 (https://www.blosc.org/python-blosc2/) is a high-performance, multi-threaded, multi-codec array container, with an integrated compute engine that allows you to compress *and compute* on large datasets efficiently. You'll gain practical experience with Python-Blosc2's latest features, including its seamless integration with NumPy and the broader Python data ecosystem. Through guided exercises, you'll discover how to tackle data challenges that exceed your available RAM while maintaining high performance.\r\n\r\nBy the end of this tutorial, you'll be able to implement Python-Blosc2 in your own workflows, dramatically increasing your ability to process large datasets on standard hardware. Participants should have basic familiarity with NumPy and Python data processing.","description":"Blosc and Blosc2 are well-known and widely used libraries for high-performance data compression. They are particularly effective for compressing large datasets, such as those encountered in data science and high-performance computing. The Blosc library has been around for more than a decade, and its design has always prioritized speed, with a focus on achieving compression and decompression speeds that are close to or even exceed memory bandwidth limits.\r\n\r\nWith the introduction of a new compute engine in Python-Blosc2 3.0, the guiding principle has evolved to \"Compress Better, Compute Bigger.\" This enhancement enables computations on datasets that are over 100 times larger than the available RAM, all while maintaining high performance.\r\n\r\nIn this hands-on tutorial, participants will learn how to effectively use Python-Blosc2 through practical exercises divided into four sections:\r\n\r\n**Section 1: Getting Started with Python-Blosc2 (20 minutes)**\r\n- Introduction to compression concepts and Blosc2 architecture\r\n- Setting up your environment and installing Python-Blosc2\r\n- Basic compression/decompression operations with various codecs\r\n- Hands-on: Creating your first compressed arrays\r\n\r\n**Section 2: Integration with NumPy and the Python Data Ecosystem (20 minutes)**\r\n- Working with NDArrays and SChunks containers\r\n- Converting between NumPy arrays and Blosc2 containers\r\n- Optimizing memory usage with minimal performance impact\r\n- Hands-on: Processing real-world datasets with NumPy and Blosc2\r\n\r\n**Section 3: The Compute Engine (30 minutes)**\r\n- Understanding the Blosc2 compute engine architecture\r\n- Using JIT compilation for expressions with NumPy functions\r\n- Processing data larger than available RAM\r\n- Hands-on: Implementing calculations on out-of-memory datasets\r\n\r\n**Section 4: Advanced Usage and Real-world Applications (20 minutes)**\r\n- Performance optimization techniques\r\n- Integration with existing data pipelines\r\n- Scaling strategies for different hardware configurations\r\n- Hands-on: Solving a complex data analysis challenge\r\n\r\nThroughout the tutorial, we'll work with practical examples demonstrating how to analyze datasets that exceed available RAM without specialized hardware. By the end, participants will have hands-on experience implementing Python-Blosc2 in data workflows and will understand how to compress data while maintaining computational efficiency.\r\n\r\nThis tutorial will help you expand your capabilities for scientific computing and data analysis while reducing memory footprint and improving processing speed. Attendees should bring laptops with Python installed; pre-tutorial setup instructions will be provided.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1222993],"answers":[382006,382007,382012,382009]},{"code":"GQBY9V","title":"Voilà Meta-Dashboards for Streamlined Geospatial Data Visualization","speakers":["SA7KZA"],"submission_type":5567,"track":5542,"tags":[1413],"state":"confirmed","abstract":"The Joint Research Centre has cultivated significant expertise in developing Voilà dashboards using Python for scientific data visualization, resulting in the design and deployment of many real-world web applications. This presentation will highlight our commitment to building a robust Voilà developer community through dedicated training and resource libraries. We will introduce and demonstrate our innovative meta-dashboards, which streamline the creation of complex, multi-page dashboards by automating framework and code generation. A live demonstration will illustrate the ease of building a geospatial application using this tool. We will conclude with a showcase of recently developed Voilà dashboards in areas such as agricultural/biodiversity surveys and air quality monitoring, demonstrating their effectiveness in data exploration and validation.","description":"This presentation delves into the Joint Research Centre's (JRC) extensive experience in leveraging Voilà dashboards, widget libraries, and geospatial web applications for scientific data visualization. Over the past several years, the JRC has cultivated a robust ecosystem around Voilà and Python, resulting in the design and deployment of numerous real-world web applications across diverse scientific domains, including agricultural/biodiversity surveys and air quality monitoring. This effort has been accompanied by a strong commitment to building a thriving developer community, supported by dedicated training and comprehensive resource libraries, like for example the vois Python library, available as open source at https://code.europa.eu/jrc-bdap/vois.\r\n\r\nA key focus of this talk is the introduction and demonstration of our innovative meta-dashboards. These are Voilà-powered applications designed to streamline the creation of complex, multi-page dashboards by automating framework and code generation. We will showcase how meta-dashboards, such as the tool accessible at https://vois.azurewebsites.net/, can significantly accelerate the development of complex geospatial web applications. Through a live demonstration, attendees will witness the ease with which a fully functional geospatial application can be built using these visual tools.\r\n\r\nThe presentation will demonstrate Voilà's efficacy in developing real-world web applications that transform complex datasets into actionable insights. Participants will gain a clear understanding of how Voilà is used at the JRC to create powerful data visualization tools, supporting comprehensive data assessment and informed decision-making across various scientific fields. The applications showcased will highlight the ability of Voilà to handle large and complex datasets, and to create interactive environments that are easy to use and understand (see https://www.daigio.it/EuroSciPy2025/ for some screenshots of the Voilà dashboards that will be presented).","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223047],"answers":[391055,391054,391050,391051,391058]},{"code":"GYBLDL","title":"Beyond the Basics: Data Visualization in Python","speakers":["9WJJPL"],"submission_type":5561,"track":5542,"tags":[1413,1637],"state":"confirmed","abstract":"The human brain excels at finding patterns in visual representations, which is why data visualizations are essential to any analysis. Done right, they bridge the gap between those analyzing the data and those consuming the analysis. However, learning to create impactful, aesthetically-pleasing visualizations can often be challenging. This session will equip you with the skills to make customized visualizations for your data using Python.\r\n\r\nWhile there are many plotting libraries to choose from, the prolific Matplotlib library is always a great place to start. Since various Python data science libraries utilize Matplotlib under the hood, familiarity with Matplotlib itself gives you the flexibility to fine tune the resulting visualizations (e.g., add annotations, animate, etc.). This session will also introduce interactive visualizations using HoloViz, which provides a higher-level plotting API capable of using Matplotlib and Bokeh (a Python library for generating interactive, JavaScript-powered visualizations) under the hood.","description":"#### Section 1: Getting Started With Matplotlib\r\n\r\nWhile there are many plotting libraries to choose from, the prolific Matplotlib library is always a great place to start. Since various Python data science libraries utilize Matplotlib under the hood, familiarity with Matplotlib itself gives you the flexibility to fine tune the resulting visualizations (e.g., add annotations, animate, etc.). Moving beyond the default options, we will explore how to customize various aspects of our visualizations. Afterward, you will be able to generate plots using the Matplotlib API directly, as well as customize the plots that other libraries create for you.\r\n\r\n#### Section 2: Moving Beyond Static Visualizations\r\n\r\nWhile static visualizations are limited in how much information they can show, animations make it possible for our visualizations to tell a story through movement of the plot components (e.g., bars, points, lines), which can encode another dimension of the data. In this section, we will focus on creating animated visualizations before moving on to create interactive visualizations in the next section.\r\n\r\n#### Section 3: Building Interactive Visualizations for Data Exploration\r\n\r\nWhen exploring our data, interactive visualizations can provide the most value. Without having to create multiple iterations of the same plot, we can use mouse actions (e.g., click, hover, zoom, etc.) to explore different aspects and subsets of the data. In this section, we will learn how to use HoloViz to create interactive visualizations for exploring our data utilizing the Bokeh backend.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":true,"resources":[],"slots":[1222994],"answers":[381373,381369,381370,381376,381372]},{"code":"HUTEDK","title":"Managing Scientific Data and Workflows with DataLad","speakers":["8YYMWZ","CERHSS"],"submission_type":5561,"track":5569,"tags":[1413,1637],"state":"confirmed","abstract":"The flourishing of open science has created an unprecedented opportunity for scientific discovery through the global exchange of data and collaboration between researchers. DataLad (datalad.org) supports this by providing the tools to develop flexible and decentralized collaborative workflows while upholding scientific rigor. It is free and open source data management software, built on top of the version control systems Git and git-annex. Among its major features are version control for files of any size or type, data transport logistics, and digital process provenance capture for reproducible digital transformations. \r\nIn this hands-on workshop, we will start by exploring DataLad’s basic functionality and learn how to run and re-run analyses while versioning and keeping track of your data. Following this, we will explore DataLad’s collaborative features and learn how to install and work with existing datasets and how to share and distribute your work online. After completing this tutorial, you will be equipped to start using DataLad to manage your own research projects and share them with the world.","description":"The tutorial will begin with a short introduction to DataLad, describe typical use-cases and explain how DataLad uses the version control systems git and git-annex to manage, track and transport data. \r\nAfter this short introduction, we will start with the first hands-on block where participants will explore the core concepts of DataLad that enable reproducible research, such as version control and digital provenance. We will learn how to create and configure a DataLad data set (datalad create), how to add and modify data (datalad [status, save, unlock]) and how to see changes in the data set's history (git log). We will see how DataLad can be used to run a Python script while keeping track of the input and output of that script (datalad run). The record of running the script that DataLad produced can then be used to conveniently re-run parts of the analysis pipeline after making changes to the script (datalad rerun). After this hand-on session, participants will understand the basic DataLad functionalities required to manage their data analysis project on their local machines.\r\nNext, we will present how DataLad can be used to install existing datasets and collaborate with others, how it supports the FAIR (Findable, Accessible, Interoperable, and Reusable) principles for data sharing and discover and how it integrates with open-science platforms like the Open Science Framework (OSF).\r\nThis is followed by another hand-on session where participants will learn how to install existing online datasets (datalad [clone, get]) and check the identity and availability of files (git-annex [info, whereis]). We will also explore how DataLad can create and manage siblings of a data set, allowing the user to back up and share their data (datalad [create-sibling-*, push, update]). After this session, participants will have the tools for working with existing data sets provided by open-science platforms and for creating collaborative workflows.\r\n\r\nParticipants are expected to bring their own computer. Before the tutorial, we will provide installation instructions and be available for potential troubleshooting to ensure that every participant is able to follow the exercises. Prior knowledge of Git or Git-annex is not required, but participants who are familiar with these tools may get a deeper understanding of the inner workings of DataLad. Familiarity with a Unix-like terminal is also an advantage.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223009],"answers":[391105,391101,391102,391108,391104]},{"code":"JACK7B","title":"The BrainGlobe initiative - image analysis in a common coordinate space.","speakers":["ZVRV3Y"],"submission_type":5566,"track":5567,"tags":[],"state":"confirmed","abstract":"The BrainGlobe initiative provides open-source tools for analysis and visualisation of brain microscopy imaging data. Neuroanatomy is key to understanding the brain. However, current tools are often specialised for a single model species or image modality and lack sustained support post-publication. BrainGlobe provides a generalised framework for representing multiple anatomical atlases within and across species, allowing our tools to be uniquely interoperable. Registration tools allow the outputs of BrainGlobe packages to be placed within the broader context of a neuroanatomical atlas. This enables unique downstream analyses that would otherwise be extremely time consuming. Our goal is to empower users with easily accessible analysis and visualisation tools that can be ready for use within minutes on a standard laptop.","description":"The [BrainGlobe](https://brainglobe.info) initiative has three main goals: providing specific tools for analysis and visualisation, cultivating core tools to facilitate development of interoperable tools in Python, and fostering a community of neuroscientists and developers to share knowledge, build software, and engage with the scientific and open source community. The development of BrainGlobe builds upon and relies heavily on established packages from the broader scientific Python ecosystem ensuring compatibility and interoperability with other tools. Our initiative addresses the crucial need for interoperability in neuroscience research by offering a comprehensive suite of tools accessible to users across different platforms. With a focus on ease of installation and usability, our goal is to empower researchers to analyse neuroanatomical data efficiently and introduce them to the broader scientific python ecosystem. \r\n\r\nIn this talk I plan to discuss the benefits of working in a common coordinate space for image analysis and visualisation. I will begin by introducing the concept of a BrainGlobe atlas and the associated [brainglobe-atlasapi](https://github.com/brainglobe/brainglobe-atlasapi) package. This standard access point has enabled the emergence of an ecosystem of BrainGlobe tools, developed both internally by the core BrainGlobe team, and externally by outside contributors. Abstracting the concept of an atlas and standardising access allows the downstream tools to be species agnostic, widening the potential user pool. I will then describe [brainreg](https://github.com/brainglobe/brainreg) and [brainglobe-registration](https://github.com/brainglobe/brainglobe-registration), the tools we use to register data into a BrainGlobe atlas. I will provide two examples of how our tools utilise BrainGlobe atlases to provide valuable context, based on the annotations of the atlas. The first example will be [brainmapper](https://github.com/brainglobe/brainglobe-workflows), a pipeline that utilises [brainreg](https://github.com/brainglobe/brainreg) and [cellfinder](https://github.com/brainglobe/cellfinder) to detect cells in large 3D volumes and output counts per anatomical region. The second example involves [brainglobe-segmentation](https://github.com/brainglobe/brainglobe-segmentation) which can be used to segment objects, and transform segmentations into sample or atlas space. Lastly, I will demonstrate visualising registered multi-modal data in 3D using [brainrender](https://github.com/brainglobe/brainrender).\r\n\r\nAt present, the BrainGlobe team maintains 17 packages which have 100+ code contributors.  The [BrainGlobe Atlas API](https://github.com/brainglobe/brainglobe-atlasapi) serves as a standardised framework for working with anatomical reference atlases, facilitating comparison across samples [1]. Using [brainreg](https://github.com/brainglobe/brainreg), 3D whole-brain imaging data can be registered to any BrainGlobe atlas [2]. [Cellfinder](https://github.com/brainglobe/cellfinder) automates cell detection in large 3D images in a computationally efficient manner [3], while common neuroanatomical segmentation issues are tackled with [brainglobe-segmentation](https://github.com/brainglobe/brainglobe-segmentation) [2]. [Brainrender](https://github.com/brainglobe/brainrender) uses [vedo](https://github.com/marcomusy/vedo) to enable visualisation of 3D neuroanatomical data from both public sources and user-generated data [4, 5].\r\n\r\nOur team is continually working on addressing the needs of the community. Currently, efforts are underway to broaden the types of microscopy data registrable into the BrainGlobe ecosystem with [brainglobe-registration](https://github.com/brainglobe/brainglobe-registration), using elastix to register 2D slices, 3D sub-volumes, and whole brain data. Additionally, we are developing [brainglobe-stitch](https://github.com/brainglobe/brainglobe-stitch), a package for fusing large tiled 3D imaging datasets (300+ GB). This package will be available as a napari plugin to allow efficient previewing of the fused dataset. Lastly, we are porting the functionality of brainrender to a napari plugin, [brainrender-napari](https://github.com/brainglobe/brainrender-napari), to provide a cohesive analysis and visualisation environment for all BrainGlobe tools.  \r\n\r\nConcurrently, we are streamlining and enhancing the developer experience. This involves consolidating related repositories within the BrainGlobe codebase and extracting duplicated code to [brainglobe-utils](https://github.com/brainglobe/brainglobe-utils), a shared library. We are also intensifying efforts to improve docstring coverage and providing introductory guides for new developers, along with a development roadmap outlining planned future work.\r\n\r\nCitations:\r\n[1]\tF. Claudi, L. Petrucco, A. Tyson, T. Branco, T. Margrie, and R. Portugues, ‘BrainGlobe Atlas API: a common interface for neuroanatomical atlases’, J. Open Source Softw., vol. 5, no. 54, p. 2668, Oct. 2020, doi: 10.21105/joss.02668.\r\n[2]\tA. L. Tyson et al., ‘Accurate determination of marker location within whole-brain microscopy images’, Sci. Rep., vol. 12, no. 1, p. 867, Dec. 2022, doi: 10.1038/s41598-021-04676-9.\r\n[3]\tA. L. Tyson et al., ‘A deep learning algorithm for 3D cell detection in whole mouse brain image datasets’, PLOS Comput. Biol., vol. 17, no. 5, p. e1009074, May 2021, doi: 10.1371/journal.pcbi.1009074.\r\n[4]\tF. Claudi, A. L. Tyson, L. Petrucco, T. W. Margrie, R. Portugues, and T. Branco, ‘Visualizing anatomically registered data with brainrender’, eLife, vol. 10, p. e65751, Mar. 2021, doi: 10.7554/eLife.65751.\r\n[5]\tM. Musy et al., \"vedo, a python module for scientific analysis and visualization of 3D objects and point clouds\", Zenodo, 2021, doi: 10.5281/zenodo.7019968.","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223059],"answers":[391304,391286,391287,391292]},{"code":"K9L3VH","title":"Units next to your Data: Arrays with Scipp","speakers":["WTK33D"],"submission_type":5567,"track":5542,"tags":[],"state":"confirmed","abstract":"Inspired by xarray, Scipp enriches raw NumPy-like multi-dimensional data arrays by adding named dimensions and associated coordinates. For an even more intuitive and less error-prone user experience, Scipp adds physical units to arrays and their coordinates. There are multiple ways of working with units in the Scientific Python world, and there are even new initiatives like the Units/Quantity API and in this talk we will look at Scipp (which wraps around llnl-units).\r\n\r\nBut units are just one part of working with scientific data. Scipp also has a powerful non-destructive binning method to sort record-based \"tabular\"/\"event\" data into arrays of bins which could be useful if you are dealing with lots of data which needs to analyzed quickly. Scipp can also natively propagate uncertainties through your computations. Stop by this talk if you would like to see how Scipp can power scientific data analysis.","description":"This talk will introduce the Scipp library, originally developed for neutron science experiments, and how it can be useful for record-based \"tabular\"/\"event\" data in general.\r\n\r\nOne of Scipp's key features is the possibility of binning to sort record-based data into arrays of bins. This provides fast and flexible binning, rebinning, and filtering operations, all while preserving the original individual records.\r\n\r\nIf your use case requires one or several of the items on the following list, using Scipp may be worth considering:\r\n- Physical units are stored with each data or coord array and are handled in arithmetic operations.\r\n- Histograms, i.e., bin-edge axes, which are by 1 longer than the data extent.\r\n- Support for non-regular or scattered data and non-destructive binning.\r\n- Support for masks stored with data.\r\n- Propagation of uncertainties.\r\n- Internals written in C++ for better performance (for certain applications), in combination with Python bindings.\r\n\r\nIn the talk we will cover:\r\n- Why units are important? What's the current landscape? (5 mins)\r\n- Labeled dimensions, Units (in Scipp) and data structures in scipp (5 mins)\r\n- Bins, Histograms and Uncertainties in Scipp (10 mins)\r\n- Tips and tricks of multi dimensional data handling (5 mins)\r\n- Buffer and Q/A (5 mins)","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223046],"answers":[391179,391175,391176,391182]},{"code":"KCYYTF","title":"Pyro Meets SBI: Unlocking Hierarchical Bayesian Inference for Complex Simulators","speakers":["YZAGH8"],"submission_type":5566,"track":5542,"tags":[1413,1465],"state":"confirmed","abstract":"This talk introduces a novel approach that bridges Simulation-Based Inference (SBI) and probabilistic programming languages like Pyro to enable simulation-based hierarchical Bayesian inference. SBI is used to perform parameter inference for intractable simulation models, while Pyro facilitates efficient Bayesian inference with complex hierarchical structures. We demonstrate how to integrate SBI-learned likelihoods into Pyro models, allowing for hierarchical Bayesian analysis of simulation-based models. Using the drift-diffusion model from decision-making research as an example, we showcase the potential of this combined approach for tackling real-world problems with complex simulation models and hierarchical data.","description":"Hierarchical Bayesian inference is a powerful framework for analyzing structured data common in complex experimental settings, like multi-subject decision-making research. Probabilistic programming languages (PPLs) such as [Pyro](https://pyro.ai/) provide excellent tools for defining and inferring these hierarchical models, leveraging features like plate notation for concisely representing repeated structures and managing dependencies.\r\nHowever, a significant challenge arises when the underlying scientific model is a complex simulator with an intractable likelihood function, rendering standard PPL-based inference inapplicable. While Simulation-Based Inference (SBI) techniques can handle such simulators by learning likelihood (or posterior) approximations from simulations, they often lack native support for easily specifying and inferring complex hierarchical dependencies.\r\nThis talk introduces a novel approach that bridges the SBI package [sbi](https://sbi.readthedocs.io/en/latest/) and `pyro`, enabling effective simulation-based hierarchical Bayesian inference. We demonstrate how likelihood approximations learned via `sbi` can be seamlessly integrated as custom components within `pyro` models. This synergistic approach combines the strengths of both methodologies: SBI's ability to perform inference on intractable simulators and Pyro's expressive power and efficiency in handling complex hierarchical structures.\r\nWe will illustrate the potential and practicality of this integrated methodology using a key example from cognitive science: fitting a hierarchical drift-diffusion model (DDM) to choice data. The focus will be on how this combined \"Pyro meets SBI\" approach successfully allows for Bayesian inference over the parameters of the hierarchical model, effectively combining the information across multiple simulated subjects while handling the intractable DDM likelihood.\r\nThis integration significantly expands the scope of rigorous Bayesian inference, opening new possibilities for analyzing complex, simulation-based models across various scientific disciplines. We will also briefly highlight how recent developments in the `sbi` package facilitate this powerful workflow, making advanced hierarchical modeling accessible for simulator-based research.","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223020],"answers":[382241,382240,382236,382237,382244]},{"code":"L9JPZE","title":"Accelerate your scientific Python code with Rust","speakers":["PLJAZV"],"submission_type":5567,"track":5542,"tags":[1413],"state":"confirmed","abstract":"Combining Python with compiled languages for speed is far from novel - the scientific Python ecosystem has been doing it for around 25 years! Specifically, Rust has proven to be a particularly solid companion for Python in recent times, thanks in large part to the great tooling available. The impact on scientific Python code can be huge. And yet, the language has a reputation of having a steep learning curve.\r\n\r\nCreating your first Rust extension for Python can be done in 5 minutes thanks to uv and maturin (no exaggeration), but of course that's just the beginning. In this talk you will learn everything else you need to make your numerical code blazing fast with Rust.","description":"The outline will be roughly as follows:\r\n\r\n1. Python is slow, you say? (3 min)\r\n2. Python + Rust = 🤜🤛 (5 min)\r\n3. Gentle introduction to PyO3 (5 min)\r\n4. Passing NumPy arrays in and out with ndarray (5 min)\r\n5. Your first Rust extension (live demo) (5 min)\r\n6. Conclusions (2 min)\r\n\r\nThe demo will showcase some simple numerical algorithms that benefit from having a Rust implementation. Light performance benchmarks will be shown.\r\n\r\nFamiliarity with Python is required. No prior exposure to Rust is needed.","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223018],"answers":[385320,385316,385317,385323]},{"code":"LNU8UV","title":"PyPI in the face: running jokes that PyPI download stats can play on you","speakers":["N7TBEV"],"submission_type":5567,"track":5542,"tags":[1413],"state":"confirmed","abstract":"We all love to tell stories with data and we all love to listen to them. Wouldn't it be great if we could also draw actionable insights from these nice stories?\r\n\r\nAs scikit-learn maintainers, we would love to use PyPI download stats and other proxy metrics (website analytics, github repository statistics, etc ...) to help inform some of our decisions like:\r\n- how do we increase user awareness of best practices (please use Pipeline and cross-validation)?\r\n- how do we advertise our recent improvements (use HistGradientBoosting rather than GradientBoosting, TunedThresholdClassifier, PCA and a few other models can run on GPU) ?\r\n- do users care more about new features from recent releases or consolidation of what already exists?\r\n- how long should we support older versions of Python, numpy or scipy ?\r\n\r\nIn this talk we will highlight a number of lessons learned while trying to understand the complex reality behind these seemingly simple metrics.\r\n\r\nTelling nice stories is not always hard, trying to grasp the reality behind these metrics is often tricky.","description":"We all love to tell stories with data and we all love to listen to them. Wouldn't it be great if we could also draw actionable insights from these nice stories?\r\n\r\nAs scikit-learn maintainers, we would love to use PyPI download stats and other proxy metrics (website analytics, github repository statistics, etc ...) to help inform some of our decisions like:\r\n- how do we increase user awareness of best practices (please use Pipeline and cross-validation)?\r\n- how do we advertise our recent improvements (use HistGradientBoosting rather than GradientBoosting, TunedThresholdClassifier, PCA and a few other models can run on GPU) ?\r\n- do users care more about new features from recent releases or consolidation of what already exists?\r\n- how long should we support older versions of Python, numpy or scipy ?\r\n\r\nIn the context of scikit-learn, we will present the kind of surprises and caveats we discovered when trying to make sense of the PyPI download stats.\r\n\r\nHighlights include:\r\n- the most downloaded scikit-learn release is from 5 years ago, maybe people actually don't care about our latest developments?\r\n- how on earth can a package that errors on install be downloaded 50k a day?\r\n- is there any hope to differentiate \"real users\" vs \"automation users\" (e.g. Continuous Integration)?\r\n\r\nWe will then zoom out a bit and talk about other metrics we looked at, for example scikit-learn.org website analytics, GitHub stars and \"Used by\" stats. After presenting all the inherent biases of these data, we will see present the kind of insights we gained by combining them.\r\n\r\nDuring the presentation, we will also highlight a few tools and websites we used along the journey to make it easier to look at PyPI download stats numbers in more details.\r\n\r\nWe will conclude with some thoughts about how to use this kind of metrics to inform some of our decisions, while at the same time not falling in love too much with the stories we tell with them.","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223028],"answers":[387821,387822,387827]},{"code":"LR7S8P","title":"Recent Developments in Pytensor, the Successor Package to Theano","speakers":["LR7MWP","C9GEJ3"],"submission_type":5567,"track":5542,"tags":[1636],"state":"confirmed","abstract":"We present the latest developments in Pytensor, the successor package to Theano. Pytensor is a package for defining, manipulating, optimizing, and compiling static computational graphs. We especially focus on full graph-to-graph transformations relevant to the goals of a Bayesian/ML workflow. These allow the user to define a single computational graph, which can then be reused in multiple contexts. In the Bayesian workflow, we are able to extract exact expressions for probabilistic inference from a generative sampling model, or automatically marginalize discrete random variables. In a deep-learning workflow, we can automatically remove dropout and normalization layers when compiling a prediction function from a training graph, or replace expensive operations, such as transformers, with specialized forms at compile time. Finally, we show how the same machinery leads naturally to transpilation into compiled languages, via packages like Numba, Jax, and Pytorch","description":"After MILA offically stopped development of Theano in 2018, the PyMC project forked the project and continued developing the package under a series of names: Theano-PyMC, Aesara, and now Pytensor. This choice was motivated by Theano's decision to put static computational graphs directly in front of the user.  This choice turned out to be ideal for Bayesian inference, because it allows for powerful graph-to-graph transformations. For example, a graph that defines a data generating process via draws from random variables can be automatically translated into a backwards process describing the log probability of observed data, conditioned on draws from prior distributions. This is precisely the same logic that underpins reverse-mode automatic differentiation. It turns out this idea of graph transformation is extremely powerful, and development of Pytensor has focused on fully leveraging this power.\r\n\r\nRecently, new features have been added to Pytensor, making it a powerful tool for workflows in statistics, machine learning, and Bayesian modeling. One of the most powerful features of Theano was it's use of graph rewrites to optimize computation beyond what a compiler is willing or able to do. A canonical example is to rewrite the express `log(1 + x)` to `log1p(x)`, a version of the computation that remains numerically stable for small values of `x`. Pytensor has fully embraced the system of graph rewrites, and has expanded it to include many new cases, including: \r\n\r\n- Op fusion: finding sub-graphs that are re-used multiple times on different inputs. These are fused into a single composite Op, compiled once, and re-used.\r\n- Aggressive inplacing: identifying all cases where intermediate computations can performed destructively \"in-place\", alleviated the need for expensive memory allocations\r\n- Pre-gradient stabilization: maintain a library of mathematically \"non-destructive\" rewrites that simplify arbitrary user expressions into simpler forms, leading to cleaner gradient graphs after applying autodiff\r\n- Linear algebra optimization: reason about the types of matrices in a graph, and automatically apply specializations to expensive operations. For example, `det(diag(x))` is rewritten to `prod(diag(x))`, and `inv(kron(a, b))` to `kron(inv(a), inv(b))`.\r\n\r\nThese rewrites are possible because as a user writes Pytensor code, a static graph is constructed. At any time, the user can inspect the graph, or directly intervene on it. These interventions include extracting sub-graphs, replacing or removing inputs or functions, or applying entire graph-to-graph transformations. The majority of our talk will focus on this last operation, which represents one of the most unique and powerful features of Pytensor. This capability was first used in Theano to perform automatic differentiation. Given a forward graph of a scalar loss function, a gradient graph can be created by walking backwards from the loss in topological order, applying the chain rule. Graph-to-graph transformations can go far beyond this application. We present the following examples:\r\n\r\n- *Vectorization*. Pytensor can replace an n-dimensional input with an (n+k) dimensional input, then propagate the consequences through the graph. For example, `dot(matrix, vector)` can be replaced with `dot(tensor3, vector)`, and Pytensor will automatically apply a batched dot product across the left-most dimension of the `tensor3` input. The new output shape will also be propagated to any down-stream computations.\r\n\r\n-  *Automatic probability derivation*. The PyMC package uses to Pytensor to transform a user-defined generative graph, which produces samples of data from priors and inputs, to a graph that gives the probability of observing an output sample, given parameters. In this way, a single model declaration is reused for both inference via Bayes' Law, and for pre- and post-estimation tasks that require forward sampling    \r\n\r\n- *Automatic marginalization*. Similarly, having access to the full graph lets Pytensor reason about non-trivial graph replacements, such as automatic marginalization of random variables. This can remove nuisance parameters from a model and significantly speed up inference algorithms. In addition, because pytensor can graphically verify that batch dimensions remain independent, the algorithm is bounded by the size of the domain of the random variables, rather than by their actual input size. When marginalizing a hidden markov chain, for example, this means we only have to do `states * steps` computations, instead of `states ^ steps` (this is the celebrated \"forward algorithm\"). \r\n\r\nWe also give several examples where Pytensor offers advantages in deep learning contexts, including:\r\n\r\n- *Training to prediction transformation*: A common source of errors when working with deep learning models is forgetting to manually remove training-only Ops like dropout and batch norm. Pytensor can automate these tasks using rewrites, which can be automatically applied by a deep learning API built on top of pytensor\r\n\r\n-*Op Specialization*: Certain extremely expensive layers used in deep learning, including convolutions and transformers, have specialized forms that can be used when the right conditions are met. Examples include FFT-convolution and FlashAttention. Pytensor can recognize Op sequences of the form `matmul-mask-softmax-dropout-matmul` and replace them with a single `FlashAttention` Op. Similarly, convolution Ops can be replaced with FFT convolutions when the kernels are sufficiently large. Both of these optimizations can be done automatically, without users necessarily having to know the intricacies of when and how these specialized versions should be used.\r\n\r\nWe conclude with a short description of planned features for our upcoming Pytensor 3.0 release, and an invitation for interested audience members to try the package and submit issues/PRs.","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223026],"answers":[381474,381473,381469,381470,381477]},{"code":"MDVAHD","title":"ELVA - Local-First Real-Time Collaboration Apps in Your Terminal","speakers":["VEXNNV","NQ9HEX"],"submission_type":5566,"track":5569,"tags":[1409,1413],"state":"confirmed","abstract":"Science evolves and flourishes through close team work and smooth information exchange.\r\nDespite the plethora of digital collaboration platforms a tool that allows for seamless collaboration does not exist, yet.\r\n\r\nWe present `ELVA`, a command-line tool and suite of terminal applications which are able to synchronize arbitrary data structures in real-time without conflicts in a peer-to-peer setup.\r\nFrom a simple text file to an IDE session, a chat, a directory's content ... All of this can be modeled with a combination of *conflict-free replicating data types* (CRDTs) provided by the [`Yrs`](https://github.com/y-crdt/y-crdt) library and its Python bindings in [`pycrdt`](https://github.com/y-crdt/pycrdt).\r\nThereby, merge conflicts as a main pain point of version control systems and file based synchronization services are mitigated or even completely avoided.\r\nIn addition, `ELVA` apps are written to be local-first: they run locally on your machine, also when you are offline, and store your data on your disk.\r\nThe local state is synchronized with remote-peers automatically when you are back online.\r\nA central server is not needed, but it can work as a relay or broker between peers to overcome restrictive firewalls.","description":"### Our Motivation\r\n\r\nAs scientists working with distributed groups all over the world, we have high and numerous expectations for a digital collaboration tool:\r\n\r\n- platform-independent\r\n- secure\r\n- reliable synchronization of any kind of data\r\n- works seamlessly offline\r\n- private, no remote cloud services involved\r\n- no central servers required, peer-to-peer\r\n- access to the local filesystem\r\n- easily adaptable and extensible\r\n- easily accessible\r\n- stable and lightweight\r\n- open-source\r\n\r\nSeveral tools already exist that can fulfill a subset of these requirements, but they come with all sort of restrictions.\r\n\r\nSome examples:\r\n\r\n- Git is great for keeping track of text files, but it requires you to manually save the current state by committing and lacks efficient native support for binary files.\r\n  Also, the user has to manually deal with merge conflicts when multiple people work on the same files.\r\n  While powerful extensions such as Git-LFS and git-annex do a great job with binary data, Git is still unsuitable as general purpose collaboration platform.\r\n- Cloud-based software such as Nextcloud, Seafile or the corresponding commercial alternatives are able to sync any type of data out of the box, but they implement\r\n  versioning only in a rudimentary fashion, if at all.\r\n  Furthermore, a central server is a vital part of the architecture.\r\n  Since their synchronization is file-based, merge conflicts still arise and need to be resolved by hand.\r\n- Browser-based editors like Visual Studio Code, HedgeDoc or Etherpad feature real-time synchronization of content, but cannot sync binary data, and when they are implemented as in-browser apps, they cannot always access the local filesystem.\r\n\r\n`ELVA` aims to close these gaps.\r\n\r\n\r\n### Our Ideas\r\n\r\nIn short, we want to have the comfortable real-time synchronization of in-browser apps but in a software that runs locally for proper access to the filesystem on every operating system.\r\n\r\nConflict-free replicating data types (CRDTs) are the backbone of `ELVA`.\r\nThey provide the fundamental logic for proper integration of shared data fragments.\r\n\r\nThe suite shall be written in Python, as it is relatively easy to learn and widely used in scientific research.\r\nIf users of `ELVA` feel the need for special functionality, they should be able to add it by themselves without much effort.\r\n\r\nAlso, as we intend `ELVA` to be also usable by non-technical people, we need to have a good user interface (UI).\r\nWe decided to settle on text-based terminal apps, which are lightweight and work consistently across platforms, but graphical interfaces are likewise implementable.\r\n\r\nVersioning could still be realized with a Git integration and synchronizing bulk storage is a question of app and protocol design.\r\n\r\n\r\n### Our Prototype\r\n\r\nWe are proud to present our prototype of `ELVA` with a working text editor and a chat app.\r\n\r\nCurrently, we use the [`pycrdt`](https://github.com/y-crdt/pycrdt) package providing Python bindings to the [`Yrs`](https://github.com/y-crdt/y-crdt) library, which holds the Rust implementation of CRDTs originating from the [`Yjs`](https://github.com/yjs/yjs) library.\r\n`Yjs` is used in a variety of popular software: [JupyterLab](https://jupyter.org/), Facebook's [Lexical](https://lexical.dev/) editor, [Nextcloud](https://nextcloud.com/) and [many other projects](https://github.com/yjs/yjs?tab=readme-ov-file#who-is-using-yjs).\r\n\r\nThe text-UI is powered by the [`Textual`](https://github.com/Textualize/textual) framework.\r\nWe intentionally kept the UI minimal to also allow pure keyboard usage.\r\nA builtin REPL widget provides the needed configuration flexibility.\r\n\r\nThe project's design concept stands: We provide apps to be directly used and library code for writing own features and apps.\r\nThe configuration specification as well as the synchronization protocol are also defined.\r\n\r\nWe publish comprehensive documentation under [https://elva.mintgruen.tu-berlin.de](https://elva.mintgruen.tu-berlin.de) alongside a few guides and a blog.\r\nCode management with versioning and changelog has been properly set up for ease in long-term maintenance. \r\n\r\nThe source code of `ELVA` is published under the AGPL-3.0 license on [https://github.com/innocampus/elva](https://github.com/innocampus/elva).\r\n\r\n\r\n### Our Vision\r\n\r\nWe hope to build up a community around this project by showing it to and improving it with other people.\r\nAs a part of that, we plan to introduce `ELVA` to our students in programming classes.\r\n\r\nA dedicated app for bulk synchronization is being developed.\r\nAdditionally, we would like to explore networking without a central server and implement easy to setup peer-to-peer communication protocols to also allow for \r\nspontaneous collaboration in meetings or at conferences.\r\nOther apps and integrations might be written in other languages than Python thanks to `ELVA`'s integrated networking and the `Yrs` bindings available in several other languages.\r\n\r\nAnother important point is integrating Git into `ELVA` for proper versioning and publishing with services such as [GIN](https://gin.g-node.org/).\r\n\r\n\r\n### Contact Information\r\n\r\nJakob Zahn\r\nHead of Software Development\r\nProjekt `ELVA`\r\nPronouns: he/his\r\nAddress: Mr.\r\n\r\nTechnische Universität Berlin\r\ninnoCampus\r\nRoom E116\r\nEinsteinufer 19\r\n10587 Berlin\r\nGermany\r\n\r\nPhone: +49 30 314 77006\r\nMobile: +49 152 06370569\r\nE-Mail: [jakob.zahn@tu-berlin.de](mailto:jakob.zahn@tu-berlin.de)\r\n\r\n[https://elva.mintgruen.tu-berlin.de](https://elva.mintgruen.tu-berlin.de)\r\n[https://www.tu.berlin](https://www.tu.berlin)","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223055],"answers":[390789,390788,390784,390785,390792]},{"code":"MLCQQL","title":"Automated Chess Analysis: Real-Time Move Detection and Game Narration Using Computer Vision and Large Language Models","speakers":["Z9BZMS","3U3KUU","GWQHEL"],"submission_type":5566,"track":5544,"tags":[],"state":"confirmed","abstract":"This talk presents a python-Streamlit application which has been developed based on integration of deep learning based automatic chess move detection and LLM-generated chess game commentary and is designed to be a powerful tool for enhancing chess learning and viewer engagement. Automatic move detection based on a high accuracy computer vision model allows chess players, learners and general viewers to accurately track the games, identify mistakes, and review tactics without the need for manual notation. Beginners gain a clearer understanding of gameplay flow, while enthusiasts can easily annotate and revisit key moments. By combining move detection with real-time, LLM-driven commentary, the system provides context-aware explanations that highlight strategic ideas, tactical patterns, and player intentions. This creates an interactive and educational experience that enriches both learning and viewing.","description":"This talk presents the development of a python application for the detection and interpretation of chess moves from video footage, blending deep learning based computer vision, motion tracking, and LLM based sequence analysis. The system is designed to identify all 12 chess piece types—pawn, rook, knight, bishop, queen, and king in both black and white—on an 8×8 board, track their movements across frames. It then converts these actions into standard algebraic notation (e.g., \"e4\", \"Nf3\", \"Qxd5\"). A key feature of this application is the ability to distinguish between valid moves and incidental adjustments, like nudging a piece. In addition, based on the chess moves an LLM is used to generate an educational commentary on the game which adds an engaging narrative dimension for users, making it suitable for learners and casual viewers alike.\r\n\r\nThe application workflow begins with object detection using a YOLOv8 model trained on a labeled chess dataset, which outputs bounding boxes and class probabilities for each chess piece. The centroids of these detected bounding boxes are then mapped to corresponding chessboard squares (e.g., \"a1\" to \"h8\"). By comparing piece positions across consecutive video frames, the system infers potential moves, which are subsequently validated using the python-chess library to ensure legality—such as preventing illegal pawn movements. Once a move is confirmed, it is passed to OpenAI’s GPT-4, which generates educational and context-aware commentary. This commentary is then converted to audio using Google Text-to-Speech (gTTS), creating an engaging and informative user experience.\r\nFinally this application is packaged within a Streamlit app that provides an interactive platform, allowing users to upload videos, view annotated outputs, and download commentary audio. This pipeline combines YOLOv8’s speed, chess-specific logic, and AI-driven narration into a cohesive system.\r\n\r\nThe computer vision and LLM based workflow successfully automates move detection in chess games by leveraging a YOLOv8s model to process user-submitted videos, accurately generating legal move sequences and producing annotated output videos. Building on this, the Streamlit application seamlessly combines visual move annotations, structured move lists, and GPT-4-generated audio commentary, delivering a rich and interactive user experience. This integrated pipeline highlights the powerful synergy between computer vision and large language models, demonstrating a practical, real-world application where automated visual recognition and natural language generation come together to create dynamic and educational chess commentary. Furthermore, integrating large language models for commentary generation opens new possibilities for smart chess boards, coaching applications, live-streamed matches with narration, and automated game archiving for tournaments and classrooms. Overall, this fusion of computer vision and natural language generation bridges the gap between physical and digital chess, fostering greater inclusivity, deeper engagement, and accelerated learning across the chess community.\r\nAdvancements in this work may include expanding to multi-player support by incorporating hand tracking or multi-camera systems to accurately detect player turns and interactions, as well as enhancing analysis capabilities to assess move quality, providing detailed feedback on mistakes and exceptional plays. \r\n\r\nThis is an open source project and the GitHub repository details and steps for installation/running this application will be shared during the talk.","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223060],"answers":[390926,390927,390932]},{"code":"MU9HAJ","title":"Beyond Likelihoods: Bayesian Parameter Inference for Black-Box Simulators with sbi","speakers":["YZAGH8","ZDDWRW"],"submission_type":5561,"track":5542,"tags":[],"state":"confirmed","abstract":"Do you spend time tuning parameters for complex scientific simulators? Perhaps you use grid search or optimization to match parameters to data. These find a best-fit set, but often don't reveal your confidence or if other parameters also fit. This uncertainty is crucial for reliable conclusions.\r\nThis tutorial introduces Simulation-Based Inference (SBI), a modern technique tackling this challenge. Unlike traditional Bayesian inference methods (like MCMC) that require mathematical likelihood functions, SBI works directly with your simulator's outputs. Using recent advances in probabilistic ML, it estimates the probability distribution of parameter values consistent with your observations, even for complex \"black-box\" simulators. It provides not just a single best guess, but full parameter distributions representing parameter uncertainties and potential interactions.\r\nIn this hands-on tutorial using the `sbi` Python package, you'll learn the practical steps: setting up the problem, running SBI for parameter distributions, and checking result reliability. We will cover different SBI techniques and how to apply them.\r\nIf you are a scientist or engineer using Python for simulations, or just interested in probabilistic inference methods, this session is designed for you. Crucially, no prior Bayesian statistics knowledge is required. You will learn to obtain more reliable and interpretable results by quantifying uncertainty and understanding how parameters interact within your model.","description":"Many scientific and engineering fields rely on complex computer simulations – for example, in particle physics, epidemiology, or computational neuroscience – to understand complex phenomena. A common challenge is finding the right input parameter settings for these simulators so that their output matches real-world observations. Determining these parameters accurately can be difficult, especially when the simulator is intricate or stochastic.\r\n\r\nTraditional methods like grid search or numerical optimization algorithms can find a single 'best' set of parameters. However, they often struggle when there are many parameters, and more importantly, they usually don't quantify the uncertainty associated with the result. Is this the only good parameter set? How much could the parameters change and still produce similar results? Answering these questions is vital for robust scientific understanding.\r\n\r\nSimulation-Based Inference (SBI) is a modern approach, drawing on machine learning, designed specifically for this problem. SBI methods learn a statistical relationship directly from running your simulator multiple times with different inputs. Their key advantage is the ability to estimate the range of parameter values (and their probabilities) that are consistent with your observed data, providing a measure of uncertainty. This works even for complex 'black-box' simulators where the internal equations might be unknown or intractable. For instance, when modeling disease spread like COVID-19, knowing the uncertainty around estimated infection rates is crucial for making informed decisions – SBI provides exactly this, but for any kind of simulator as long as we can simulate enough data.\r\n\r\nThis tutorial provides a comprehensive, practical introduction to SBI using the `sbi` [Python package](https://sbi.readthedocs.io/en/latest/). `sbi` implements state-of-the-art SBI algorithms, often using neural networks, and is actively developed by a large community (it's a NumFOCUS affiliated project with over 70 contributors and yearly collaborative hackathons). We will guide you through the entire practical workflow:\r\n\r\n- A-prior checks: Defining parameter ranges of interest and running initial simulations to check if the model can plausibly generate data similar to observations.\r\n- Choosing and applying SBI methods: Learning about different SBI strategies (like those that directly estimate parameter probabilities, approximate likelihoods, or work with probability ratios) and selecting one suitable for your specific problem.\r\n- A-posteriori checks: Evaluating how accurately the underlying machine learning models have learned to infer parameters.\r\n- Interpretation: understanding and drawing scientific conclusions from SBI results. \r\n\r\nThe tutorial combines accessible explanations of the concepts with hands-on coding exercises using `sbi`, enabling you to apply these techniques to your own research problems.\r\n\r\n**Target Audience:** This tutorial is aimed at individuals comfortable with Python programming who work with computational simulation models in science or engineering and need to estimate parameters from data. Researchers and practitioners looking for practical methods to quantify parameter uncertainty in complex systems will find this useful.\r\n\r\n**Prerequisites:**\r\n- Proficiency in Python programming.\r\n- No prior knowledge of Bayesian statistics is required. \r\n\r\n**Learning Objectives:** Upon completion, participants will be able to:\r\n- Understand the concept of Simulation-Based Inference (SBI) and its advantages for parameter estimation in complex simulators.\r\n- Apply the `sbi` Python package to estimate parameter ranges and uncertainty from simulation outputs.\r\n- Become familiar with common neural network-based SBI techniques and their use cases.\r\n- Evaluate the results of an SBI analysis to ensure reliability.\r\n- Perform the complete SBI workflow using the `sbi` package on their own problems.\r\n\r\nThis tutorial will equip participants with the practical skills to effectively use the sbi package for more reliable parameter estimation and uncertainty quantification in their simulation-based models.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223010],"answers":[382169,382165,382166,382172,382168]},{"code":"NHKMDP","title":"Standardised Quantity/Unit APIs discussion","speakers":["L3QKJA"],"submission_type":5565,"track":5542,"tags":[],"state":"confirmed","abstract":"Work with quantities (values with units) in Python? Come and help brainstorm ideas and voice your opinions for standardised APIs!\r\n\r\nDiscussion session for https://github.com/quantity-dev/metrology-apis and related efforts.","description":"https://github.com/quantity-dev/metrology-apis is a cross-ecosystem effort to standardise APIs for Metrology in Python. The vision is to have libraries like Pint, `astropy.units`, and Unyt all adopt common APIs which consumers can then use in a ‘backend-agnostic’ fashion.\r\n\r\nCome along to learn about the current design prototypes and proposals, and voice your ideas for what these APIs should look like!","duration":45,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223058],"answers":[392394,392395,392398]},{"code":"NKLD7R","title":"Let's rewrite optimagic from scratch in half an hour and see what we can learn","speakers":["DMPC8P"],"submission_type":5567,"track":5542,"tags":[],"state":"confirmed","abstract":"Optimagic provides a unified interface to optimization algorithms from various packages while adding convenience features like optimizer histories, error handling, and flexible parameter formats — all  in a relatively small code base and without modifying the source code of optimizers. In this talk, we'll build a simplified version of optimagic to demonstrate the core architectural principles that make this possible. By exploring these ideas, we'll show how they can be applied beyond optimization to simplify and enhance other scientific Python projects.","description":"Optimagic provides a unified interface to optimizers from SciPy, NlOpt, Pygmo and many other packages. In a relatively small code base, we add many convenience features to the algorithms we wrap.  Collecting and plotting optimizer histories, error handling, and flexible parameter formats are just a few examples. All of this is done without accessing or modifying the source code of the original optimizers, which makes it very simple to add more optimizers to optimagic. \r\n\r\nThis is made possible by a few simple architectural ideas that could also be applied in other scientific packages or research code. In this talk we create a super-simplified educational rewrite of optimagic that illustrates the core ideas and explains how we use them to implement powerful features. We finish by collecting a few examples where the same ideas could be applied to create simple, robust and user-friendly code in other open-source packages or research projects. \r\n\r\nThe talk is meant for users and maintainers of scientific packages alike. No previous experience or interest in numerical optimization is required.","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223027],"answers":[390494,390490,390491,390497]},{"code":"PTLGMW","title":"Use napari for easier interactive extraction of knowledge from images and other spatial data","speakers":["FGEUGZ","Z9XNXN"],"submission_type":5561,"track":5542,"tags":[1636],"state":"confirmed","abstract":"With cameras in everything from microscopes to telescopes to satellites, scientists produce image data in countless formats, shapes, sizes, and dimensions. Python provides a rich ecosystem of libraries to make sense of them. napari is a Python library for interactive multidimensional image visualization, but it does double duty as a standalone application that can be easily extended with GUI tools for analysis, visualization, and annotation. In this tutorial, we'll start with the basics of interacting with the napari interface. Then we will show how to extend napari, from script, with your own functions and widgets. At the end, we will describe how to convert such custom modifications into a plugin that can be easily shared with other people as a Python package.","description":"As we collect more and more images and other spatial information, we need to improve tooling to be able to process all the data that we gather from microscopes, telescopes, satellites, MRI machines and a myriad of other sensors. \r\nFor the phase of data exploration, we need a tool that allows smooth comparison of results from various knowledge extraction methods, in order to choose the best method of processing our data.\r\n\r\nThis tutorial is aimed at people who have some experience in scientific computing with Python. To get the most out of it, you should be familiar with NumPy arrays, Jupyter notebooks, and Python scripts. Ideally, you should have some idea of how images can be represented as arrays of numbers, and the types of analyses that might be performed on these arrays, e.g. filtering and segmentation. You don’t necessarily need to be familiar with how these tools and methods work - it’s enough to know that they are out there! \r\n\r\nThis tutorial will be split into three parts:\r\n\r\nPart 1: Introduction: using napari, exploring the interface, using sample plugins to perform basic data manipulation \r\nIn this section, we will introduce participants to the napari interface and show how to perform basic operations. It will also ensure that participants have properly created the virtual environment. \r\nPart 2: Extending napari with custom functionalities: custom widgets and mouse callbacks, from a Python script or Jupyter notebook. \r\nIn this section, we will teach: How to allow trigger custom functions from napari GUI. How to create a simple widget from a function and how to prepare a more complex widget to fit better into workflow. How to integrate the napari viewer with an interactive session in jupyter. \r\nPart 3: How to convert your custom extension of napari into a plugin, Python package, that can be easily shared with other people. \r\nIn this section, we will show how to pack custom widgets into plugins and share them as Python packages.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1222989],"answers":[382582,382578,382579,382585,382581]},{"code":"Q3FERF","title":"Guardians of Science: A Python Tutorial on a RAG-Powered Compliance Plug-In and Ethical AI tools","speakers":["Z9BZMS","3U3KUU","GWQHEL"],"submission_type":5561,"track":5545,"tags":[],"state":"confirmed","abstract":"As AI adoption accelerates across industries, ensuring ethical integrity and reproducibility has become increasingly critical for enterprises and developers. This tutorial presents a Retrieval-Augmented Generation (RAG)-based compliance plug-in designed to promote responsible AI practices. Through a hands-on session, participants will learn how to integrate external compliance knowledge bases with generative models to automate ethical checks, document decision-making processes, and enhance the reproducibility of AI outputs. The session will cover system architecture, implementation using popular frameworks, and practical use cases, equipping attendees with tools to embed trust and accountability into AI workflows from the outset.\r\nOver the course of 90 minutes, we will introduce the core concepts behind the Python-based plug-in, including RAG architecture and vector-based retrieval techniques. Participants will engage with live demonstrations on querying regulatory standards such as the European Union Artificial Intelligence Act and FAIR (Findable, Accessible, Interoperable, Reusable) principles. The tutorial will also showcase bias auditing and model transparency features, using a healthcare case study to illustrate real-world application and highlight model tracking and reproducibility capabilities.","description":"## Background\r\nThe growing integration of AI in research and everyday applications brings significant challenges related to ethics, compliance, and reproducibility, which are key pillars for maintaining our trust in AI's capabilities.  Compliance challenges in AI applications arise from the need to navigate evolving regulatory standards, ensure fairness and transparency, and maintain data privacy and accountability across diverse and complex systems. Bias related issues , for example bias across demographic groups pose serious risks, while reproducibility issues, such as incomplete experiment documentation or unstable software environments, further erode confidence in AI-driven applications. To tackle these problems, we present a Retrieval-Augmented Generation (RAG) framework that combines vector-based retrieval methods (e.g., FAISS for fast data access) with transformer-based language models (e.g., Mistral) to deliver transparent, standards-compliant recommendations. We also familiarize participants with complementary tools like IBM’s AI Fairness 360. that support bias detection and mitigation through metrics such as disparate impact and reweighing techniques. \r\n\r\n##  Significance\r\nAs AI becomes central to research across disciplines, maintaining ethical standards and reproducibility is essential. Compliance with frameworks like the EU Artificial Intelligence Act and FAIR principles helps prevent biased outcomes and irreproducible results that can damage scientific credibility. In fields such as biomedicine, social sciences, and environmental science, biased or unreliable models can have serious consequences. To address these challenges, this tutorial offers hands-on exercises with a Retrieval-Augmented Generation (RAG) compliance plugin and the IBM AI Fairness 360 toolkit. These tools help researchers detect bias, improve transparency, and ensure reproducibility, supporting the creation of trustworthy, accountable AI systems across diverse domains.\r\n\r\n## Objectives\r\nThe tutorial aims to teach participants about the concepts like ethical compliance, RAG capabilities and tools for estimating and mitigating bias in AI workflows. We will have focused demos using the python RAG plug-in and Fairness 360 that will show how these tools solve compliance issues, using a healthcare case study. Participants will gain skills in ethical AI checks and reproducible methods, preparing them to meet global standards in their research.\r\n\r\n## Tutorial Breakdown\r\n* Introduction (5 min): Describe AI research challenges, like bias and reproducibility, introduce RAG, Fairness360, noting links to standards like the European Union Artificial Intelligence Act and FAIR principles.\r\n\r\n* Setup and Data Preparation (15 min): Look at setting up a Jupyter environment (Python 3.8+, FAISS, Transformers, Fairness360, MLflow, Docker, Plotly, Gradio) and review a synthetic healthcare dataset (10,000+ records, World Health Organization-aligned, with age, gender, ethnicity, and ICD-10 codes) and a pre-built compliance knowledge base (EU AI Act, FAIR, GDPR). \r\n\r\n* Demonstration 1: Retrieval-Augmented Generation (35 min): Explore RAG’s system, showing vector-based retrieval (e.g., FAISS searching EU AI Act rules on data use) and LLM generation (e.g., Mistral creating compliance tips). Demonstrate the plug-in’s process for finding standards and making checks in the healthcare case study, like ensuring GDPR-compliant data use for diagnostic models, with MLflow tracking results. \r\n\r\n* Demonstration 2:  Fairness360 (35 min): Study Fairness360’s bias-checking tools, calculating measures like disparate impact and statistical parity for the healthcare diagnostic model (e.g., comparing accuracy for gender and ethnicity groups). Show how to fix bias (e.g., reweighing to balance groups), using Plotly to display bias measures (e.g., bar charts of disparity) and Gradio to interact with audit reports, focusing on fair results.  \r\n\r\n* Q&A and Wrap-Up (5 min): Talk about uses in biomedicine (e.g., fair diagnostics), social sciences (e.g., unbiased data analysis), and environmental science (e.g., repeatable climate models), and answer questions to clarify concepts.\r\n\r\n## Outcomes\r\nParticipants will deeply understand Retrieval-Augmented Generation and Fairness360, and see how they work in a compliance plug-in. They will build skills in ethical AI checks and reproducible methods, ready to use in biomedicine, social sciences, and environmental science, meeting standards like the European Union Artificial Intelligence Act and FAIR principles.","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1222982],"answers":[390980,390981,390986,390983]},{"code":"QPF9N7","title":"Maintaining People, Not Just Projects: Attracting and Retaining Talent in FOSS","speakers":["EQJWS9"],"submission_type":5568,"track":5542,"tags":[],"state":"confirmed","abstract":"The scientific Python ecosystem powers research, education, and innovation across disciplines from physics and biology to finance and AI. However, the long-term sustainability of this ecosystem depends on the people behind it. While the Scientific Python ecosystem continues to attract new contributors, retaining them remains a challenge with factors such as unclear career pathways, emotional labor, burnout, funding limitations, and project governance can discourage continued involvement. \r\n\r\nThis discussion is about the human side of open source: mentorship, collaboration, recognition, and belonging. The discussion will aim to surface practical ideas we can take back to our respective projects, as well as identify shared challenges we may be able to address together across the ecosystem.","description":"The scientific Python ecosystem powers research, education, and innovation across disciplines from physics and biology to finance and AI. But its long-term sustainability doesn’t just depend on code, it depends on people!\r\n\r\nAs maintainers, we often focus on the technical infrastructure of our projects: CI pipelines, packaging, release cycles. Yet the more enduring challenge is sustaining the human infrastructure that makes open source possible in the first place. While new contributors continue to find their way into the ecosystem, many don’t stay. Unclear career pathways, emotional labor, burnout, limited funding, and governance challenges all play a role in attrition.\r\n\r\nThis round table is an opportunity for maintainers to step back from triage and talk candidly with peers about the human side of open source: mentorship, collaboration, recognition, and belonging. We’ll explore the shared challenges we face in attracting and retaining contributors and the practical strategies that have helped us build healthier, more resilient communities.\r\n\r\nThis session is designed as a participatory, peer-to-peer discussion. However some topics that we could explore are:\r\n\r\n* Why contributors join—and why they leave\r\n* Mentorship models that actually scale\r\n* Recognition and credit (in academia and beyond)\r\n* Balancing paid and volunteer contributions\r\n* Avoiding burnout (yours and others’)","duration":90,"slot_count":1,"content_locale":"en","do_not_record":true,"resources":[],"slots":[1223025],"answers":[396540,396541,396544]},{"code":"SNPKGF","title":"Industrial-Level Documentation for Scientific Projects","speakers":["BKUYND"],"submission_type":5566,"track":5541,"tags":[],"state":"confirmed","abstract":"**Tools Used:**\r\n \r\n- Sphinx\r\n- Sphinx AutoAPI\r\n- Fuse.js\r\n- Towncrier\r\n- Sphinx Design\r\n- Google Search Console\r\n \r\n## Abstract\r\n \r\nMaintaining high-quality documentation in large-scale open-source organizations is a complex and time-consuming challenge, despite significant advancements in documentation tools. This talk presents a collection of strategies, tools, and workflows designed to optimize the documentation process for scientific projects, improving both efficiency and user experience.\r\n \r\nWe will explore techniques for building dynamic, user-friendly documentation using Sphinx, including:\r\n \r\n- Auto-generating API documentation\r\n- Implementing fast, client-side search\r\n- Enhancing SEO for better discoverability\r\n- Streamlining CI/CD workflows for seamless documentation deployment\r\n \r\nAttendees will gain insights into evolving existing documentation themes or creating new ones tailored for scalable, modern scientific projects.","description":"This talk focuses on making documentation effortless by centralizing key updates and methodologies, ensuring consistency across multiple libraries. By automating documentation workflows, organizations can reduce manual effort while maintaining high standards of clarity and usability.\r\n \r\nKey topics covered include:\r\n \r\n- **Automated API Documentation with AutoAPI:** Using Sphinx AutoAPI, Jinja, and Sphinx Design to create dynamic, auto-generated documentation that communicates all methods, classes, attributes, and imports depending on the module. Proper structuring and a solid table of contents ensure clarity and navigability.\r\n \r\n- **Changelog Generation with Towncrier:** Implementing automatic changelog generation that creates a dynamic \"What's New\" section, clearly displaying issues and links in tabs for concise, understandable release notes.\r\n \r\n- **Real-Time search with Fuse.js:** The importance of real-time search results as users type. We use Fuse.js to integrate live search into the documentation with minimal configuration. The search is browser-based, ensuring it works efficiently, even when servers or third-party libraries are not available.\r\n \r\n- **SEO for Better Discoverability:** Understanding the key steps to improve SEO for documentation, ensuring that the content is easily discoverable on Google. We will discuss techniques to enhance the visibility of your docs and increase search clicks using Google Search Console (GSC).\r\n \r\n- **Accessibility and UX Enhancements:** The importance of ensuring documentation is accessible to everyone, regardless of ability. We will explore how to implement dark/light themes, improve visual elements like icons, fonts, and snippets, and ensure the overall UI/UX design promotes inclusivity.\r\n \r\nThese approaches are applicable to organizations of all sizes, offering scalable solutions that not only streamline documentation processes but also enhance the overall user experience.","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223029],"answers":[390838,390839,390844]},{"code":"T88GQE","title":"Machine learning for ecotoxicology and bee pesticide toxicity prediction","speakers":["RCGPHK"],"submission_type":5567,"track":5568,"tags":[1413,1465],"state":"confirmed","abstract":"Machine learning (ML) is widely applied in medicinal chemistry and pharmaceutical industry. Chemoinformatics and molecular ML have been used for decades for safer, faster drug design. However, the important area of agrochemistry has been relatively neglected. New regulations, with strong focus on ecotoxicology, necessitate creation of novel, safer pesticides.\r\n\r\nIn this talk, I will describe how and why we can apply ML in predictive ecotoxicology, and how those models can be applied in agrochemistry. In particular, I will present ApisTox, a novel dataset about pesticide bee toxicity, how we can construct such datasets from publicly available data sources, and what are the challenges.\r\n\r\nThen, we will cover predictive ML applications in ecotoxicology, and how to apply data science tools for agrochemical data. Examples include molecular fingerprints, graph kernels, and graph neural networks. We will also discuss quantitative measures for describing differences between medicinal chemistry and agrochemistry, and how it impacts practical results.","description":"Agrochemistry, in contrast to medicinal chemistry, is a relatively unexplored area in terms of rational drug design and molecular ML. Data science techniques and predictive ML models, exemplified by ADMET QSAR models, have long been used in pharmaceutical industry. Pesticides are the largest, and most economically important, group of agrochemicals. They need to pass multiple regulatory requirements in order to be used, showing safety not only to humans (toxicology), but also to a variety of wildlife organisms, such as honey bees, earthworms, birds, and fish (ecotoxicology). This is in many ways much more challenging, due to a wide variety of properties that need to be analyzed and predicted. At the same time, we actually require strong toxicity from pesticides, but highly selective, killing preferably only target organisms, e.g. weeds in case of herbicides.\r\n\r\nRecently created ApisTox (https://doi.org/10.1038/s41597-024-04232-w) is the largest dataset in the literature concerning toxicity of pesticides to honey bees (Apis mellifera). It allows broad analyses of agrochemicals and building ML models for predicting toxicity of pesticides to honey bees. This required creating a complex data processing workflow, which utilized freely available data sources, like ECOTOX database. In this talk, we will go over tools and techniques used, so that attendees will understand challenges related to such tasks, and how to create other similar datasets for practical usage.\r\n\r\nApisTox paper was followed up by additional molecular datasets' analyzes and building ML models (currently under review). In this talk, we will also explore initial results of pesticide toxicity classification and how we can approach building molecular ML models for agrochemistry, e.g. molecular fingerprints, graph kernels, and graph neural networks. Results are highly distinct from those on molecular chemistry datasets, indicating a lot of unexplored potential.","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223016],"answers":[379650,379649,379645,379646,379653]},{"code":"TAXVPC","title":"Sensor data processing on microcontrollers with MicroPython","speakers":["CVFFNV"],"submission_type":5567,"track":5542,"tags":[1413],"state":"confirmed","abstract":"Being able to sense physical phenomena is critical to many areas of science;\r\nfrom detecting particles in physics, to measuring pollution in public health, to monitoring bio-diversity in ecology. Over the last decades, the capabilities and costs of sensor system has become much better,\r\ndriven by improvements in microprocessors, MEMS sensor technology, and low-energy wireless communication. Thanks to this, Wireless Sensor Networks and \"Internet of Things\" (IoT) sensor systems are becoming common.\r\n\r\nTypically sensor nodes use microcontroller-based hardware, and the firmware developed primarily using C (or C++). However, it is now becoming feasible to write microcontroller firmware using Python.\r\nThis is thanks to the MicroPython project, combined with affordable and powerful hardware from the last couple of years. Using the familiar and high-level Python programming language makes the process of creating sensor nodes more accessible to an engineer or scientist.\r\n\r\nIn this talk, we will discuss developing microcontroller-based sensors using MicroPython. This includes a brief introduction to MicroPython, how to do efficient data processing, and share our experience applying this to process accelerometer and microphone data, using both Digital Signal Processing and Machine Learning techniques.","description":"Here is an overview of the topics you can learn about in this presentation.\r\n\r\n#### Sensor nodes and Wireless Sensor Networks\r\n\r\nA sensor node is a combined hardware + software system that can sense things in the physical world.\r\nIt uses sensor elements such as camera, microphone, accelerometer, radar, temperature et.c.\r\nA node has a microcontroller that does data aquition and processing, and also some way of storing data,\r\nor transmitting it to another system for further processing and storage.\r\nThe typical functional blocks of firmware for a sensor node are:\r\n\r\n- Data readout. Fetching data from each of the attached sensors.\r\n- Processing. Extracting useful information from the data.\r\n- Data storage. Storing either for long term, or as a transmission buffer.\r\n- Data transmission. Sending the extracted information to external systems.\r\n- Power management. Transitioning between sleep and aware as needed.\r\n\r\nFor low cost installation and operation,\r\nmany sensor nodes are battery-powered and use wireless connectivity.\r\nAnd they are often deployed together as part of larger Wireless Sensor Networks.\r\n\r\n### About MicroPython\r\nMicroPython is an implementation of Python that runs on practically all microcontrollers with 16kB+ RAM.\r\nIt provides access to the microcontroller hardware, functions for interacting with sensors and external pheripherals,\r\nas well as connectivity options such as WiFi, Ethernet, Bluetooth Low Energy, etc.\r\n\r\nWhile MicroPython (and the emlearn library) can target a very wide range of hardware,\r\nwe will focus on the Espressif ESP32 family of devices.\r\nThese are very powerful and affordable, with good WiFi+BLE connectivity support,\r\ngpod open-source toolchains, are very popular both among hobbyist and companies,\r\nand have many good ready-to-use hardware development kits.\r\n\r\n#### Challenges and constraints of microcontroller-based sensor nodes\r\nWhile microcontrollers are getting more powerful year by year,\r\nit is still important to fit within the limited RAM, program size and CPU time available.\r\nFor sensors with low datarates (like accelerometers) this is rather doable,\r\nbut for higher datarates such as audio or images good practices can be critical.\r\nFurthermore we may wish to operate on low-power with long battery life.\r\nIn that case it is critical to maximize sleeping, which means to reduce device wakeups,\r\nand to quickly return back to sleep.\r\nEnsuring that we stay within the resource budgets requires some care (in any programming language),\r\nand a high-level language like Python poses some particular challenges.\r\n\r\n#### Tools and practices for efficient sensor data processing in MicroPython\r\nWe will go through the tools which MicroPython provides for efficient sensor data processing.\r\nThis includes:\r\n\r\n- Ways of writing (Micro)Python code that are faster. For example reducing allocations\r\n- Optimizing subsets of Python using the @native and @viper code emitters\r\n- The built-in Python-based assembler for ARM Cortex M chip\r\n- Dynamic native C modules. Can be installed at runtime\r\n- User C modules. Can be baked into a custom MicroPython image\r\n\r\nWe will compare these approaches on a few algorithms that are often used of typical sensor data processing.\r\nThis includes algorithms from the world of Digital Signal Processing as well as Machine Learning.\r\nCandidates include Fast Fourier Transform (FFT), Root Mean Square (RMS),\r\nConvolutional Neural Network (CNN), and Random Forest (RF).\r\n\r\nThe emlearn-micropython packages provided a set of MicroPython modules\r\nthat can be installed onto a device, without having to recompile any C code.\r\nThis preserves the ease-of-use that Python developers are used to on a desktop system.\r\nCompared to pure-Python approaches, the emlearn-micropython modules are typically 10-100x faster.\r\n\r\n#### Intended audience and expected background\r\n\r\nAny developer or data scientist curious about sensor data processing, IoT,\r\nand how Python scales down to the smallest of devices.\r\n\r\nThe audience is expected to have a basic literacy in Python, proficiency in programming,\r\nand know the basics of data processing.\r\nSome familiarity in time-series processing, Digital Signal Processing or \r\nMachine Learning, will make the talk much more relevant to you.\r\nFamiliarity with microcontrollers and embedded systems is of course an advantage\r\n- but the talk should be approachable to those who are new to this area.\r\n\r\n#### Focus and scope of the talk\r\nThe main part of the talk will be how to built sensor-nodes for scientific applications with MicroPython,\r\n\r\nThe general introduction to MicroPython will be kept rather brief, as there are many resources for this available already.","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[9304],"slots":[1223024],"answers":[391139,391135,391136,391142]},{"code":"TP8ZB7","title":"Python-Blosc2: Compress Better, Compute Bigger!","speakers":["BMFCA3","WFFNSW"],"submission_type":5566,"track":5542,"tags":[1413],"state":"confirmed","abstract":"Have you ever experienced the frustration of not being able to analyze a dataset because it's too large to fit in memory? Or perhaps you've encountered the memory wall, where computation is hindered by slow memory access? These are common challenges in data science and high-performance computing.\r\n\r\nPython-Blosc2 (https://www.blosc.org/python-blosc2/) is a high-performance, multi-threaded, multi-codec array container, with an integrated compute engine that allows you to compress *and compute* on large datasets efficiently. In this talk, we will explore the latest features of Python-Blosc2, including its seamless integration with NumPy, and the Python Data ecosystem in general, and how it can help you tackle data challenges that exceed the limits of your available RAM, all while maintaining high performance.","description":"Blosc and Blosc2 are well-known and widely used libraries for high-performance data compression. They are particularly effective for compressing large datasets, such as those encountered in data science and high-performance computing. The Blosc library has been around for over a decade, and its design has always prioritized speed, with a focus on achieving compression and decompression speeds that are close to or even exceed memory bandwidth limits.\r\n\r\nWith the introduction of a new compute engine in Python-Blosc2 3.0, the guiding principle has evolved to \"Compress Better, Compute Bigger.\" This enhancement enables computations on datasets that are over 100 times larger than the available RAM, all while maintaining high performance.\r\n\r\nDuring our talk, we will delve into the latest features of Python-Blosc2, including:\r\n\r\n* Seamless integration with NumPy and the Python Data ecosystem\r\n* High-performance compression and decompression\r\n* The new compute engine and its capabilities\r\n* A JIT (Just-In-Time) compiler for Python functions including almost all NumPy functions\r\n* The ability to perform computations on datasets that exceed available RAM\r\n\r\nTo illustrate this, we will present an example of using Python-Blosc2 to analyze a dataset that largely exceeds the capacity of the available RAM. We will demonstrate how to leverage the new compute engine to perform computations efficiently, without the need for specialized hardware or infrastructure.\r\n\r\nBy the end of this talk, attendees will understand how Python-Blosc2 can help overcome memory constraints in their data workflows. Whether you're working with medium-sized datasets on modest hardware or large datasets on high-performance systems, you'll learn practical techniques to compress data while maintaining computational efficiency.\r\n\r\nJoin us to explore how this powerful library can expand your capabilities for scientific computing and data analysis while reducing memory footprint and improving processing speed.","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223054],"answers":[381901,381900,381896,381897,381904]},{"code":"X9TUZ8","title":"A Hitchhiker's Guide to the Array API Standard Ecosystem","speakers":["L3QKJA"],"submission_type":5567,"track":5542,"tags":[],"state":"confirmed","abstract":"The array API standard is unifying the ecosystem of Python array computing, facilitating greater interoperability between code written for different array libraries, including NumPy, CuPy, PyTorch, JAX, and Dask.\r\n\r\nBut what are all of these \"array-api-*\" libraries for? How can you use these libraries to 'future-proof' *your* libraries, and provide support for GPU and distributed arrays to your users? Find out in this talk, where I'll guide you through every corner of the array API standard ecosystem, explaining how SciPy and scikit-learn are using all of these tools to adopt the standard. I'll also be sharing progress updates from the past year, to give you a clear picture of where we are now, and what the future holds.","description":"Support for alternative array libraries is one of the most common requests put to libraries written on top of NumPy. Users are often interested in boosting their performance by using libraries which can make the most of the hardware available to them, as well as techniques like Just-In-Time compilation. Unfortunately, supporting multiple array libraries is not simple due to API differences, so such support has historically meant heavy duplication of work, with a whole new implementation for each alternative array library. This duplication of work is at best inefficient, and at worst infeasible for maintainers of many libraries.\r\n\r\nThe Python array API standard aims to standardise functionality which exists in most array libraries. It specifies an API which 'array-consumer' libraries can use to write 'array-agnostic' code, where the same codebase can support many array libraries at once.\r\n\r\nIn this talk, I give a brief introduction to the standard, before taking you on a tour of the standard's ecosystem, explaining how each library built around the standard fits into the bigger picture. I'll explain what the standard enables for SciPy and scikit-learn, and what else may be needed in the future.\r\n\r\nAfter this talk, you'll be well-acquainted with the following libraries from the data-apis org:\r\n- array-api\r\n- array-api-tests\r\n- array-api-compat\r\n- array-api-strict\r\n- array-api-extra\r\n\r\nI'll also be mentioning some exciting new tools like MArray.\r\n\r\nRough talk outline:\r\n- 5 mins — what is the array API standard, and what problem is it addressing?\r\n- 10 mins — a tour of the ecosystem: what do all of these libraries do, and how can you use them?\r\n- 10 mins — updates from the past year, status of where we are now, what is coming in the future?\r\n- Any spare time — try some live coding!","duration":30,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223017],"answers":[379787,379783,379784,379790]},{"code":"Z83QKH","title":"Deploy your Machine Learning model with Fast API","speakers":["8EGVC9"],"submission_type":5561,"track":5545,"tags":[1413,1637],"state":"confirmed","abstract":"One of the challenges for a machine learning project is to deploy it. Fast API provides a fast and easy way to deploy a prototype with less software development expertise and yet allow it to be developed into a professional web service. We will look at how to do it.","description":"In this workshop, we will go deeper into how to prototype a machine-learning project with Fast API. Fast API allows the creation API server with very little effort, it is easy to deploy a pre-trained model, but for models that require re-training, the challenge of when and how to retrain a model and update for a service in use becomes complicated. We will cover the aspect of delivering a pre-trained model and the design of re-training the model. This workshop will also provide suggestions for deploying the machine learning project so it can migrate from a prototype to a functional service in production.\r\n\r\n## Goal\r\n\r\nThe workshop aims to equip a data science team capability to convert their machine learning project into a prototype service using Fast API, at the end of the workshop, they will not just be able to deliver API calls to a pre-trained model, but they will also be able to design when to re-train and update the model and be ready to migrate the prototype into production.\r\n\r\n## Target audience\r\n\r\nData scientists who have little or no experience using Fast API or putting a machine learning model into production. This workshop will assume the audience already knows how to build and train a basic machine learning model (e.g. using Sci-kit learn). \r\n\r\n## Outline\r\n\r\nPart 1 - Introduction to Fast APi and prediction on demand\r\n\r\n- Understand the basics of Fast API\r\n- Using a pre-trained model for prediction with API calls\r\n- Validating the query parameters\r\n\r\nPart 2 - Re-train and update models\r\n\r\n- Problem with updating model: Race conditions\r\n- Scheduled re-training\r\n- Re-training on demand with Fast API\r\n\r\nPart 3 - Machine learning model in production\r\n\r\n- Fast API in docker containers\r\n- Fast API on the cloud","duration":90,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1222997],"answers":[382256,382257,382262,382259]},{"code":"ZRA3GV","title":"Solving Hard Optimization Problems with Pyomo and HiGHS: A Practical Introduction","speakers":["8LQU9C"],"submission_type":5566,"track":5542,"tags":[1413],"state":"confirmed","abstract":"Mixed-Integer Programming (MIP) is a fundamental technique for solving complex real-world optimization problems in logistics, scheduling, and resource allocation. However, these problems are combinatorially hard, requiring specialized solvers to find optimal solutions efficiently. This talk introduces Pyomo, a Python-based modeling language, and HiGHS, a state-of-the-art open-source solver. We will first explore the class of problems that MIP can solve, discuss why they are computationally challenging, and then explain how modern solvers like HiGHS tackle these challenges. Using conference scheduling as a real-world example, we demonstrate how Pyomo and HiGHS work together to model and solve an optimization problem. Attendees will leave with a clear understanding of how to leverage these tools for scientific and industrial optimization tasks.","description":"### **1. Introduction (3 min) – What Kind of Problems Can MIP Solve?**\r\n- MIP is widely used in scheduling, logistics, and operations research.\r\n- Examples of real-world problems: workforce scheduling, vehicle routing, and conference scheduling.\r\n- Why are these problems hard? The explosion of possible solutions in combinatorial optimization.\r\n\r\n### **2. Why Are These Problems Difficult? (2 min) – The Challenge of Combinatorial Optimization**\r\n- Theoretical complexity: Why brute-force search is infeasible.\r\n- How solvers like **HiGHS** approach the problem efficiently:\r\n  - **Branch-and-Bound**: How it systematically narrows down the search space.\r\n  - **Cutting Planes & Presolve Techniques**: Reducing problem size before solving.\r\n  - **Heuristics vs. Exact Solutions**: Trade-offs in computation time.\r\n\r\n### **3. Introduction to Pyomo & HiGHS (4 min) – The Optimization Toolkit**\r\n- **Pyomo**: A structured way to define optimization models in Python.\r\n- **HiGHS**: A high-performance solver for linear and mixed-integer problems.\r\n- How they work together: Model in Pyomo → Solve with HiGHS → Interpret results.\r\n\r\n### **4. Conference Scheduling (5 min) – A Practical Example**\r\n- Formulating the problem as a **MIP model**:\r\n  - Decision variables: Assigning talks to rooms and time slots.\r\n  - Constraints: Speaker availability, room capacities, topic grouping.\r\n  - Objective function: Maximizing session coherence and fairness.\r\n- Broad overview of the **Python implementation**:\r\n  - Writing the model in Pyomo.\r\n  - Solving it using HiGHS.\r\n  - Evaluating the solution.\r\n\r\n### **5. Conclusion & Wrap-up (1 min) – Key Takeaways**\r\n- Why Pyomo and HiGHS? Flexibility, efficiency, and scalability.\r\n- Where to go next: Learning resources and real-world applications beyond scheduling.","duration":20,"slot_count":1,"content_locale":"en","do_not_record":false,"resources":[],"slots":[1223056],"answers":[376038,376034,376035]}]}